O PROJETO MONUMENTAL 




Stephen Hawking & Leonard Mlodinow 




 




Conteúdo 




O MISTÉRIO DO SER .............................................................................................................................. 4 




O IMPÉRIO DA LEI ................................................................................................................................. 9 




O QUE É REALIDADE? .......................................................................................................................... 22 




HISTÓRIAS ALTERNATIVAS .................................................................................................................. 37 




A TEORIA DE TUDO ............................................................................................................................. 52 




ESCOLHENDO O NOSSO UNIVERSO ..................................................................................................... 73 




O MILAGRE APARENTE ....................................................................................................................... 87 




O PROJETO MONUMENTAL ................................................................................................................ 98

1 




O MISTÉRIO DO SER 




 




EXISTIMOS, CADA UM DE NÓS POR UM CURTO PERÍODO e neste período exploramos 




apenas uma pequena parte de todo o universo. Mas os humanos são uma espécie curiosa. Nós 




nos perguntamos, procuramos respostas. Viver nesse vasto mundo que é ao mesmo tempo 




gentil e cruel, e olhar para o imenso céu acima, as pessoas sempre fizeram muitas perguntas: 




Como podemos entender o mundo em que nos encontramos? Como o universo se comporta? 




Qual é a natureza da realidade? De onde veio tudo isso? O universo precisou de um criador? A 




maioria de nós não gasta muito tempo preocupando-se com essas questões, mas quase todos nos 




preocupamos com eles uma parte do tempo. 




Tradicionalmente, essas são questões para a filosofia, mas a filosofia está morta. A filosofia não 




tem acompanhado a evolução da ciência moderna, particularmente da física. Os cientistas se 




tornaram os portadores da tocha da descoberta, em nossa busca pelo conhecimento. O propósito 




deste livro é dar as respostas que são sugeridas por recentes descobertas e avanços teóricos. 




Eles nos levam a um novo quadro do universo e nosso lugar nele que é muito diferente do 




tradicional, e diferente até mesmo da imagem que podíamos pintar apenas uma década ou duas 




atrás. Ainda assim, os primeiros esboços do novo conceito podem ser rastreados até quase um 




século atrás. 




De acordo com a concepção tradicional do universo, os objetos se movem por caminhos bem 




definidos e têm histórias definitivas. Podemos especificar suas posições exatas a cada momento 




no tempo. Embora esse relato seja bem-sucedido o suficiente para fins práticos, foi descoberto 




em 1920 que esta imagem "clássica" não poderia explicar o comportamento aparentemente 




estranho observado nas escalas atômicas e subatômicas da existência. Em vez disso, foi 




necessário adotar um quadro diferente, chamado física quântica. As teorias do Quantum 




acabaram por ser extremamente precisas na predição de eventos nessas escalas, ao mesmo 




tempo em que reproduziam as previsões das teorias clássicas antigas, quando aplicadas ao 




mundo macroscópico da vida diária. Mas, física quântica e física clássica são baseadas em 




concepções muito diferentes da realidade física. 










As teorias quânticas podem ser formuladas de várias maneiras diferentes, mas a descrição que 




provavelmente é a mais intuitiva foi dada por Richard (Dick) Feynman, uma personagem 




pitoresca, que trabalhava no Instituto de Tecnologia da Califórnia e tocava bongô em um 




inferninho de striptease perto dali. Segundo Feynman, um sistema não tem apenas uma história, 




mas todas as histórias possíveis. À medida que buscamos nossas respostas, explicaremos a 




abordagem de Feynman em detalhe, e a empregaremos para explorar a idéia de que o universo 




em si não tem uma história única, nem mesmo uma existência independente. Esta parece ser 




uma idéia radical, mesmo para muitos físicos. Na verdade, como muitas noções da ciência 




atual, ela parece violar o senso comum. Mas, o senso comum é baseado na experiência 




cotidiana, e não sobre o universo como ele é revelado através das maravilhas da tecnologia, tais 




como as que nos permitem olhar em profundidade o átomo ou voltar ao início do universo. 




Até o advento da física moderna, em geral se pensava que todo o conhecimento do mundo 




poderia ser obtido através da observação direta, que as coisas são o que parecem ser, como são 




percebidas através de nossos sentidos. Mas o sucesso espetacular da física moderna, que é 




baseada em conceitos tais como o de Feynman que se chocam contra a experiência do dia-a-dia, 




mostrou que não é esse o caso. A visão ingênua da realidade, portanto, não é compatível com a 




física moderna. Para lidar com tais paradoxos, adotaremos uma abordagem que chamamos 




realismo dependente de modelo. Ele é baseado na ideia de que nosso cérebro interpreta a 




entrada de nossos órgãos sensoriais criando um modelo do mundo. Quando este modelo é bem-




sucedido ao explicar eventos, tendemos a atribuir-lhe, e aos elementos e conceitos que o 




constituem, a qualidade da realidade ou verdade absoluta. Mas pode haver diferentes formas em 




que se pode modelar a mesma situação física, com cada uma empregando diferentes elementos 




e conceitos fundamentais. Se duas dessas teorias físicas ou modelos prevêem com precisão os 




mesmos acontecimentos, não se pode dizer que uma é mais real que a outra, mas sim, que 




estamos livres para usar qualquer modelo que seja mais conveniente. 




Na história da ciência, descobrimos uma sequência de teorias ou modelos cada vez melhores, de 




Platão à teoria clássica de Newton até a teoria quântica moderna. É natural perguntar: Será que 







esta sequência, eventualmente chegará a um ponto final, uma teoria definitiva do Universo, que 




incluirá todas as forças e preverá cada observação que possamos fazer, ou vamos continuar para 




sempre encontrando teorias melhores, mas nunca uma que não possa ser melhorada? Nós ainda 




não temos uma resposta definitiva para esta pergunta, mas agora temos um candidato à teoria 




final de tudo, se esta realmente existe, chamada teoria-M. A Teoria-M é o único modelo que 




tem todas as propriedades que nós achamos que a teoria final devesse ter, e é a teoria sobre a 




qual muito da nossa discussão posterior está baseada. 




A Teoria-M não é uma teoria no sentido habitual. Ela é uma família inteira de diferentes teorias, 




cada uma delas uma boa descrição das observações apenas em algum domínio de situações 




físicas. É um pouco como um mapa. Como se sabe muito bem, não se pode mostrar toda a 




superfície da Terra em um único mapa. A projeção comum de Mercator utilizada para os mapas 




do mundo faz as áreas parecerem cada vez maiores, no extremo norte e no extremo sul, e não 




abrange os pólos Norte e Sul. O mapa fiel de toda a terra precisa usar uma coleção de mapas, 




cada uma das quais cobrindo uma região limitada. Os mapas se sobrepõem uns aos outros, e 




quando fazem isso, eles mostram a mesma paisagem. A Teoria-M é semelhante. As diferentes 




teorias da família da teoria-M podem parecer muito diferentes, mas todas elas podem ser 




consideradas aspectos da mesma teoria subjacente. Elas são versões da teoria de que são 




aplicáveis apenas em intervalos limitados, por exemplo, quando certas quantidades, tais como a 




energia são pequenas. Como os mapas sobrepostos em uma projeção de Mercator, onde os 




intervalos de diferentes versões se sobrepõem, eles prevêem o mesmo fenômeno. Mas, assim 




como não existe um mapa plano que seja uma boa representação de toda a superfície da Terra, 




não existe uma única teoria que seja uma boa representação das observações em todas as 




situações. 













Mapa Mundi

. Pode ser necessário uma série de teorias sobrepostas para representar o 




universo, assim como se exige a sobreposição de mapas para representar a Terra. 







Descreveremos como a teoria-M pode oferecer respostas à questão da criação. De acordo com a teoria-M, o nosso não é o único universo. Ao invés disso, a teoria-M prevê que um grande 




número de universos foram criados a partir do nada. Sua criação não exige a intervenção de 




algum ser sobrenatural ou um deus. Pelo contrário, estes múltiplos universos surgem 




naturalmente da lei física. Eles são uma previsão da ciência. Cada universo tem muitas histórias 




possíveis e muitos estados possíveis em momentos posteriores, ou seja, em momentos como o 




presente, muito tempo depois de sua criação. A maioria desses estados será muito diferente do 




universo que observamos e bastante inadequada para a existência de qualquer forma de vida. 




Apenas alguns poucos permitiriam a existência de criaturas como nós. Assim, nossa presença 




seleciona a partir desta vasta série apenas aqueles universos que são compatíveis com nossa 




existência. Apesar de sermos fracos e insignificantes na escala do cosmos, isto faz de nós em 




certo sentido os senhores da criação. 




Para compreender o universo no nível mais profundo, precisamos saber não só como o universo 




se comporta, mas por quê. 




1.  Por que existe algo ao invés de nada? 




2.  Por que existimos? 




3.  Por este conjunto específico de leis e não algum outro? 




Esta é a Questão Fundamental da Vida, do Universo e de Tudo. Tentaremos responder a ela 




neste livro. Ao contrário da resposta dada no Guia do Mochileiro das Galáxias, o nosso não será 




simplesmente "42".

2 




O IMPÉRIO DA LEI 







 

Skoll o lobo que assustará a Lua 




 

Até que ele voa para a Floresta Assustadora: 




 

Hati o lobo, parente de Hridvitnir, 




 

Quem perseguirá o sol. 




- "GRIMNISMAL," The Elder Edda 




NA MITOLOGIA VIKING, Skoll e Hati perseguem o sol e a lua. Quando os lobos pegam 




qualquer um deles, há um eclipse. Quando isso acontece, as pessoas na terra correm para salvar 




o sol ou a lua, fazendo tanto barulho quanto possam, na esperança de espantar os lobos. Há 




mitos semelhantes em outras culturas. Mas, depois de algum tempo, as pessoas devem ter 




notado que o sol e a lua logo surgiam do eclipse independentemente de eles correrem gritando e 




batendo nas coisas. Depois de algum tempo, eles também devem ter notado que os eclipses não 




acontecem ao acaso: Eles ocorriam em padrões regulares que se repetiam. Esses padrões eram 




mais óbvios para eclipses da Lua e permitiam aos antigos babilônios prever os eclipses lunares 




com bastante precisão, embora eles não percebessem que eles eram causados pela terra 




bloqueando a luz do sol. Os eclipses do Sol eram mais difíceis de prever, porque eles são 




visíveis apenas em um corredor na Terra de cerca de 30 milhas de largura. Ainda assim, uma 




vez entendidos, os padrões deixaram claro que os eclipses não dependiam dos caprichos 




arbitrários de seres sobrenaturais, mas que eram governados por leis. 










Eclipse

 Os antigos não sabiam o que causava um eclipse, mas eles notaram padrões na sua 




ocorrência 







Apesar de algum sucesso inicial em prever o movimento dos corpos celestes, a maioria dos 




eventos na natureza parecia aos nossos antepassados serem impossíveis de prever. Vulcões, 




terremotos, tempestades, pestes e unhas encravadas, tudo parecia ocorrer sem causa ou padrão 




aparente. Nos tempos antigos, era natural de atribuir os atos violentos da natureza a um panteão de divindades travessas ou malevolentes. Calamidades eram muitas vezes tomadas como um 




sinal de que tínhamos, de alguma forma, ofendido os deuses. Por exemplo, em 




aproximadamente 5600 aC, o vulcão Monte Mazama no Oregon entrou em erupção, lançando 




rocha e cinzas acesas por anos, e levando a muitos anos de chuvas que, eventualmente, 




encheram a cratera vulcânica, hoje chamada Crater Lake. Os índios Klamath do Oregon têm 




uma lenda que corresponde fielmente a todos os detalhes geológicos do evento, mas adiciona 




um pouco de drama ao retratar um ser humano como a causa da catástrofe. A capacidade 




humana para a culpa é tal que as pessoas podem sempre encontrar maneiras de se culpar. Como 




diz a lenda, Llao, o chefe do Mundo Inferior, apaixona-se pela bela filha humana de um chefe 




Klamath. Ela o despreza, e como vingança Llao tenta destruir os Klamath com fogo. 




Felizmente, segundo a lenda, Skell, o chefe do Mundo Superior, se compadece do ser humano e 




luta com o seu homólogo do submundo. Eventualmente, Llao, ferido, cai de volta dentro de 




Monte Mazama, deixando um enorme buraco, a cratera que, eventualmente, ficou cheia de 




água. 




A ignorância dos caminhos da natureza levou as pessoas em tempos antigos a inventar deuses 




para dominar cada aspecto da vida humana. Havia deuses do amor e da guerra, do sol, da terra e 




do céu; dos oceanos e rios; da chuva e trovoadas, até mesmo dos terremotos e dos vulcões. 




Quando os deuses estavam satisfeitos, os homens eram tratados com bom tempo, paz e 




liberdade de desastres naturais e doenças. Quando eles estavam descontentes, vinha a seca, 




guerra, peste e epidemias. Dado que a ligação de causa e efeito na natureza era invisível aos 




seus olhos, esses deuses pareciam inescrutáveis, e as pessoas estavam à sua mercê. Mas, com 




Tales de Mileto (cerca de 624 bC - 546 bC), cerca de 2.600 anos atrás, isso começou a mudar. 




A idéia surgiu de que a natureza segue princípios consistentes e que podem ser decifrados. E, 




assim, começou o longo processo de substituição da noção do reino dos deuses pelo conceito de 




um universo regido pelas leis da natureza, e criado de acordo com um plano que poderemos 




algum dia aprender a ler. 




Vistos na linha do tempo da história humana, a investigação científica é um esforço muito 




recente. Nossa espécie, o Homo sapiens, originou-se na África subsaariana em torno de 200.000 




aC. A linguagem escrita remonta apenas a cerca de 7000 aC, o produto das sociedades 




centradas no cultivo de grãos. (Algumas das mais antigas inscrições escritas refere-se à ração 




diária de cerveja permitida a cada cidadão.) Os primeiros registros escritos da grande 




civilização da Grécia antiga, data do século IX aC, mas o auge daquela civilização, o "período 




clássico", veio várias centenas de anos mais tarde, começando um pouco antes de 500 aC. 




Segundo Aristóteles (384 aC-322 aC), foi nessa época que Thales desenvolveu pela primeira 




vez a ideia de que o mundo pode ser entendido, que os acontecimentos complexos em torno de 




nós podiam ser reduzidos a princípios simples e explicados sem recorrer a explicações míticas 




ou teológicas. 




Credita-se a Thales a primeira previsão de um eclipse solar em 585 aC, embora a grande 




precisão da sua previsão era provavelmente um palpite de sorte. Ele era uma figura sombria que 




nada deixou escrito de sua autoria. Sua casa era um dos centros intelectuais em uma região 




denominada Jônia, colonizada pelos gregos e exerceu uma influência que, eventualmente, 




alcançou desde a Turquia até a Itália. A ciência jônica foi um empreendimento marcado por um 




forte interesse em descobrir leis fundamentais para explicar fenômenos naturais, um tremendo 







marco na história das idéias humanas. Sua abordagem era racional e em muitos casos, levou a 




conclusões surpreendentemente semelhantes ao que nossos métodos mais sofisticados nos 




levaram a acreditar hoje. Ela representou um grande começo. Mas ao longo dos séculos, grande 




parte da ciência jônica seria esquecida - para ser redescoberto e reinventado, às vezes mais de 




uma vez. 




Segundo a lenda, a primeira formulação matemática do que poderíamos chamar hoje de uma lei 




da natureza remonta a um Jônico chamado Pitágoras (ca. 580 a-BC até ca. 490 aC), famoso 




pelo teorema que leva seu nome: que o quadrado da hipotenusa (lado maior) de um triângulo 




retângulo é igual à soma dos quadrados dos outros dois lados. Diz-se que Pitágoras descobriu a 




relação numérica entre o comprimento das cordas utilizadas em instrumentos musicais e as 




combinações harmônicas dos sons. Na linguagem de hoje poderíamos descrever essa relação, 




dizendo que a frequência - o número de vibrações por segundo - de uma corda vibrante fixada 




sob tensão é inversamente proporcional ao comprimento da corda. Do ponto de vista prático, 




isso explica por que o as guitarras baixo deve ter cordas mais longas que guitarras normais. 




Pitágoras provavelmente não descobrir isso realmente - ele também não descobriu o teorema 




que leva seu nome - mas há indícios de que alguma relação entre o comprimento da corda e a 




altura era conhecida em sua época. Em caso afirmativo, poderíamos chamar a isso simples 




fórmula matemática o primeiro caso do que hoje conhecemos como a física teórica. 










Jônia

 - Estudiosos da Jônia antiga estavam entre os primeiros a explicar os fenômenos naturais através de leis da natureza, ao invés de mito ou teologia 










Além da lei das cordas de Pitágoras, as únicas leis da física conhecida corretamente pelos 




antigos eram as três leis detalhadas por Arquimedes (c. 287 aC–ca. 212 aC), de longe, o mais 




eminente físico da antiguidade. Na terminologia atual, a lei da alavanca explica que pequenas 




forças podem levantar grandes pesos porque a alavanca amplifica uma força de acordo com a 




proporção entre as distâncias do apoio da alavanca. A lei da flutuabilidade afirma que qualquer objeto imerso em um fluido experimentará uma força para cima igual ao peso do fluido 




deslocado. E a lei da reflexão afirma que o ângulo entre um feixe de luz e um espelho é igual ao 




ângulo entre o espelho e o raio refletido. Mas, Arquimedes não as chamou de leis, nem as 




explicou com referência à observação e medição. Em vez disso, ela as tratou como se fossem 




teoremas puramente matemáticos, em um sistema axiomático muito parecido com aquele com 




que Euclides criou a geometria. 




Com a expansão da influência jônica, apareceram outros que viram que o universo possuía uma 




ordem interna, que pode ser entendida através da observação e da razão. Anaximandro (cerca de 




610 aC – ca. 546 aC), um amigo e, possivelmente, um estudante de Thales, argumentava que 




desde que os bebês humanos são indefesos ao nascer, se o primeiro ser humano, de alguma 




forma apareceu sobre a terra como um bebê, ele não teria sobrevivido. No que pode ter sido 




primeiro indício de evolução da humanidade, as pessoas, Anaximandro fundamentava, devem 




ter evoluído de outros animais cujos filhotes eram mais resistentes. Na Sicília, Empédocles (ca. 




490 a-BC até ca. 430 aC) observou o uso de um instrumento chamado clepsidra. Às vezes 




usado como uma concha, ele consistia de uma esfera com um pescoço aberto e pequenos 




orifícios na sua parte inferior. Quando imerso em água ele se enchia, e se o pescoço aberto 




fosse coberto, a clepsidra podia ser levantada sem que a água caísse pelos buracos. Empédocles 




notou que se você cobrisse o pescoço antes de mergulhá-lo, uma clepsidra não se enchia. Ele 




raciocinou que algo invisível devia estar impedindo a água de entrar na esfera através dos 




orifícios - ele tinha descoberto a substância material que nós chamamos de ar. 




Por volta da mesma época, Demócrito (cerca de 460 aC - ca. 370 aC), de uma colônia Jônica ao 




norte da Grécia ponderou sobre o que acontecia quando você quebrava ou cortava um objeto 




em pedaços. Ele argumentava que você não deveria ser capaz de continuar o processo 




indefinidamente. Em vez disso, ele postulou que tudo, incluindo todos os seres vivos, é feito de 




partículas fundamentais que não podem ser cortadas ou divididas em partes. Ele chamou a essas 




partículas de átomos finais, a partir do adjetivo grego que significa "indivisível". Demócrito acreditava que todo fenômeno material é o produto da colisão de átomos. Em sua opinião, 




chamada de atomismo, todos os átomos se movem pelo espaço, e, a menos que sejam 




perturbados, avançam indefinidamente. Hoje essa idéia é chamada lei da inércia. 




A ideia revolucionária de que somos nada mais que moradores comuns do universo, e não seres 




especiais distinguidos por existir no seu centro foi defendida primeiro por Aristarco (ca. 310 aC 




– ca. 230 aC), um dos últimos cientistas Jônicos. Apenas um dos seus cálculos sobrevive, uma 




análise geométrica complexa de cuidadosas observações que fez do tamanho da sombra da 




Terra na Lua durante um eclipse lunar. Ele concluiu a partir de seus dados que o sol deve ser 




muito maior que a Terra. Talvez inspirado pela ideia de que objetos pequenos devessem orbitar 




outros gigantescos, e não o contrário, ele se tornou a primeira pessoa a argumentar que a Terra 




não é o centro do nosso sistema planetário, mas sim que ela e os outros planetas orbitam o sol 




muito maior. É um pequeno passo da percepção de que a Terra é apenas outro planeta até a 




ideia de que nosso sol também não é nada especial. Aristarco suspeitava que este fosse o caso e 




acreditava que as estrelas que vemos no céu à noite são, na verdade, nada mais que sóis 




distantes. 




Os jônios foram apenas uma das muitas escolas de filosofia da Grécia antiga, cada uma com 




tradições diferentes e, muitas vezes, contraditórias. Infelizmente, a visão dos jônicos da 




natureza - que ela pode ser explicada através de leis gerais e reduzida a um simples conjunto de princípios - exerceu uma forte influência por apenas alguns séculos. Uma razão é que as teorias 




Jônicas muitas vezes pareciam não ter lugar para a noção de livre arbítrio ou finalidade, ou o 




conceito que os deuses intervêm no funcionamento do mundo. Estas eram omissões 




surpreendentes como profundamente perturbadoras para muitos pensadores gregos, da mesma 




forma que são para muitas pessoas hoje. O filósofo Epicuro (341 aC - 270 aC), por exemplo, se 




opunha ao atomismo com o fundamento de que "é melhor seguir os mitos sobre os deuses que 




tornar-se um "escravo" do destino de filósofos naturais." Aristóteles também rejeitou a conceito de átomo, porque ele não podia aceitar que os seres humanos fossem compostos de objetos sem 




alma, inanimados. A ideia Jônica, de que o universo não é centrado no homem, foi um marco 




em nossa compreensão do cosmos, mas era uma ideia que seria descartada e não mais retomada 




ou comumente aceita até Galileu, quase vinte séculos mais tarde. 




Tão perspicazes quanto possam ser algumas das suas especulações sobre a natureza, a maioria 




das ideias dos gregos antigos não seriam válidas como ciência nos tempos modernos. Por um 




lado, porque os gregos não tinham inventado o método científico, suas teorias não eram 




desenvolvidas com o objetivo de verificação experimental. Portanto, se um estudioso afirmasse 




que um átomo se movia em linha reta até que colidisse com um segundo átomo, e outro 




especialista afirmasse que se movia em linha reta até esbarrasse em um ciclope, não havia 




maneira objetiva de resolver a discussão. Além disso, não havia distinção clara entre as leis 




humanas e físicas. No século V aC, por exemplo, Anaximandro escreveu que todas as coisas 




surgem de uma substância primária, e volta a ela, a menos que "paguem multa e penalidade por 




sua iniquidade". E, de acordo com o filósofo Jônico Heráclito (ca. 535 aC – ca. 475 aC), o sol 




se comporta como faz porque senão a deusa da justiça o caçará. Centenas de anos mais tarde, os 




estóicos, uma escola de filósofos gregos que surgiu em torno do terceiro século aC, fazia uma 




distinção entre estatutos humanos e as leis naturais, mas eles incluíam normas de conduta 




humana que eles considerados universais - tais como a veneração de Deus e obediência aos pais 




- na categoria de leis naturais. Por outro lado, eles muitas vezes descreviam processos físicos 




em termos jurídicos e acreditavam ser necessário fazer cumprir a lei, mesmo que os objetos dos 




quais se exigia que "obedecessem" às leis fossem inanimados. Se você acha que é difícil 




conseguir que as pessoas obedeçam às leis de trânsito, imagine convencer um asteróide a se 




mover ao longo de uma elipse. 




Esta tradição continuou a influenciar os pensadores que sucederam os gregos, por muitos 




séculos depois. No século XIII o primeiro filósofo cristão Tomás de Aquino (ca. 1225-1274) 




adotou esta visão e a usou para defender a existência de Deus, escrevendo: "É claro que [corpos inanimados] chegam ao fim não por acaso, mas por vontade... Há, portanto, um ser pessoal 




inteligente por quem tudo na natureza é ordenado até seu fim". Mesmo no século XVI, o grande 




astrônomo alemão Johannes Kepler (1571-1630) acreditava que os planetas tinham senso de 




percepção e seguiam conscientes as leis de movimento que eram captadas por suas ―mentes‖. 




A noção de que as leis da natureza tinham de ser obedecida intencionalmente reflete o foco dos 




antigos sobre por que a natureza se comporta como o faz, e não como ele se comporta. 




Aristóteles foi um dos principais defensores dessa abordagem, rejeitando a ideia de ciência 




baseada principalmente na observação. Medição precisa e cálculo matemático eram, em todo 




caso difíceis nos tempos antigos. A notação de número base dez que achamos muito 




conveniente para a aritmética remonta a cerca de 700 dC, quando os hindus deram os primeiros 




grandes passos em direção a tornar o assunto uma ferramenta poderosa. As abreviaturas dos sinais de mais e menos não apareceram até o século XV. E nem o sinal de igual, nem relógios 




que podiam medir o tempo até os segundos existiam antes do século XVI. 




Aristóteles, no entanto, não via problemas na medição e cálculo como impedimentos para o 




desenvolvimento de uma física capaz de produzir previsões quantitativas. Ao contrário, ele não 




via necessidade de fazê-las. Em vez disso, Aristóteles construiu sua física baseada nos 




princípios que o atraíam intelectualmente. Ele suprimiu fatos que achava desagradáveis e 




concentrou seus esforços sobre os motivos pelos quais as coisas aconteçam, com relativamente 




pouca energia investida no detalhando exato do que estava acontecendo. Aristóteles ajustou 




suas conclusões, quando o seu desacordo flagrante com a observação não podia mais ser 




ignorado. Mas, esses ajustes eram, com frequência, explicações ad hoc que pouco fizeram a 




mais que colar sobre a contradição. Dessa maneira, não importa quão severamente sua teoria 




desviou-se da realidade, ele podia sempre alterá-la apenas o suficiente para parecer remover o 




conflito. Por exemplo, sua teoria do movimento especificava que os corpos pesados caem com 




uma velocidade constante, que é proporcional ao seu peso. Para explicar o fato de que os 




objetos claramente se aceleram à medida que caem, ele inventou um novo princípio - de que os 




organismos avançam com mais alegria, e, consequentemente, aceleram, quando se aproximam 




de seu lugar natural de repouso, um princípio que hoje parece ser uma descrição mais apta de 




certas pessoas do que de objetos inanimados. Embora as teorias de Aristóteles, muitas vezes 




tivessem pouco valor preditivo, sua abordagem da ciência dominou o pensamento ocidental por 




quase dois mil anos. 




Os sucessores cristãos dos gregos rejeitaram a ideia de que o universo fosse governado por leis 




naturais indiferentes. Eles também rejeitaram a ideia de que os seres humanos não ocupam um 




lugar privilegiado dentro desse universo. E embora o período medieval não tivesse nenhum 




sistema filosófico único e coerente, um tema comum era de que o universo é casa de bonecas de 




Deus, e a religião um estudo muito mais digno do que o estudo dos fenômenos da natureza. 




Com efeito, em 1277 o bispo Tempier de Paris, sob as instruções do Papa João XXI, publicou 




uma lista de 219 erros ou heresias que deviam ser condenados. Entre as heresias estava a ideia 




de que a natureza segue leis, porque isso entra em conflito com a onipotência de Deus. 




Curiosamente, o Papa João foi morto pelos efeitos da lei da gravidade alguns meses mais tarde, 




quando o telhado de seu palácio caiu sobre dele. 










O conceito moderno de leis da natureza surgiu no século XVII. Kepler parece ter sido o 




primeiro cientista a compreender o termo no sentido da ciência moderna, embora, como já 




dissemos, ele mantivesse uma visão animista dos objetos físicos. Galileu (1564-1642) não 




utilizou o termo "lei" em seus trabalhos mais científicos (embora ele apareça em algumas 




traduções daquelas obras). Se usou ou não a palavra, no entanto, Galileu descobriu um grande 




número de leis, e defendeu os princípios importantes de que a observação é a base da ciência e 




que o objetivo da ciência é a investigação das relações quantitativas que existem entre os 




fenômenos físicos. Mas, a primeira pessoa que expressa e rigorosamente formulou o conceito 




de leis da natureza como nós os entendemos hoje foi René Descartes (1596-1650). 




Descartes acreditava que todos os fenômenos físicos devem ser explicados em termos de 




colisões de massas em movimento, que foram governados por três leis, precursoras das famosas 




leis do movimento de Newton. Ele afirmava que aquelas leis da natureza eram válidas em todos 




os lugares e em todos os momentos, e declarava explicitamente que a obediência a essas leis 




não implica que estes corpos em movimento tivessem mentes. Descartes também entendia a 




importância do que chamamos hoje "condições iniciais". Estas descrevem o estado de um 




sistema no início de qualquer intervalo de tempo durante o qual se procura fazer previsões. 




Com um determinado conjunto de condições iniciais, as leis da natureza determinam como um 




sistema evoluirá ao longo do tempo, mas sem um conjunto específico de condições iniciais, a 




evolução não pode ser especificada. Se, por exemplo, no momento zero um pombo diretamente 




acima da cabeça dá uma cagada, o caminho da merda em queda é determinado por leis de 




Newton. Mas, o resultado será muito diferente, dependendo se, no momento zero, o pombo 




ainda está sentado em um fio de telefone ou voando a 20 quilômetros por hora. Para aplicar as 




leis da física é preciso saber como o sistema começou, ou pelo menos seu estado em algum 




momento definido. (Também é possível utilizar as leis para seguir um sistema para trás no tempo.)  




Com esta crença renovada na existência de leis da natureza, surgiram novas tentativas de 




conciliar essas leis e o conceito de Deus. Segundo Descartes, Deus poderia alterar à vontade a 




verdade ou falsificar proposições éticas ou teoremas matemáticos, mas não a natureza. Ele 




acreditava que Deus criou as leis da natureza, mas não tinha a escolha das leis, mas, ao invés, 




ele as escolheu, porque as leis que experimentamos são as únicas leis possíveis. Isto parecia 




conflitar com a autoridade de Deus, mas Descartes contornou isso argumentando que as leis são 




imutáveis porque elas são um reflexo da própria natureza intrínseca de Deus. Se isso fosse 




verdade, poderíamos pensar que Deus ainda tinha a opção de criar uma variedade de mundos 




diferentes, cada um correspondendo a um conjunto diferente de condições iniciais, mas 




Descartes também negou isso. Não importa qual o arranjo da matéria no início do universo, 




argumentou ele, ao longo do tempo um mundo idêntico ao nosso evoluiria. Além disso, 




Descartes achava que uma vez que Deus colocou no mundo em movimento, ele o deixou 




completamente sozinho. 




Uma posição semelhante (com algumas exceções) foi adotada por Isaac Newton (1643-1727). 




Newton foi a pessoa que ganhou ampla aceitação do conceito moderno de uma lei científica 




com suas três leis do movimento e sua lei da gravidade, que representavam as órbitas da Terra, 




Lua e planetas, e explicava fenômenos tais como as marés. O punhado de equações que ele 




criou, e o quadro matemático elaborado que se originou delas desde então ainda são ensinados 




hoje, e empregados, sempre que um arquiteto desenha um prédio, um engenheiro projeta um 




carro, ou um físico calcula como mirar um foguete destinado a pousar em Marte. Como disse o 




poeta Alexander Pope:  




 

A Natureza e as leis da Natureza jaziam escondidas na noite: 




 

Deus disse: Que Newton seja criado! e tudo era luz. 




Hoje, a maioria dos cientistas diria que uma lei da natureza é uma regra que se baseia em uma 




regularidade observada e que fornece previsões que vão além da situação imediata na qual se 




baseia. Por exemplo, podemos notar que o sol nasceu no leste todos os dias de nossas vidas, e 




postular a lei "O sol sempre nasce no leste." Esta é uma generalização que ultrapassa as nossas observações limitadas do sol nascente, e faz predições testáveis sobre o futuro. Por outro lado, 




uma declaração como "Os computadores neste escritório são pretos" não é uma lei da natureza, porque diz respeito apenas aos computadores do escritório e não faz previsões, tais como "Se 




meu escritório comprar um computador novo, ele será preto. " 




Nosso entendimento moderno do termo "lei da natureza" é uma questão que os filósofos 




discutem longamente, e é uma questão mais sutil do que se pode pensar à primeira vista. Por 




exemplo, o filósofo John W. Carroll comparou a afirmação: "Todas as esferas de ouro são 




menores de um quilômetro de diâmetro" a uma declaração tal como "Todas as esferas de 




urânio-235 têm menos de um quilômetro de diâmetro." Nossas observações do mundo nos 




dizem que não existem esferas de ouro maiores que um quilômetro de largura, e podemos ser 




bastante confiantes de que nunca existirão. Ainda assim, não temos razão para acreditar que não 




poderia existir uma, e, portanto a afirmação não é considerada uma lei. Por outro lado, a 




afirmação "Todas as esferas de urânio-235 têm menos de um quilômetro de diâmetro" poderia 




ser pensada como uma lei da natureza, porque, de acordo com o que sabemos sobre física 




nuclear, uma vez que uma esfera de urânio-235 cresça até um diâmetro maior que cerca de seis polegadas, ela se destruiria em uma explosão nuclear. Daí podemos ter certeza de que tais 




esferas não existem. (Nem seria uma boa idéia tentar fazer uma!) Esta distinção é importante 




porque ela mostra que nem todas as generalizações que observamos podem ser pensadas como 




leis da natureza, e que a maioria das leis da natureza existem como parte de um sistema maior, 




interligados de leis. 




Na ciência moderna as leis da natureza são geralmente redigidas em matemática. Elas podem 




ser exatas ou aproximadas, mas deve ter sido observado que se mantêm, sem exceção - se não 




universalmente, pelo menos sob um determinado conjunto de condições. Por exemplo, hoje 




sabemos que as leis de Newton devem ser modificadas se os objetos estiverem se movendo em 




velocidades próximas à da luz. Mas, ainda consideramos as leis de Newton como leis, porque 




elas se mantêm, pelo menos até uma aproximação muito boa, para as condições do mundo 




cotidiano, no qual as velocidades que encontramos são muito inferiores à velocidade da luz. 




Se a natureza é regida por leis, três questões se colocam: 




1.  Qual é a origem das leis? 




2.  Existe alguma exceção às leis, isto é, milagres? 




3.  Existe apenas um conjunto possível de leis? 




Essas importantes questões foram abordadas de diversas formas por cientistas, filósofos e 




teólogos. A resposta tradicionalmente dada à primeira pergunta - a resposta de Kepler, Galileu, 




Descartes e Newton - foi de que as leis eram obra de Deus. No entanto, isso não é mais que 




uma definição de Deus como a personificação das leis da natureza. A menos que se dote Deus 




com alguns outros atributos, tais como sendo o Deus do Antigo Testamento, empregar Deus 




como resposta à primeira pergunta meramente substitui um mistério por outro. Então, se 




envolvemos Deus ma resposta à primeira pergunta, a crise real vem com a segunda pergunta: 




Existem milagres, exceções às leis? 




As opiniões sobre a resposta à segunda pergunta estavam muito bem divididas. Platão e 




Aristóteles, o mais influente entre os antigos escritores gregos, consideravam que não pode 




haver exceções às leis. Mas, se assumimos a visão bíblica, então Deus não só criou as leis, mas 




pode ser invocado através de oração para fazer exceções - curar doentes terminais, terminar 




prematuramente as secas, ou restabelecer croquet como esporte olímpico. Em oposição à visão 




de Descartes, quase todos os pensadores cristãos defendiam que Deus deve ser capaz de 




suspender as leis para realizar milagres. Mesmo Newton acreditava em milagres de alguma 




espécie. Ele achava que a órbita dos planetas seria instável, pois a atração gravitacional de um 




planeta por outro poderia causar perturbações nas órbitas que cresceriam com o tempo e 




resultariam em os planetas ou caindo em direção ao sol ou a sendo arremessado para fora do 




sistema solar. Deus devia passar o tempo todo redefinindo as órbitas, ele acreditava, ou "dando corda no relógio celestial, para que não parasse". No entanto, Pierre-Simon, marquês de 




Laplace (1749-1827), vulgarmente conhecido como Laplace, argumentava que os transtornos 




seriam periódicos, isto é, marcados por repetidos ciclos, ao invés de serem cumulativos. O 




sistema solar, assim, se restauraria e não haveria necessidade de intervenção divina para 




explicar por que ele tinha sobrevivido até os dias atuais. 







É Laplace que geralmente é creditado como sendo o primeiro claramente postulando o 




determinismo científico: Dado o estado do universo em um dado momento, um conjunto 




completo de leis determina totalmente, tanto o futuro quanto o passado. Isso excluiria a 




possibilidade de milagres ou um papel ativo para Deus. O determinismo científico que Laplace 




formulou é a resposta do cientista moderno à pergunta dois. Ele é, de fato, a base de toda a 




ciência moderna, e um princípio que é importante em todo este livro. Uma lei científica não é 




uma lei científica se ela se mantém apenas quando algum ser sobrenatural decide não intervir. 




Reconhecendo isso, diz-e que Napoleão teria perguntado a Laplace como Deus se encaixaria 




nesse quadro. Laplace respondeu: "Senhor, eu não precisei dessa hipótese". 




Como as pessoas vivem no universo e interagem com os outros objetos nele, o determinismo 




científico deve se manter também em relação às pessoas. Muitos, no entanto, embora admitam 




que o determinismo científico reja os processos físicos, fariam uma exceção para o 




comportamento humano, porque eles acreditam que temos livre arbítrio. Descartes, por 




exemplo, para preservar a idéia de livre-arbítrio, afirmou que a mente humana era algo diferente 




do mundo físico e não seguia suas leis. Em sua opinião, uma pessoa consiste em dois 




componentes, um corpo e uma alma. Os corpos nada mais são que máquinas comuns, ao passo 




que a alma não está sujeita à lei científica. Descartes estava muito interessado em anatomia e 




fisiologia, e considerava um pequeno órgão localizado no centro do cérebro, denominado 




glândula pineal, como a principal sede da alma. Essa glândula, ele acreditava, era o lugar onde 




todos os nossos pensamentos são formados, a fonte de nosso livre arbítrio. 







―Eu acho que você deveria ser mais explícito aqui no passo dois.‖ 







As pessoas têm livre arbítrio? Se temos livre arbítrio, onde na árvore evolutiva ele se 




desenvolveu? As algas azul-verdes ou as bactérias têm livre-arbítrio, ou seus comportamentos 




são automáticos e dentro da esfera da lei científica? Só os organismos multicelulares têm livre-




arbítrio, ou só os mamíferos? Podemos pensar que um chimpanzé está exercitando o livre 




arbítrio quando opta por morder uma banana, ou um gato quando rasga o sofá, mas e a lombriga 




chamada Caenorhabditis elegans, uma criatura simples, feitas de apenas 959 células? Ela 




provavelmente nunca pensa, "Esta era uma bactéria muito saborosa, preciso voltar a jantar ali", e mesmo assim, ela também tem uma clara preferência alimentar e ou aceitará uma refeição 




sem atrativo ou procurará por algo melhor, dependendo da experiência recente. Isso é exercício 




de livre arbítrio? 




Embora sintamos que podemos escolher o que fazemos, nossa compreensão da base molecular 




da biologia mostra que processos biológicos são regidos pelas leis da física e da química e, 




portanto, são tão determinados quanto as órbitas dos planetas. Experiências recentes em 




neurociência dão apoio à opinião de que é nosso cérebro físico, seguindo as leis conhecidas da 




ciência, que determina nossas ações, e não alguma agência que existe fora daquelas leis. Por 




exemplo, um estudo de pacientes submetidos acordados a cirurgia de cérebro descobriu que ao 




estimular eletricamente regiões apropriadas do cérebro, pode-se criar no paciente o desejo de 




mover a mão, braço ou pé, ou mover os lábios e falar. É difícil imaginar como o livre arbítrio 




pode funcionar se o nosso comportamento é determinado pelas leis da física, por isso parece 




que não somos mais que máquinas biológicas e que o livre arbítrio é apenas uma ilusão. 




Embora admitindo que o comportamento humano seja de fato determinado pelas leis da 




natureza, também parece razoável concluir que o resultado é determinado de tal forma 




complicada e com tantas variáveis, de modo a tornar, na prática, impossível prever. Para isso, 




seria necessário um conhecimento do estado inicial de cada um dos milhares de trilhões de 




trilhões de moléculas no corpo humano e resolver alguma coisa como aquele mesmo número de 




equações. Isso levaria alguns bilhões de anos, o que seria um pouco tarde para se abaixar 




quando o opositor desferir um soco. 




Porque é tão pouco prático usar as leis físicas subjacentes para prever o comportamento 




humano, nós adotamos o que é chamado uma teoria efetiva. Em física, uma teoria efetiva é uma 




estrutura criada para modelar certos fenômenos observados sem descrever em detalhes todos os 




processos subjacentes. Por exemplo, não podemos resolver exatamente as equações que regem 




as interações gravitacionais de cada átomo no corpo de uma pessoa com cada átomo da Terra. 




Mas, para todos os fins práticos, a força gravitacional entre uma pessoa e a Terra pode ser 




descrita em termos de apenas alguns números, tais como a massa total da pessoa. Da mesma 




forma, não podemos resolver as equações que regem o comportamento dos átomos e moléculas 




complexas, mas desenvolvemos uma teoria efetiva chamada química que fornece uma 




explicação adequada de como os átomos e moléculas se comportam em reações químicas, sem 




levar em conta todos os detalhes das interações. No caso das pessoas, uma vez que não 




podemos resolver as equações que determinam nosso comportamento, utilizamos a teoria 




efetiva de que as pessoas têm livre-arbítrio. O estudo da nossa vontade e do comportamento que 




surge a partir dela é a ciência da psicologia. Economia também é uma teoria efetiva, baseada na 




noção de livre arbítrio, somada ao pressuposto de que as pessoas avaliam seus cursos 




alternativos de ação possíveis e escolhem o melhor. Essa teoria efetiva é apenas 




moderadamente bem-sucedida para prever comportamento, porque, como todos sabemos, as 




decisões muitas vezes não são racionais ou são baseadas em uma análise defeituosa das 




consequências da escolha. É por isso que o mundo está uma bagunça. 




A terceira pergunta aborda a questão de saber se as leis que determinam tanto o universo quanto 




o comportamento humano são exclusivas. Se a sua resposta à primeira questão é que Deus criou 




as leis, então esta pergunta é, Deus tem alguma flexibilidade na escolha delas? Tanto 




Aristóteles quanto Platão acreditavam, como Descartes e Einstein mais tarde, que os princípios 




da natureza existem fora da "necessidade", isto é, porque são as únicas regras que fazem sentido lógico. Devido à sua crença na origem das leis da natureza na lógica, Aristóteles e seus 




seguidores achavam que se poderia "derivar" aquelas leis sem prestar muita atenção à forma como a natureza realmente se comportou. Isso, e o foco na razão pela qual os objetos seguem 




regras, ao invés das especificidades do que são as regras, o levaram a leis, principalmente 




qualitativas, que muitas vezes estavam erradas e em qualquer caso, não se mostraram muito 




úteis, mesmo dominando o pensamento científico por muitos séculos. Foi só muito mais tarde 




que pessoas como Galileu ousaram desafiar a autoridade de Aristóteles e observar o que a 




natureza realmente fazia, ao invés do que a "razão" pura dizia que deveria fazer. 




Este livro tem suas raízes na noção de determinismo científico, o que implica que a resposta à 




segunda pergunta é que não existem milagres, ou exceções às leis da natureza. Nós, no entanto, 




voltaremos a abordar as perguntas um e três em profundidade, as questões de como as leis 




surgiram, e se elas são as únicas leis possível. Mas, primeiro, no próximo capítulo, abordaremos 




a questão do que as leis da natureza descrevem. A maioria dos cientistas diria que elas são o 




reflexo matemática de uma realidade externa que existe independente do observador que a vê. 




Mas como refletiremos sobre a maneira pela qual observamos e formamos conceitos sobre o 




nosso ambiente, topamos com a pergunta, será que temos realmente motivos para acreditar que 




existe uma realidade objetiva? 




3 




O QUE É REALIDADE? 




 




ALGUNS ANOS ATRÁS a câmara municipal da cidade de Monza, na Itália, impediu os donos 




de animais de estimação de manter peixinho dourados em aquários de vidro curvo. O 




proponente explicou a medida, em parte, dizendo que é cruel para manter um peixe em um 




aquário com lados curvos, porque, olhando para fora, os peixes teriam uma visão distorcida da 




realidade. Mas como sabemos que temos a imagem verdadeira, sem distorções da realidade? 




Não poderíamos também estarmos dentro de algum aquário grande e termos a nossa visão 




distorcida por lentes enormes? A imagem de realidade do peixinho dourado é diferente da 




nossa, mas podemos ter certeza de que é menos real? 




A visão de peixinho não é a mesma que a nossa, mas o peixinho ainda assim poderia formular 




leis científicas regendo o movimento dos objetos observados fora de seu aquário. Por exemplo, 




devido à distorção, um objeto movendo-se livremente que nós observaríamos se movendo em 




linha reta seria observado pelo peixinho movendo-se ao longo de uma trajetória curva. No 




entanto, o peixinho poderia formular leis científicas a partir de seu quadro distorcido de 




referência que sempre se manteriam verdadeiras e que lhes permitiriam fazer previsões sobre o 




movimento futuro de objetos fora do aquário. Suas leis seriam mais complicadas do que as leis 




em nosso quadro, mas simplicidade é uma questão de gosto. Se um peixinho formulasse tal 




teoria, teríamos que admitir a visão do peixe como uma imagem válida da realidade. 




Um exemplo famoso de diferentes imagens da realidade é o modelo introduzido por volta de 




150 dC por Ptolomeu (ca. 85—ca. 165) para descrever o movimento dos corpos celestes. 




Ptolomeu publicou sua obra em um tratado de treze livros geralmente conhecido pelo seu título 




em árabe, Almagesto. O Almagesto começa explicando as razões para pensar que a Terra é 




esférica, imóvel, posicionado no centro do universo, e desprezívelmente pequena em 




comparação com a distância dos céus. Apesar do modelo heliocêntrico de Aristarco, essas 




crenças eram detidas pela maioria dos gregos educados, pelo menos desde os tempos de 




Aristóteles, que acreditava por razões místicas que a terra deveria estar no centro do universo. 




No modelo de Ptolomeu, a Terra ainda estava no centro, os planetas e as estrelas se moviam em 




torno dela em órbitas complicadas envolvendo epiciclos, como rodas sobre rodas. 













O Universo de Ptolomeu - Na visão de Ptolomeu, vivemos no centro do universo 







Este modelo parecia natural porque não sentimos a terra movendo-se sob os nossos pés (exceto 




em terremotos ou momentos de paixão). Mais tarde, a aprendizagem européia estava baseada 




em fontes gregas que tinham sido passadas à frente, de modo que as idéias de Aristóteles e 




Ptolomeu se tornaram a base para grande parte do pensamento ocidental. O modelo de cosmos 




de Ptolomeu foi adotado pela Igreja Católica que o sustentou como doutrina oficial por 1.400 




anos. Não foi senão em 1543 que um modelo alternativo foi proposto por Copérnico no seu 




livro  

De revolutionibus orbium coelestium 

(Sobre as Revoluções das Esferas Celestes), 




publicado apenas no ano da sua morte (embora ele tivesse trabalhado em sua teoria por várias 




décadas). 




Copérnico, como Aristarco cerca de dezessete séculos antes, descreveu um mundo em que o sol 




estava em repouso e os planetas giravam em torno dele em órbitas circulares. Embora a idéia 




não fosse nova, seu renascimento foi recebidp com apaixonada resistência. Foi considerado que 




o modelo de Copérnico contradizia a Bíblia, que era interpretada como dizendo que os planetas 




se moviam em torno da Terra, embora a Bíblia nunca tenha afirmado isso. Na verdade, no 




tempo em que a Bíblia foi escrita, as pessoas acreditavam que a Terra era plana. O modelo de 




Copérnico levou a um acalorado debate sobre se a Terra estava em repouso, culminando no 




julgamento de Galileu por heresia em 1633 para defender o modelo de Copérnico, e por pensar 




"que se pode sustentar e defender uma opinião depois de ela ter sido declarada e definida como 




contrária à Sagrada Escritura". Ele foi considerado culpado, confinados em prisão domiciliar 




pelo resto de sua vida, e obrigado a se retratar. Dizem que ele murmurou com seu último alento 




"  

Eppur si muove

, "Mas, que se move, se move". Em 1992 a Igreja Católica finalmente reconheceu que tinha errado ao condenar Galileu. 




Então, o que é real, o sistema de Ptolomeu ou o de Copérnico? Embora não seja incomum que 




as pessoas digam que Ptolomeu provou que Copérnico estava errado, isso não é verdade. Como 




acontece no caso da nossa visão normal versus a do peixe dourado, pode-se usar qualquer 




imagem como um modelo do universo, pois nossas observações do céu pode ser explicadas 




assumindo-se que tanto a terra quanto o sol estejam em repouso. Apesar do seu papel nos 




debates filosóficos sobre a natureza do nosso universo, a vantagem real do sistema copernicano 




é simplesmente que as equações de movimento são muito mais simples no quadro de referência 




em que o sol está em repouso. 




Um tipo diferente de realidade alternativa ocorre no filme de ficção científica The Matrix, em 




que a raça humana vive, sem saber, em uma realidade virtual simulada criada por computadores 




inteligentes para mantê-los pacificados e contentes, enquanto os computadores sugam sua 




energia bioelétrica (seja lá o for) . Talvez isto não é tão absurdo, porque muitas pessoas 




preferem gastar seu tempo na realidade simulada de sites como o Second Life. Como sabemos 




que não somos apenas personagens de uma novela gerada por computador? Se vivêssemos em 




um mundo imaginário sintético, os eventos não teriam, necessariamente, qualquer lógica ou 




coerência ou obedeceriam a qualquer lei. Os extra-terrestres no controle podem achar mais 




interessante ou divertido ver nossas reações, por exemplo, se a lua se dividisse pela metade, ou 




todos no mundo em dieta desenvolveriam um desejo incontrolável de comer torta de creme de 




banana. Mas se os estra-terrestres fazem cumprir leis consistentes, não há maneira alguma de 




dizer que havia outra realidade por trás da simulação. Seria fácil chamar o mundo em que os 




alienígenas vivem de mundo "real" e o mundo sintético "falso". Mas se - como nós - os seres no mundo simulado não pudessem olhar para seu universo a partir do exterior, não haveria razão 




para que eles duvidassem de suas próprias imagens da realidade. Esta é uma versão moderna da 




idéia de que todos nós somos fruto da sonho de outra pessoa. 




Esses exemplos nos levam a uma conclusão que vai ser importante neste livro: Não existe 




conceito de realidade independente de imagem ou da teoria. Em vez disso, adotaremos uma 




visão a que chamaremos realismo dependente de modelo: a idéia de que uma teoria física ou 




visão de mundo é um modelo (geralmente de natureza matemática) e um conjunto de regras que 




ligam os elementos do modelo a observações. Isto fornece um quadro com o qual interpretar a 




ciência moderna. 










Filósofos desde Platão em diante têm argumentado ao longo dos anos sobre a natureza da 




realidade. A ciência clássica está baseada na crença de que existe um mundo externo real cujas 




propriedades são definidas e independente do observador que as percebe. De acordo com a 




ciência clássica, certos objetos existem e que têm propriedades físicas, tais como velocidade e 




massa, que têm valores bem definidos. Nessa visão, nossas teorias são tentativas de descrever 




os objetos e suas propriedades, e nossas medições e percepções correspondem a eles. Tanto o 




observador quanto o observado são partes de um mundo que tem uma existência objetiva, e 




qualquer distinção entre eles não tem qualquer significado significante. Em outras palavras, se 




você vê uma manada de zebras lutando por uma vaga no estacionamento, é porque realmente 




existe uma manada de zebras lutando por uma vaga no estacionamento. Todos os outros 




observadores que olham medirão as mesmas propriedades e o rebanho terá essas propriedades, 




se alguém os observa ou não. Na filosofia esta crença é chamada realismo. 




Embora o realismo possa ser um ponto de vista tentador, como veremos mais tarde, o que 




sabemos sobre a física moderna o torna uma questão difícil de defender. Por exemplo, de 




acordo com os princípios da física quântica, que é uma descrição precisa da natureza, uma 




partícula não tem nem uma posição definitiva, nem uma velocidade definida, a menos e até que 




essas quantidades sejam medidas por um observador. Portanto, não é correto dizer que uma 




medida dá um certo resultado, porque a quantidade sendo medida tinha aquele valor no 




momento da medição. De fato, em alguns casos, objetos individuais nem sequer têm uma 




existência independente, mas existem apenas como parte de um conjunto de muitos. E se uma 




teoria chamada princípio holográfico se provar correta, nós e nosso mundo em quatro 




dimensões podem ser sombras sobre o limite de um espaço-tempo maior, com cinco dimensões. 




Nesse caso, o nosso status no universo é semelhante ao do peixe dourado. 




Realistas estritos muitas vezes argumentam que prova de que as teorias científicas representam 




a realidade reside no seu sucesso. Mas, diferentes teorias podem com sucesso descrever o 




mesmo fenômeno através de diferentes modelos conceituais. Na realidade, muitas teorias 




científicas que tinham se provado bem sucedidas, foram mais tarde substituídas por outras 




teorias igualmente bem sucedidas baseadas em conceitos de realidade totalmente novos. 







Tradicionalmente, os que não aceitam o realismo tem sido chamado de anti-realistas. Anti-




realistas supõem uma distinção entre conhecimento empírico e conhecimento teórico. Eles 




geralmente argumentam que a observação e a experiência são significativos, mas que as teorias 




não são mais que instrumentos úteis que não incorporam qualquer verdade mais profunda 




subjacente aos fenômenos observados. Alguns anti-realistas até mesmo quiseram restringir a 




ciência às coisas que podem ser observadas. Por essa razão, muitos no século XIX rejeitaram a 




idéia de átomos com fundamento em que nunca iríamos ver um. George Berkeley (1685-1753) 




chegou mesmo a dizer que nada existe, exceto a mente e suas idéias. Quando um amigo 




comentou ao autor inglês e lexicógrafo Dr. Samuel Johnson (1709-1784) que a alegação de 




Berkeley não poderia ser refutada, diz-se que Johnson respondeu caminhando até uma grande 




pedra, chutando-a e proclamando, "Eu, assim, a refuto". Claro que a dor que o Dr. Johnson 




experimentou em seu pé era também uma idéia em sua mente, então ele não estava realmente 




refutar as idéias de Berkeley. Mas, seu ato ilustrou a visão do filósofo David Hume (1711-




1776), que escreveu que, embora não tenhamos motivos racionais para acreditar em uma 




realidade objetiva, também não temos escolha senão agir como se ela fosse verdadeira. 




Realismo dependentes de modelo corta esta discussão e argumentação entre as escolas realista e 




anti-realista de pensamento. 







"Vocês dois têm algo em comum. O Dr. Davis 




descobriu uma partícula que ninguém viu, e o 




Prof. Higbe descobriu uma galáxia ninguém 




viu". 







De acordo com o realismo dependente de modelo, não faz sentido perguntar se um modelo é 




real, só se está de acordo com a observação. Se existem dois modelos que concordam, ambos, 




com a observação, como a imagem do peixinho dourado e a nossa, então não se pode dizer que 




uma seja mais real que a outra. Pode-se usar o modelo que for mais conveniente na situação em 




questão. Por exemplo, se alguém estivesse dentro do aquário, a imagem do peixinho dourado 




seria útil, mas para aqueles do lado de fora, seria muito estranho descrever eventos de uma 




galáxia distante no quadro de um aquário na terra, especialmente porque o aquário estaria se 




movendo à medida que a Terra orbita o Sol e gira sobre seu eixo. 




Criamos modelos na ciência, mas também os criamos na vida cotidiana. O realismo dependente de modelo não se aplica apenas aos modelos científicos, mas também aos modelos mentais 




conscientes e subconscientes que todos nós criamos para interpretar e compreender o mundo 




cotidiano. Não há maneira de remover o observador - nós - de nossa percepção do mundo, que é 




criada através do nosso processamento sensorial e através da nossa maneira de pensar e 




raciocinar. A nossa percepção - e, portanto, as observações sobre as quais estão baseadas as 




nossas teorias - não é direta, mas é formada por uma espécie de lente, a estrutura interpretativa 




do nosso cérebro humano. 




O realismo dependentes de modelo corresponde à maneira como percebemos os objetos. Na 




visão, o cérebro recebe uma série de sinais através do nervo ótico. Esses sinais não constituem 




o tipo de imagem que você aceitaria em seu televisor. Há um ponto cego onde o nervo óptico se 




conecta à retina; a única parte do seu campo de visão com boa resolução é uma estreita área de 




cerca de 1 grau de ângulo visual em torno do centro da retina, uma área da largura do seu 




polegar quando mantido no comprimento do braço. E assim, os dados brutos enviados ao 




cérebro são como um retrato mal definido, com um buraco no meio. Felizmente, o cérebro 




humano processa estes dados, combinando a entrada de dados de ambos os olhos, preenchendo 




as lacunas existentes, presumindo que as propriedades visuais dos locais vizinhos são 




semelhantes e interpolando. Além disso, ele lê uma matriz bidimensional de dados a partir da 




retina e cria a partir dele a impressão de espaço tridimensional. O cérebro, em outras palavras, 




constrói uma imagem ou modelo mental. 




O cérebro é tão bom na construção de modelos que se as pessoas estiverem equipadas com 




óculos que viram as imagens em seus olhos de cabeça para baixo, seus cérebros, depois de um 




tempo, mudam o modelo de modo a que eles voltam a ver as coisas do jeito certo. Se os óculos 




são então removidos, elas vêem o mundo de cabeça para baixo por algum tempo, e então, se 




adaptam novamente. Isso mostra que o que se quer dizer quando se diz "Eu vejo uma cadeira" é apenas que se usou a luz espalhada pela cadeira para construir uma imagem ou modelo mental 




da cadeira. Se o modelo está de cabeça para baixo, com sorte o cérebro a corrigirá antes que 




alguém tente se sentar na cadeira. 




Outro problema que o realismo dependente de modelo resolve, ou pelo menos evita, é o sentido 




da existência. Como eu sei que uma mesa ainda existe se eu saio da sala e não posso vê-la? O 




que significa dizer que coisas que não podemos ver, tais como elétrons ou quarks - as partículas 




que se diz comporem o próton e o nêutron - existem? Poderíamos ter um modelo no qual a 




mesa desaparece quando eu saio da sala e reaparece na mesma posição quando eu retorno, mas 




isso seria estranho, e se alguma coisa acontecesse quando eu estivesse fora, por exemplo o teto 




cair? Como, sob o modelo mesa-desaparece-quando-eu-saio-da-sala, eu poderia explicar o fato 




de que na próxima vez que eu entro, a mesa reaparece quebrada, sob os escombros do teto? O 




modelo em que a mesa fica é muito mais simples e concorda com a observação. Isso é tudo que 




alguém pode perguntar. 




No caso de partículas subatômicas que não podemos ver, os elétrons são um modelo útil, que 




explica observações, tais como trillhas em uma câmara de nuvem e os pontos de luz em um 




tubo de televisão, assim como muitos outros fenômenos. Diz-se que o elétron foi descoberto em 




1897 pelo físico britânico JJ Thomson no Laboratório Cavendish da Universidade de 




Cambridge. Ele estava experimentando com correntes de eletricidade dentro de tubos de vidro 




vazio, um fenômeno conhecido como raios catódicos. Suas experiências o levaram à conclusão 







ousada de que os raios misteriosos foram compostas por minúsculos "corpúsculos" que eram 




constituídos materialmente de átomos, que se pensava então serem a unidade indivisível 




fundamental da matéria. Thomson não "viu" um elétron, nem foi sua especulação direta ou 




inequivocamente demonstrada por seus experimentos. Mas, o modelo se revelou crucial em 




aplicações da ciência fundamental à engenharia, e hoje todos os físicos crêem em elétrons, 




mesmo que você não os possa ver. 







Raios Catódicos

 - Não podemos ver os elétrons individualmente, mas podemos ver os efeitos 




que produzem. 







Quarks, que também não podemos ver, são um modelo para explicar as propriedades dos 




prótons e nêutrons no núcleo de um átomo. Embora se diga que prótons e nêutrons sejam feitos 




de quarks, nunca observaremos um quark porque a força de ligação entre quarks aumenta com a 




separação e, portanto, isolado, quarks livres não podem existir na natureza. Em vez disso, eles 




sempre ocorrem em grupos de três (prótons e nêutrons), ou em pares de um quark e um 




antiquark (mésons pi), e se comportam como se estivessem unidos por tiras de borracha. 




A questão se faz sentido dizer que os quarks existem realmente, se você nunca pode isolar um 




deles foi um assunto controverso nos anos após o modelo dos quarks ter sido proposto pela 




primeira vez. A idéia de que certas partículas eram constituídas de diferentes combinações de 




algumas partículas sub-subnucleares ofereceu um princípio organizador que rendeu uma 




explicação simples e atraente para suas propriedades. Mas, embora os físicos estivessem 




habituados a aceitar partículas cuja existência era apenas inferida a partir de blips estatístico em dados referentes à dispersão de outras partículas, a idéia de atribuir realidade a uma partícula 




que podia ser, em princípio, não observável era demais para muitos físicos. Ao longo dos anos, 




no entanto, à medida que o modelo do quark levou a previsões cada vez mais corretas, a 




oposição desapareceu. É certamente possível que alguns seres alienígenas com dezessete 




braços, olhos infravermelhos, e um hábito de soprar creme talhado de suas orelhas fariam as 




mesmas observações experimentais que fazemos, mas as descreveriam sem quarks. No entanto, 




de acordo com o realismo dependente de modelo, os quarks existem em um modelo que está de 




acordo com nossas observações de como as partículas sub-nucleares se comportam. 







O realismo dependente de modelo pode fornecer um quadro para discutir questões como: Se o 




mundo foi criado há um tempo finito, o que aconteceu antes daquilo? Um filósofo cristão, 




Santo Agostinho (354-430), disse que a resposta não era que Deus estava preparando o inferno 




para as pessoas que fazem tais perguntas, mas que o tempo era uma propriedade do mundo que 




Deus criou, e que o tempo não existia antes da criação, que ele acreditava tinha ocorrido não 




fazia muito tempo. Esse é um modelo possível, que é favorecida por aqueles que sustentam que 




a explicação dada no Gênesis é literalmente verdadeira, embora o mundo contenha evidências 




fósseis e outras evidências que o fazem parecer muito mais velho. (Eles foram colocados ali 




para nos enganar?) Pode-se também ter um modelo diferente, em que o tempo volta 13,7 




bilhões anos até o big bang. O modelo que explica mais sobre nossas observações atuais, 




incluindo as provas históricas e geológicas, é a melhor representação que temos do passado. O 




segundo modelo pode explicar os registros fósseis e radioativos e o fato de que recebemos a luz 




de galáxias milhões de anos-luz longe de nós e, portanto, este modelo - a teoria do Big Bang - é 




mais útil que o primeiro. Ainda assim, não se pode dizer que qualquer um dos modelos é mais 




real que o outro. 













Quarks - O conceito de quarks é um elemento vital na nossas 




teorias da física fundamental, embora quarks individuais não 




possam ser observados. 










Algumas pessoas defendem um modelo em que o tempo retrocede ainda mais que o big bang. 




Ainda não está claro se um modelo em que o tempo retrocede além do "big bang" seria melhor para explicar a apresentar observações, porque parece que as leis da evolução do universo 




podem se desintegrar no big bang. Se isso acontecer, não faria qualquer sentido criar um 




modelo que englobe o tempo antes do big bang, porque o que existia então não teria 




consequências observáveis para o presente e, assim, podemos também nos prender à ideia de que o big bang foi o criação do mundo. 




Um modelo é um bom modelo, se: 




1.  É elegante 




2.  Contém poucos elementos arbitrários ou ajustáveis 




3.  Está de acordo com, e explica todas as observações já existentes 




4.  Faz previsões detalhadas sobre futuras observações que podem refutar ou falsificar o 




modelo, se não forem confirmadas. 




Por exemplo, a teoria de Aristóteles de que o mundo era composto de quatro elementos, terra, 




ar, fogo e água, e que os objetos agiam para cumprir suas finalidades era elegante e não 




continha elementos ajustáveis. Mas, em muitos casos, ela não fazia previsões definitivas, e 




quando o fazia, as previsões não estavam sempre de acordo com a observação. Uma dessas 




previsões era a de que objetos mais pesados deviam cair mais rápido, pois sua finalidade é cair. 




Ninguém parecia ter pensado que era importante testar isso até Galileu. Há uma história de que 




ele testou a questão deixando cair pesos da Torre Inclinada de Pisa. Isto é provavelmente 




apócrifo, mas sabemos que ele rolou diferentes pesos abaixo em um plano inclinado e observou 




que todos eles se ganhavam velocidade à mesma taxa, contrariamente à previsão de Aristóteles. 




Os critérios acima são, obviamente, subjetivos. Elegância, por exemplo, não é algo facilmente 




mensurável, mas é altamente valorizada entre os cientistas, pois as leis da natureza são 




destinadas a comprimir economicamente uma série de casos particulares em uma fórmula 




simples. A elegância se refere à forma de uma teoria, mas está intimamente relacionada à falta 




de elementos ajustáveis, uma vez que uma teoria repleta de fatores de correção não é muito 




elegante. Parafraseando Einstein, uma teoria deve ser tão simples quanto possível, mas não 




mais simples. Ptolomeu acrescentou epiciclos à órbitas circulares dos corpos celestes, a fim de 




que este modelo pudesse descrever com precisão os seus movimentos. O modelo poderia ter 




sido mais precisa adicionando epiciclos à epiciclos, ou até mesmo epiciclos a estes epiciclos. 




Embora complexidade adicional possa tornar o modelo mais preciso, os cientistas vêem um 




modelo que é contorcido para corresponder a um conjunto específico de observações como 




insatisfatório, mais um catálogo de dados do que uma teoria com probabilidade de incorporar 




um princípio útil. 




Veremos no capítulo 5, que muitas pessoas vêem o "modelo padrão", que descreve as 




interações das partículas elementares da natureza, como deselegante. Esse modelo é muito mais 




bem-sucedido que os epiciclos de Ptolomeu. Ele previu a existência de diversas novas 




partículas antes que elas fossem observadas, e descreveu o resultado de inúmeras experiências 




por várias décadas com grande precisão. Mas ela contém dezenas de parâmetros ajustáveis, 




cujos valores devem ser fixados para coincidir com as observações, ao invés de serem 




determinados pela própria teoria. 




Quanto ao quarto ponto, os cientistas ficam sempre impressionado quando as previsões novas e 




impressionantes se provam corretas. Por outro lado, quando um modelo é considerado 




incompleto, uma reação comum é dizer que o experimento estava errado. Se isso não provar ser 




o caso, as pessoas ainda, muitas vezes, não abandonam o modelo, mas tentam salvá-lo através 




de modificações. Embora os físicos sejam verdadeiramente tenazes em suas tentativas de 




resgatar as teorias que eles admiram, a tendência para modificar uma teoria desaparece à medida que as alterações se tornam artificiais ou confusas e, portanto, "deselegantes". 




Se as modificações necessárias para acomodar novas observações se tornam demasiado 




barrocas, isso sinaliza a necessidade de um novo modelo. Um exemplo de um modelo antigo 




que cedeu sob o peso de novas observações foi a idéia de um universo estático. Na década de 




20, a maioria dos físicos acreditava que o Universo era estático ou imutável em tamanho. Então, 




em 1929, Edwin Hubble publicou suas observações mostrando que o universo está se 




expandindo. Mas, Hubble não observou diretamente a expansão do Universo. Ele observou a 




luz emitida por galáxias. Aquela luz possui uma assinatura característica, ou espectro, com base 




na composição de cada galáxia, que se altera segundo uma quantidade conhecida, se a galáxia 




estiver se movendo em relação a nós. Portanto, analisando os espectros de galáxias distantes, 




Hubble foi capaz de determinar suas velocidades. Ele esperava encontrar tantas galáxias se 




afastando de nós quanto galáxias movendo-se em nossa direção. Ao invés disso, ele descobriu 




que quase todas as galáxias estavam se afastando de nós, e quanto mais longe elas estavam, 




mais rápido estavam se movendo. Hubble concluiu que o universo está se expandindo, mas 




outros, tentando apegar-se ao modelo anterior, tentaram explicar suas observações dentro do 




contexto do universo estático. Por exemplo, físico da Caltech, Fritz Zwicky, sugeriu que, por 




algum motivo ainda desconhecido, a luz podia perder energia lentamente, à medida que viaja 




grandes distâncias. Esta diminuição da energia corresponderia a uma mudança no espectro da 




luz, que Zwicky sugeriu poderia imitar as observações de Hubble. Durante décadas depois do 




Hubble, muitos cientistas continuaram a se apegar à teoria do estado estacionário. Mas, o 




modelo mais natural era o de Hubble, de um universo em expansão, e ele veio a ser aquele 




aceito. 







Em nossa busca para encontrar as leis que regem o universo, formulamos uma série de teorias 




ou modelos, tais como a teoria dos quatro elementos, o modelo de Ptolomeu, a teoria do 




flogisto, a teoria do big bang, e assim por diante. A cada teoria ou modelo, nossos conceitos de 




realidade e os componentes fundamentais do universo foram alterados. Por exemplo, 




consideremos a teoria da luz. Newton pensava que a luz era composta de pequenas partículas 




ou corpúsculos. Isso explicaria por que a luz viaja em linha reta, e Newton também usou isso 




para explicar por que a luz é desviada ou refratada ao passar de um meio para outro, por 




exemplo, do ar para o vidro ou do ar para a água. 







Refração - o modelo de luz de Newton podia explicar por que a luz se desvia quando passa de 




um meio para outro, mas não podia explicar outro fenômeno que hoje chamamos hoje de anéis 




de Newton. 










A teoria do corpúsculo não podia, entretanto, ser usada para explicar um fenômeno que o 




próprio Newton observou, que é conhecido como anéis de Newton. Coloque uma lente em uma 




placa reflectora plana, e ilumine-a com a luz de cor única, por exeplo uma luz de sódio. 




Olhando de cima para baixo, ver-se-á uma série de anéis de luz e sobra centrados no ponto onde 




a lente toca a superfície. Isto seria difícil de explicar com a teoria corpuscular da luz, mas pode 




ser atribuído à teoria das ondas. 




Segundo a teoria ondulatória da luz, os anéis de luz e sombra são causadas por um fenômeno 




chamado interferência. Uma onda, por exemplo uma onda de água, consiste de uma série de 




cristas e vales. Quando as ondas se chocam, se acontece de as cristas e vales corresponder, elas 




reforçam-se mutuamente, gerando uma onda maior. Isso é chamado de interferência 




construtiva. Nesse caso, diz-se que as ondas estão "em fase". No outro extremo, quando as 




ondas se encontram, as cristas de uma onda podem coincidir com os vales das outras. Nesse 







caso as ondas se cancelam mutuamente e se dz que estão "fora de fase". Essa situação é 




chamada interferência destrutiva. 




Nos anéis de Newton, os anéis luminosos estão localizados a distâncias do centro onde a 




separação entre a lente e a placa reflectora é tal que a onda refletida na lente difere da onda 




refletida a partir da placa, à razão de um número inteiro (1, 2, 3, ...) de comprimentos de onda, 




criando interferência construtiva. (A onda é a distância entre uma crista ou vale de uma onda 




até a outra.) Os anéis de sombra, por outro lado, estão localizados a distâncias do centro onde a 




separação entre as duas ondas refletidas é um número é um número semi-inteiro (½, 1 ½, 2 ½, 




...) de comprimentos de onda, causando interferência destrutiva - a onda refletida da lente 




cancela a onda refletida da placa. 







Interferência 

- Como as pessoas, quando as ondas se encontram, elas podem tender a 




aumentar ou de diminuir a outra. 







No século XIX, isto foi tomado como uma confirmação da teoria ondulatória da luz, mostrando 




que a teoria das partículas estava errada. No entanto, no início do século XX, Einstein mostrou 




que o efeito fotoelétrico (usado agora na televisão e câmeras digitais) pode ser explicado por 







uma partícula ou quantum de luz atingindo um átomo e neutralizando um elétron. Assim, a luz 




se comporta tanto como partícula quanto onda. 




O conceito de ondas provavelmente entrou pensamento humano, porque as pessoas observavam 




o oceano, ou uma poça d'água depois que uma pedra caia nela. Na verdade, se você alguma vez 




derrubou duas pedras em uma poça d'água, provavelmente já viu interferência funcionar, como 




na imagem acima. Observou-se que outros líquidos se comportavam de forma semelhante, 




exceto, talvez, o vinho, se você já tomou demais. A idéia de partículas era familiar a partir de 




rochas, pedras e areia. Mas esta dualidade onda/partícula - a idéia de que um objeto possa ser 




descrito tanto como partícula quanto como onda - é tão estranha à experiência cotidiana quanto 




a idéia de que você pode beber um pedaço de pedra. 







Interferência na poça d'água 

- O conceito de interferência aparece na vida cotidiana em 




corpos de água, de poças d'água a oceanos. 







Dualidades como esta - situações em que duas teorias muito diferentes descrevem com precisão 




o mesmo fenômeno - são consistentes com realismo dependente de modelo. Cada teoria pode 




descrever e explicar determinadas propriedades, e nenhuma das teorias pode ser considerada 




melhor ou mais real que a outra. No que diz respeito às leis que regem o universo, o que 




podemos dizer é isto: Não parece haver algum modelo matemático ou teoria que possa 




descrever todos os aspectos do universo. Em vez disso, conforme mencionado no capítulo de 




abertura, parece existir a rede de teorias chamada teoria-M. Cada teoria na rede da Teoria-M é 




boa para descrever os fenômenos dentro de um determinado intervalo. Onde quer que seus 




domínios se sobreponham, as diferentes teorias da rede estão de acordo; assim, pode-se dizer 




que são partes da mesma teoria. Mas, nenhuma teoria isolada dentro da rede pode descrever 




todos os aspectos do universo - todas as forças da natureza, as partículas que sofrrem essas 




forças, e o quadro geral de espaço e tempo em que tudo se desenrola. Embora esta situação não 




atenda ao sonho dos físicos tradicionais de uma teoria unificada, ela é aceitável dentro do 




quadro do realismo dependente de modelo. 




Discutiremos dualidade e Teoria-M com mais detalhes no Capítulo 5, mas antes disso voltemo-nos para um princípio fundamental sobre o qual se baseia a nossa visão moderna da natureza: a 




teoria quântica e, em particular, a abordagem da teoria quântica chamada de histórias 




alternativas. Nessa visão, o universo não tem apenas uma única existência ou história, mas, ao 




invés, todas as versões possíveis do universo existem, simultaneamente, no que é chamado de 




superposição quântica. Isso pode soar tão escandaloso quanto a teoria de que a mesa desaparece 




sempre que deixamos a sala, mas neste caso a teoria passou por todos os testes experimentais 




aos quais ela já foi submetida. 




4 




HISTÓRIAS ALTERNATIVAS 










EM 1999, UMA EQUIPE DE FÍSICOS na Áustria disparou uma série de moléculas em forma 




de bola de futebol em direção a uma barreira. Essas moléculas, cada uma delas formada por 




sessenta átomos de carbono, algumas vezes chamadas de "buckyballs" porque o arquiteto 




Buckminster Fuller construía prédios nessa forma. Os domos geodésicos de Fuller foram 




provavelmente os maiores objetos em forma de bola de futebol já existentes. As "buckyballs" 




foram as menores. A barreira contra a qual os cientistas atiraram tinha, com efeito, duas fendas 




através das quais as buckyballs podiam passar. Além da barreira, os físicos colocaram o 




equivalente a uma tela para detectar e contar as moléculas que saiam das fendas. 







Buckyballs 

- Buckyballs são como bolas de futebol microscópicas feitas de átomos de 




carbono. 







Se fôssemos criar uma experiência análoga com bolas de futebol reais, seria necessário um 




jogador com um chute um tanto instável, mas com a habilidade de lançar as bolas de forma 




consistente a uma velocidade de nossa escolha. Posicionaríamos este jogador diante de um 




muro no qual haveria duas aberturas. Do outro lado do muro, e paralelo a ele, colocaríamos 




uma rede muito longa. A maioria dos chutes do jogador bateria na parede e voltaria, mas 




algumas bolas passariam por uma ou outra abertura e cairiam na rede. Se as aberturas fossem 




apenas ligeiramente maiores que as bolas, duas correntes altamente colimadas sairiam do outro 




lado. Se as falhas fossem um pouco mais largas, cada fluxo de bolas variaria um pouco, 




conforme mostra a figura abaixo. 




Observe que se fecharmos uma das aberturas, o fluxo correspondente de bolas não mais 




passaria, mas isso não teria qualquer efeito sobre o outro fluxo de bolas. Se reabríssemos a 




segunda abertura, isso só iria aumentar o número de bolas que caem em qualquer lugar do outro 




lado, porque, então, obteríamos todas as bolas que passassem pela abertura que havia ficado 




aberta, mais as outras bolas provenientes da abertura recém-reaberta. O que observamos com as 




duas aberturas abertas, em outras palavras, é a soma do que observamos em cada brecha do 







muro separadamente. Essa é a realidade a que estamos acostumados na vida cotidiana. Mas, não 




foi isso que os pesquisadores austríacos encontraram quando dispararam suas moléculas. 































Futebol de duas aberturas 

- Um jogador de futebol chutando bolas contra aberturas em uma 




parede produzem um padrão óbvio 







No experimento austríaco, a abertura da segunda brecha, de fato, aumentar o número de 




moléculas que chegam em alguns pontos na tela - mas diminuiu o número de outras, como 




mostra a figura abaixo. Na verdade, havia lugares onde nenhuma buckyballs caiu quando ambas 




as fendas estavam abertas, mas onde bolas cairam quando apenas uma ou a outra fenda estava 




aberta. Isso parece muito estranho. Como pode a abertura de uma segunda fenda fazer com que 




menos moléculas caiam em determinados pontos? 



















Futebol de Buckyball 

- Quando as bolas de futebol moleculares são disparadas contra fendas 




em uma tela, o padrão resultante reflete leis quânticas desconhecidas. 







Podemos ter uma pista para a resposta ao examinar os detalhes. No experimento, muitas das 




bolas de futebol moleculares caíram em um ponto central a meio caminho entre onde você 




esperaria que elas caíssem, se as bolas passaram por uma ou por outra abertura. Um pouco mais 




afastado dessa posição central, poucas moléculas chegaram, mas um pouco mais longe do 




centro que isso, observou-se que as moléculas novamente chegavam. Esse padrão não é a soma 




dos padrões formados quando cada fenda é aberta separadamente, mas você pode reconhecê-lo 




a partir do capítulo 3, como o padrão característico de ondas interferentes. As áreas onde 




nenhuma moléculas chegou correspondem às regiões em que as ondas emitidas a partir das 




duas fenda chegam fora de fase e criam uma interferência destrutiva; as áreas onde muitas 




moléculas chegam correspondem a regiões onde as ondas chegam em fase e criam interferência 




construtiva. 




Nos primeiros dois mil e tantos anos de pensamento científico, a experiência comum e a 




intuição eram a base para a explicação teórica. À medida que melhoramos nossa tecnologia e 




ampliamos o leque de fenômenos que podíamos observar, começamos a encontrar a natureza 




comportando-se de maneiras que estavam cada vez menos em linha com a nossa experiência 




cotidiana e, portanto, com nossa intuição, conforme foi evidenciado pela experiência com 




buckyballs. Essa experiência é típica do tipo de fenômeno que não pode ser abrangido pela 




ciência clássica, mas é descrito pelo que chamamos física quântica. De fato, Richard Feynman 




escreveu que a experiência da dupla fenda como aquela descrita acima "contém todo o mistério 




da mecânica quântica." 




Os princípios da física quântica foram desenvolvidos nos primeiros decênios do século XX, 




após a teoria de Newton ter sido considerada inadequada para a descrição da natureza em nível 




atômico ou subatômico. As teorias fundamentais da física descrevem as forças da natureza e 




como os objetos reagem a elas. Teorias clássicas, como as de Newton estão construídas sobre 




um quadro que reflete a experiência cotidiana, onde os objetos materiais têm uma existência 




individual, podem ser localizados em locais definidos, seguem caminhos definidos, e assim por diante. A física quântica fornece uma estrutura para a compreensão de como a natureza opera 




em escalas atômica e subatômica, mas como veremos em detalhes mais adiante, isso dita um 




esquema conceitual completamente diferente, no qual um a posição e caminho de um objeto, e 




até mesmo seu passado e futuro não são precisamente determinados. As teorias quânticas de 




forças tais como a gravidade ou a força eletromagnética são construídas dentro desse quadro. 




Podem as teorias construídas em cima de um quadro tão estranho à experiência cotidiana 




também explicar os eventos da experiência comum, que foram modelados com tanta precisão 




pela física clássica? Elas podem, porque nós e nosso ambiente são estruturas compostas, feitas 




de um número inimaginavelmente grande de átomos, mais átomos do que há estrelas no 




universo observável. E embora os átomos componentes obedeçam aos princípios da física 




quântica, pode-se mostrar que as grandes montagens que formam bolas de futebol, nabos, e 




jatos jumbo - assim como nós - certamente conseguem evitar serem difratadas através de 




fendas. Assim, embora os componentes de objetos do cotidiano obedeçam à física quântica, as 




leis de Newton formam uma teoria eficaz que descreve com muita precisão a maneira como se 




comportam as estruturas compostas que formam o nosso mundo cotidiano. 




Isso pode soar estranho, mas há muitos exemplos na ciência onde uma montagem grande parece 




se comportar de uma maneira que é diferente do comportamento de seus componentes 




individuais. As respostas de um único neurônio dificilmente representam as do cérebro 




humano, nem saber sobre uma molécula de água lhe diz muito sobre o comportamento de um 




lago. No caso da física quântica, os físicos ainda estão trabalhando para descobrir os detalhes de 




como as leis de Newton emergem do domínio quântico. O que sabemos é que os componentes 




de todos os objetos obedecem às leis da física quântica, e as leis de Newton são uma boa 




aproximação para descrever o modo como os objetos macroscópicos feitos daqueles 




componentes quânticos se comportam. 




As previsões da teoria newtoniana, portanto, correspondem à visão da realidade que todos nós 




desenvolvemos à medida que experimentamos o mundo que nos rodeia. Mas os átomos e 




moléculas individuais funcionam de uma forma profundamente diferente da nossa experiência 




cotidiana. A Física Quântica é um novo modelo de realidade que nos dá um retrato do universo. 




Este é um quadro em que muitos conceitos fundamentais para nossa compreensão intuitiva da 




realidade já não têm significado. 




A experiência da dupla fenda foi realizada pela primeria vez em 1927 por Clinton Davisson e 




Lester Germer, físicos experimentais na Bell Labs, que estavam estudando como um feixe de 




elétrons - objetos muito mais simples do que as buckyballs - interage com um cristal feito de 




níquel. O fato de que partículas de matéria, tais como os elétrons se comportam como ondas de 




água era o tipo de experiência surpreendente que inspirou a física quântica. Uma vez que este 




comportamento não é observado em escala macroscópica, os cientistas há muito se 




perguntavam apenas como algo grande e complexo poderia existir e ainda assim exibir tais 




propriedades ondulatórias. Causaria uma grande celeuma se o efeito pudesse ser demonstrado 




através de pessoas ou um hipopótamo, mas como dissemos, em geral, quanto maior for o 




objeto, menos aparentes e robustos são os efeitos quânticos. Portanto, é improvável que 




qualquer animal do jardim zoológico estará passando como uma onda através das grades de 




suas jaulas. Ainda assim, os físicos experimentais observaram o fenômeno da onda com 




partículas de tamanho cada vez maior. Os cientistas esperam reproduzir o experimento 







buckyball algum dia usando um vírus, que não só é muito maior, mas também é considerado 




por alguns como uma coisa viva. 




Há apenas alguns aspectos da física quântica necessários para se entender os argumentos que 




faremos em próximos capítulos. Uma das principais características é dualidade onda / partícula. 




Que partículas de matéria se comportassem como uma onda surpreendeu todo mundo. Que a 




luz se comporte como uma onda não surpreende mais ninguém. O comportamento ondulatório 




da luz parece natural para nós, e tem sido considerado um fato aceito por quase dois séculos. Se 




você projeta um feixe de luz sobre as duas fendas no experimento anterior, duas ondas 




emergirão e se encontrarão na tela. Em alguns pontos, suas cristas ou vales coincidirão e 




constituirão um ponto brilhante; em outros as cristas de um raio encontrarão os vales do outro, 




anulando-os e deixando uma área escura. O físico inglês Thomas Young realizou esse 




experimento no início do século XIX, convencendo as pessoas de que a luz era uma onda e não, 




como Newton acreditara, composto de partículas. 






















Experiência de Young 

- O padrão buckyball era familiar a partir da teoria ondulatória da luz. 







Embora se possa concluir que Newton estava errado ao dizer que a luz não era uma onda, ele 




estava certo quando disse que a luz pode agir como se ela fosse composta de partículas. Hoje 




nós os chamamos de fótons. Assim como nós somos compostos por um grande número de 




átomos, a luz que vemos na vida cotidiana é composta no sentido de que ela é feita de um 




grande número de fótons - até mesmo uma luz de cabeceira de um watt emite um bilhão de 




bilhões a cada segundo. Fótons isolados geralmente não são evidentes, mas no laboratório podemos produzir um feixe de luz tão fraco que consiste em um fluxo de fótons individuais, 




que podemos detectar como indivíduos da mesma maneira que podemos detectar elétrons 




individuais ou buckyballs. E podemos repetir a experiência de Young empregando um feixe 




suficientemente esparso que os fótons atingem a barreira um de cada vez, com alguns poucos 




segundos entre cada chegada. Se fizermos isso, e em seguida somar todos os impactos 




individuais gravados pela tela do outro lado da barreira, descobrimos que, juntos, eles 




constroem o mesmo padrão de interferência que seria construído se realizássemos o 




experimento de Davisson-Germer, mas disparassemos os elétrons (ou buckyballs) contra a tela 




um de cada vez. Para os físicos, esta foi uma revelação surpreendente: Se as partículas 




individuais interferem entre si, então a natureza ondulatória da luz é a propriedade não só de um 




feixe ou de uma grande coleção de fótons, mas das partículas individuais. 




Outro dos principais dogmas da física quântica é o princípio da incerteza, formulado por 




Werner Heisenberg em 1926. O princípio da incerteza nos diz que há limites para nossa 




capacidade de medir simultaneamente certos dados, tais como a posição e a velocidade de uma 




partícula. De acordo com o princípio da incerteza, por exemplo, se você multiplicar a incerteza 




na posição de uma partícula pela incerteza em seu momento (sua massa multiplicada por sua 




velocidade), o resultado nunca pode ser menor do que uma certa quantidade fixa, chamada 




constante de Planck. Isso é um trava-língua, mas sua essência pode ser afirmada de maneira 




simples: Quanto mais precisamente você medir a velocidade, menos precisamente você pode 




medir a posição, e vice-versa. Por exemplo, se você reduzir à metade a incerteza na posição, 




você precisa dobrar a incerteza na velocidade. Também é importante notar que, em comparação 




com as unidades cotidianas de medição, tais como metros, quilos e segundos, a constante de 




Planck é muito pequena. De fato, se relatado nessas unidades, ele tem o valor de cerca de 




6/10,000.000.000.000.000.000.000.000.000.000.000. Como resultado, se você localizar um 




objeto macroscópico como uma bola de futebol, com uma massa de um terço de um quilo 




dentro de um milímetro em qualquer direção, ainda podemos medir sua velocidade com uma 




precisão muito maior que até mesmo um bilionésimo de um bilionésimo de um bilionésimo de 




um quilômetro por hora. Isso ocorre porque, medido nessas unidades, a bola de futebol tem 




uma massa de 1/3, e a incerteza na posição é 1/1, 000. Nenhuma delas é suficiente para explicar 




todos aqueles zeros na constante de Planck, e assim aquele papel cai para a incerteza na 




velocidade. Mas nas mesmas unidades, um elétron tem uma massa de 




0,000000000000000000000000000001, portanto, para os elétrons, a situação é bem diferente. 




Se medirmos a posição de um elétron com uma precisão correspondente a aproximadamente o 




tamanho de um átomo, o princípio da incerteza dita que não podemos saber a velocidade do 




elétron, mais precisamente do que cerca de mais ou menos 1.000 quilômetros por segundo, o 




que absolutamente não é muito preciso. 










"Se isso estiver correto, então tudo o que pensávamos que era uma onda é realmente uma 




partícula, e tudo o que pensávamos ser uma partícula é realmente uma onda". 










Segundo a física quântica, não importa o quanta informação obtemos e quão poderosas as 




nossas capacidades de computação, os resultados dos processos físicos não podem ser previstos 




com certeza, porque eles não são determinados com certeza. Em vez disso, dado o estado inicial 




de um sistema, a natureza determina seu estado futuro, através de um processo que é 




fundamentalmente incerto. Em outras palavras, a natureza não dita o resultado de qualquer 




processo ou experiência, mesmo na mais simples das situações. Em vez disso, ela permite uma 




série de eventualidades diferentes, cada uma delas com uma certa probabilidade de ser 




realizada. Isso é, parafraseando Einstein, como se Deus jogasse os dados antes de decidir o 




resultado de cada processo físico. Essa idéia incomodava Einstein, e assim, embora ele fosse 




um dos pais da física quântica, mais tarde ele se tornou um crítico dela. 




A física quântica pode parecer minar a idéia de que a natureza é regida por leis, mas não é esse 




o caso. Em vez disso, ela nos leva a aceitar uma nova forma de determinismo: Dado o estado de 




um sistema em algum momento, as leis da natureza determinam as probabilidades de diversos 




futuros e passados, ao invés de determinar o futuro e o passado com certeza. Embora seja 




desagradável para alguns, os cientistas devem aceitar teorias que concordam com o 




experimento, não suas próprias noções preconcebidas. 




O que a ciência exige de uma teoria é que ela seja testável. Se a natureza probabilística das 




previsões da física quântica significa que é impossível confirmar essas previsões, então as 




teorias quânticas não se qualificariam como teorias válidas. Mas, apesar de a natureza 




probabilística das suas previsões, ainda assim podemos testar as teorias quânticas. Por exemplo, 




podemos repetir uma experiência muitas vezes e confirmar que a freqüência de vários 




resultados está em conformidade com as probabilidades previstas. Considere o experimento 




buckyball. A física quântica nos diz que nada é jamis localizado em um ponto definido, porque 




se fosse, a incerteza no momentum teria que ser infinita. De fato, segundo a física quântica, 




cada partícula tem alguma probabilidade de ser encontrada em qualquer lugar do universo. 




Assim, mesmo que as chances de encontrar um determinado elétron dentro do aparelho de 




dupla fenda sejam muito elevadas, sempre haverá alguma chance de que ele poderia ser 




encontrado, ao invés, no outro lado da estrela Alfa Centauro, ou na torta do pastor na cafeteria 




de seu escritório. Como resultado, se você chutar uma buckyball quântica e deixá-la voar, 




nenhuma quantidade de habilidade ou conhecimento permitirá que você diga de antemão 




exatamente onde vai cair. Mas, se você repetir essa experiência várias vezes, os dados que você 




obtém refletirão a probabilidade de encontrar a bola em várias posições, e pesquisadores 




confirmaram que os resultados de tais experiências concordam com as previsões da teoria. 




É importante perceber que as probabilidades na física quântica não são como as probabilidades 




na física newtoniana, ou na vida cotidiana. Podemos entender isso comparando os padrões 




criados pelo fluxo constante de buckyballs atirados contra uma tela ao padrão de furos 




construído por jogadores atirando contra um alvo de dardos. A não ser que os jogadores tenha 




consumido muita cerveja, as chances de um dardo cair perto do centro são maiores, e menores à 




medida que você se afasta. Tal como acontece com as buckyballs, qualquer dardo determinado 




pode cair em qualquer lugar, e ao longo do tempo um padrão de furos que reflete as 




probabilidades subjacentes surgirá. Na vida cotidiana podemos refletir aquela situação dizendo 




que um dardo tem uma certa probabilidade de cair em vários pontos; mas se dissermos isso, 




diferentemente do caso das buckyballs ou fulerenos, é só porque o nosso conhecimento das 




condições do seu lançamento é incompleto . Poderíamos melhorar nossa descrição, se 




soubéssemos exatamente a maneira em que o jogador lançou o dardo, o seu ângulo, rotação, 




velocidade, e assim por diante. Em princípio, então, poderíamos prever onde o dardo cairia com 




uma precisão tão grande quanto desejássemos. Nosso uso de termos probabilísticos para 




descrever o resultado de acontecimentos na vida cotidiana é, portanto, um reflexo, não da 




natureza intrínseca do processo, mas apenas de nossa ignorância sobre certos aspectos dela. 




As probabilidades na teoria quântica são diferentes. Elas refletem uma aleatoriedade 




fundamental por natureza. O modelo quântico da natureza engloba princípios que contradizem 




não apenas a nossa experiência cotidiana, mas nosso conceito intuitivo da realidade. Aqueles 




que acham esses princípios estranhos ou difíceis de acreditar estão em boa companhia, a 




companhia de grandes físicos tais como Einstein e até mesmo Feynman, cuja descrição da 




teoria quântica apresentaremos em breve. Na verdade, Feynman escreveu certa vez: "Eu acho 




que posso dizer com segurança que ninguém entende a mecânica quântica." Mas a física 




quântica concorda com a observação. Ela nunca falhou um teste, e ela foi testada mais do que 




qualquer outra teoria científica. 




Na década de 40, Richard Feynman teve um insight surpreendente a respeito da diferença entre 




os mundos quântico e newtoniano. Feynman ficou intrigado com a questão de como surge o 




padrão de interferência na experiência de dupla fenda. Lembre-se que o padrão que 




encontramos quando disparamos as moléculas com as duas fendas abertas não é a soma dos 




padrões que encontramos quando executamos a experiência duas vezes, uma com apenas uma 




fenda aberta, e outra vez só com a outra aberta. Em vez disso, quando as duas fendas estão 




abertas, encontramos uma série de bandas claras e escuras, sendo esta última as regiões onde 




nenhuma partícula caiu. Isso significa que as partículas que teria caído na área da faixa escura 




se, digamos, apenas uma fenda estava aberta, não cai ali quando a fenda dois também está 




aberta. É como se, em algum lugar em sua viagem da fonte até a tela, as partículas adquirissem 




informações sobre as duas fendas. Esse tipo de comportamento é drasticamente diferente da forma como as coisas parecem se comportar na vida cotidiana, onde a bola iria seguir um 




caminho através de uma das fendas e não seria afetada pela situação da outra. 




De acordo com a física newtoniana - e a forma como o experimento funcionaria se o 




realizássemos com bolas de futebol em vez de moléculas - cada partícula segue um único 




percurso bem definido desde sua fonte até a tela. Não há lugar neste quadro para um desvio em 




que a partícula visita a vizinhança de cada fenda ao longo do caminho. De acordo com o 




modelo quântico, porém, diz-se que a partícula não têm posição definida durante o tempo em 




que está entre o ponto de partida e o ponto final. Feynman percebeu que não se tem que 




interpretar isso como se significasse que as partículas não tomam qualquer caminho ao viajar 




entre a fonte e a tela. Isso pode significar, ao invés, que as partículas tomam todos os caminhos 




possíveis ligando esses pontos. Isto, Feynman afirmava, é o que torna a física quântica diferente 




da física newtoniana. A situação em ambas as fendas importa, porque, ao invés de seguir um 




único caminho definido, as partículas tomar todos os caminhos, e elas os percorrem todos ao 




mesmo tempo! Isso soa como ficção científica, mas não é. Feynman formulou uma expressão 




matemática - a soma de Feynman sobre histórias - que reflete essa idéia e reproduz todas as leis 




da física quântica. Na teoria de Feynman a matemática e a imagem física são diferentes da 




formulação original da física quântica, mas as previsões são as mesmas. 




No experimento de dupla-fenda, as idéias de Feynman significam que as partículas seguem 




caminhos que passam por apenas uma fenda ou somente a outra; caminhos que passam através 




da primeira fenda voltam através da segunda fenda e em seguida passm novamente pela 




primeira; os caminhos que visitam o restaurante que serve o excelente curry de camarão e, em 




seguida, circula Júpiter algumas vezes antes de ir para casa; até mesmo caminhos que 




percorrem todo o universo e voltam. Isto, na visão de Feynman, explica como a partícula 




adquire as informações sobre quais fendas estão abertas - se uma fenda está aberta, a partícula 




toma caminhos através dela. Quando as duas fendas estão abertas, os caminhos que a partícula 




percorre através de uma fenda podem interferir com os caminhos em que ela viaja através da 




outra, causando a interferência. Isso pode parecer maluco, mas para os propósitos da física mais 




fundamental feita hoje - e para as finalidades deste livro - a formulação de Feynman provou ser 




mais útil que a original. 










Caminhos de Partícula 

- A Formulação de Feynman da teoria quântica fornece uma imagem 




de por quê partículas como elétrons e buckyballs formam padrões de interferência quando são 




disparadas através de fendas em uma tela. 










A visão de Feynman da realidade quântica é fundamental na compreensão das teorias que 




apresentaremos em breve, por isso vale a pena gastar algum tempo para ter uma idéia de como 




ela funciona. Imagine um processo simples no qual uma partícula começa em algum local A e 




se move livremente. No modelo de Newton aquela partícula seguirá uma linha reta. Depois de 




passar um certo tempo preciso, vamos encontrar a partícula em algum local B precisa ao longo 




dessa linha. No modelo de Feynman uma partícula quântica testa cada caminho ligando A e B, 




coletando um número chamado de uma fase para cada caminho. Esta fase representa a posição 




no ciclo de uma onda, ou seja, se a vaga é em uma crista ou vale ou alguma posição 




intermediária precisa. A prescrição matemática de Feynman para calcular essa fase mostrou que 




quando você soma as ondas de todos os caminhos, você obtém a "amplitude de probabilidade" 




de que a partícula, a partir de A chegará a B. O quadrado daquela amplitude de probabilidade, 




então, dá a probabilidade correta de que a partícula atingirá B. 




A fase que cada caminho individual contribui para a soma de Feynman (e, portanto, a 




probabilidade de ir de A até B) pode ser visualizada como uma flecha que é de comprimento 




fixo, mas pode apontar em qualquer direção. Para adicionar duas fases, você coloca a seta que 




representa uma fase no final da seta que representa a outra, para obter uma nova seta que 




representa a soma. Para adicionar mais fases, você simplesmente continua o processo. Observe 




que quando as fases se alinham, a seta que representa o total pode ser bastante longa. Mas se 




apontam em direções diferentes, eles tendem a se anular quando você as adiciona, deixando 




você com não muito mais que uma flecha no total. A figura abaixo ilustra a idéia. 




Para realizar a prescrição de Feynman para o cálculo da amplitude de probabilidade que uma 




partícula iniciando em um local A acabará em um local B, você adiciona as fases, ou flechas, 







associadas a cada caminho que conecta A e B. Há um número infinito de caminhos, o que torna 




a matemática um pouco complicado, mas ela funciona. Alguns dos caminhos são retratados 




abaixo. 







Adicionando Caminhos de Feynman 

- Os efeitos devidos a caminhos de Feynman diferentes 




pode aumentar ou diminuir uns aos outros, exatamente como fazem as ondas. As setas 




amarelas representam as fases a serem adicionadas. As linhas azuis representam a soma deles, 




uma linha a partir da cauda da primeira seta até a ponta da última. Na parte inferior da imagem, 




as setas apontam para direções diferentes, e assim a sua soma, a linha azul, é muito curta. 










A teoria de Feynman dá uma imagem especialmente clara de como uma visão de mundo 




newtoniana pode surgir da física quântica, que parece muito diferente. Segundo a teoria de 




Feynman, as fases associadas a cada caminho dependem da constante de Planck. A teoria 




afirma que, devido a constante de Planck ser tão pequena, quando você adiciona a participação 




dos caminhos que estão próximos uns dos outros, as fases normalmente variam muito, e assim, 




conforme mostra a figura acima, eles tendem a somar zero. Mas, a teoria também mostra que 




existem certos caminhos para os quais as fases têm uma tendência a se alinhar, e assim esses 




caminhos são favorecidos; isto é, eles dão uma contribuição maior para o comportamento 




observado da partícula. Acontece que para objetos grandes, caminhos muito semelhantes ao 




caminho previsto por Newton terão fases semelhantes e somarão para dar, de longe, a maior 




contribuição à soma, e assim o único destino que tem uma probabilidade efetivamente maior 




que zero é o destino previsto pela teoria de Newton, e aquele destino tem uma probabilidade 




que é muito próxima de um. Daí objetos grandes se movem conforme a teoria de Newton 




previu que fariam. 










Os Caminhos de A até B 

- O caminho "clássico" entre dois pontos é uma linha reta. As fases dos caminhos que estão próximos do caminho clássico, tendem a melhorar uns aos outros, 




enquanto que as fases de caminhos mais distantes dela tendem a se anular. 










Até agora, discutimos as idéias de Feynman no contexto da experiência da dupla fenda. Nesse 




experimento, as partículas são disparadas em direção a uma parede com fendas, e medimos o 




local, em uma tela colocada além do muro, na qual as partículas caem. Mais genericamente, em 




vez de uma única partícula, a teoria de Feynman nos permite prever os resultados prováveis de 




um "sistema", que poderia ser uma partícula, um conjunto de partículas, ou até mesmo o 




universo inteiro. Entre o estado inicial do sistema e nossa medição posterior de suas 




propriedades, essas propriedades evoluem de alguma forma, o que os físicos chamam de 




história do sistema. No experimento da dupla fenda, por exemplo, a história da partícula é 




simplesmente o seu caminho. Assim como no experimento de dupla fenda, a chance de 




observar a partícula cair em qualquer ponto determinado depende de todos os caminhos que 




poderia ter chegado lá; Feynman mostrou que, para um sistema geral, a probabilidade de 




qualquer observação é construído a partir de todas as histórias possíveis que poderiam ter 




levado a essa observação. Devido a que seu método é chamado de formulação da "soma de 




histórias" ou "histórias alternativas" da física quântica. 




Agora que temos uma noção da abordagem de Feynman à física quântica, é hora de analisar 




outro princípio-chave quântico que usaremos mais tarde - o princípio de que a observação de 




um sistema deve alterar o seu curso. Não podemos, como fazemos quando o nosso supervisor 




tem uma mancha de mostarda no queixo, discretamente observar, mas não interferir? Não. De 




acordo com a física quântica, você não pode "apenas" observar algo. Ou seja, a física quântica reconhece que, para fazer uma observação, você precisa interagir com o objeto que está 




observando. Por exemplo, para ver um objeto no sentido tradicional, projetamos uma luz sobre 




ele. Projetar uma luz sobre uma abóbora, naturalmente, têm pouco efeito sobre ela. Mas, 




projetar até mesmo uma luz pálida sobre uma pequena partícula quântica - isto é, atirar fótons 




contra ela - tem um efeito considerável, e as experiências mostram que altera os resultados de um experimento da maneira como a física quântica o descreve. 




Suponha que, como antes, enviamos um fluxo de partículas em direção à barreira no 




experimento de dupla fenda e recolhemos dados sobre o primeiro milhão de partículas a passar. 




Quando traçamos o gráfico do número de part[iculas caindo em diferentes pontos de detecção, 




os dados formarão o padrão de interferência da foto, e quando adicionamos as fases associadas 




a todos os caminhos possíveis a partir de ponto A de partida de uma partícula até seu ponto de 




detecção B, veremos que a probabilidade que calculamos da queda em diversos pontos estão de 




acordo com aqueles dados. 




Agora, suponha que repetimos a experiência, desta vez, projetando luzes brilhantes sobre as 




fendas de modos que conhecemos que um ponto intermediário, C, através do qual a partícula 




passou. (C é a posição de uma das fendas ou a outra.) Isso é chamado de informação "qual 




caminho" porque ela nos diz se cada partícula foi de A para a fenda 1 até B, ou de A pela fenda 2 até B. Uma vez que agora sabemos por qual fenda cada partícula passou, os caminhos do 




nossa soma para aquela partícula incluirá somente caminhos que passam através da fenda 1 ou 




somente os caminhos que passam através da fenda 2. Ela nunca incluirá ambos os caminhos 




que passam pela fenda 1 e os caminhos que passam pela fenda 2. Porque Feynman explicou o 




padrão de interferência, dizendo que os caminhos que passam por uma fenda interferem com os 




caminhos que passam através da outra, se você acende uma luz para determinar qual fenda pela 




qual passam as partículas, eliminando assim a outra opção, você fará desaparecer o padrão de 




interferência. E, de fato, quando o experimento é realizado, acender uma luz muda os resultados 




do padrão de interferência, para um padrão como esse! Além disso, podemos variar a 




experiência, empregando luz muito fraca, de modo que nem todas as partículas interajam com a 




luz. Nesse caso, podemos obter informações "qual caminho" para apenas um subconjunto das 




partículas. Se, então, dividirmos os dados referentes às chegadas de partículas dependendo se 




conseguimos ou não as informações "qual caminho", descobrimos que os dados referentes ao 




subconjunto para o qual não temos nenhuma informação Qual-caminho formarão um padrão de 




interferência, e o subconjunto de dados pertencentes às partículas para as quais temos 




informações "qual-caminho" não mostrarão interferência. 




Esta idéia tem implicações importantes para o nosso conceito de "passado." Na teoria 




newtoniana, presume-se que o passado existe como uma série definida de eventos. Se você ve 




aquele vaso que você comprou na Itália no ano passado espalhado pelo chão e seu filhinho de 




pé sobre ele parecendo tímido, você pode retraçar os acontecimentos que levaram ao acidente: 




os dedinhos soltando, o vaso caindo e explodindo em mil peças quando atingir o chão. De fato, 




dado os dados completos sobre o presente, as leis de Newton permitem calcular um quadro 




completo do passado. Isto é consistente com nossa compreensão intuitiva de que, seja doloroso 




ou alegre, o mundo tem um passado definido. Pode não ter havido ninguém assistindo, mas o 




passado existe, tão certo como se você tivesse tirado uma série de fotos dele. Mas, uma 




buckyball quântica não se pode dizer que tomou um determinado caminho da fonte até a tela. 




Podemos marcar uma localização da buckyball pela observação, mas entre nossas observações, 




ela toma todos os caminhos. A física quântica nos diz que não importa quão rigorosa seja a 




nossa observação do presente, o passado (não observado), como o futuro é indeterminado e só 




existe como um espectro de possibilidades. O universo, segundo a física quântica, não tem um 




único passados, ou única história. 




O fato de que o passado não assume qualquer forma definida significa que as observações feitas em um sistema no presente afetam o seu passado. Isso é bastante dramaticamente sublinhado 




por um tipo de experimento de pensamento realizado pelo físico John Wheeler, chamado de 




experimento da escolha retardada. Esquematicamente, um experimento da escolha retardada é 




como a experiência da dupla fenda que acabamos de descrever, em que você tem a opção de 




observar o caminho que a partícula percorre, exceto que no experimento de escolha retardada 




você adia sua decisão sobre se deve ou não observar o caminho até pouco antes da partícula 




atingir a tela de detecção. 




Experimentos de escolha retardada resultam em dados idênticos aos que obtemos quando 




escolhemos observar (ou não observar) as informações de qual-caminho observando as próprias 




fendas. Mas, neste caso o caminho tomado por cada partícula - isto é, seu passado - é 




determinado muito tempo depois que ela passou pelas fendas e presumivelmente tinham de 




"decidir" se passariam atravé de apenas uma fenda, que não produz interferência, ou ambas as fendas, que o faz. 




Wheeler até considerou uma versão cósmica do experimento, no qual as partículas envolvidas 




são fótons emitidos por potentes quasares a bilhões de anos-luz de distância. Essa luz podia ser 




dividida em dois caminhos e reorientada em direção à terra pela lente gravitacional de uma 




galáxia interveniente. Embora o experimento esteja além do alcance da tecnologia atual, se 




pudéssemos coletar fótons suficiente desta luz, eles devem formar um padrão de interferência. 




No entanto, se colocarmos um dispositivo para medir a informação de qual-caminho pouco 




antes de detecção, o padrão deve desaparecer. A escolha se deve tomar um caminhos ou ambos, 




neste caso teria sido feito há bilhões de anos, antes da terra, ou talvez até mesmo nosso Sol ter 




se formado, e ainda assim com nossa observação em laboratório, estaremos afetando aquela 




escolha. 




Neste capítulo ilustramos a física quântica empregando a experiência da dupla fenda. No que se 




segue, aplicaremos a formulação de Feynman da mecânica quântica ao universo como um todo. 




Veremos que, como uma partícula, o universo não tem apenas uma história única, mas todas as 




histórias possíveis, cada uma com sua própria probabilidade, e as nossas observações sobre seu 




estado atual afeta seu passado e determina as diferentes histórias do universo, assim como as 




observações das partículas no experimento de dupla fenda afetam o passado das partículas. Esta 




análise mostrará como as leis da natureza em nosso universo surgiram do big bang. Mas, antes 




de examinarmos como surgiram as leis, falaremos um pouco sobre o que são essas leis, e alguns 




dos mistérios que elas provocam. 




5

A TEORIA DE TUDO 




 




 

A coisa mais incompreensível sobre o universo é que ele é compreensível. 




—ALBERT EINSTEIN 




O UNIVERSO É COMPREENSÍVEL, pois é regido por leis científicas, isto é, seu 




comportamento pode ser modelado. Mas, quais são essas leis ou modelos? A primeira força a 




ser descrita em linguagem matemática foi a gravidade. A lei da gravitação de Newton publicada 




em 1687, dizia que cada objeto no Universo atrai todos os outros objetos com uma força 




proporcional à sua massa. Ela produziu uma grande impressão sobre a vida intelectual da sua 




época, porque mostrou pela primeira vez, que pelo menos um aspecto do universo podia ser 




modelado com precisão, e ela estabeleceu o mecanismo matemático para fazê-lo. A idéia de que 




existem leis da natureza traz à tona questões semelhantes àquelas pelas quais Galileu tinha sido 




condenado por heresia cerca de cinqüenta anos antes. Por exemplo, a Bíblia conta a história de 




Josué, rezando para que o sol e a lua parassem em suas trajetórias, de modo que ele tivesse luz 




do dia extra para terminar o combate com os Amorreus em Canaã. De acordo com o livro de 




Josué, o sol ficou parado por cerca de um dia. Hoje sabemos que isso teria significado que a 




terra parou de girar. Se a Terra parasse, de acordo com as leis de Newton nada que estivesse 




amarrado teria permanecido em movimento na velocidade original da Terra (1.100 milhas por 




hora no equador) - um preço alto a pagar por um atraso no por do sol. Nada disso incomodou o 




próprio Newton, pois, como já dissemos, Newton acreditava que Deus podia e interviu no 




funcionamento do universo. 




Os próximos aspectos do universo para os quais uma lei ou modelo foi descoberto foram as 




forças elétrica e magnética. Estes se comportam como a gravidade, com a importante diferença 




de que duas cargas elétricas ou dois ímãs do mesmo tipo se repelem, enquanto que cargas ou 




magnetos diferentes se atraem. As forças elétrica e magnética são muito mais fortes que a 




gravidade, mas nós não costumamos percebê-las na vida cotidiana porque um corpo 




macroscópico contém quase o mesmo número de cargas elétricas positivas e negativas. Isto 




significa que as forças elétrica e magnética entre dois corpos macroscópicos quase se anulam 




mutuamente, ao contrário das forças gravitacionais, que se somam. 




Nossas idéias atuais sobre eletricidade e magnetismo foram desenvolvidos ao longo de um 




período de cerca de cem anos a partir de meados do século XVIII até meados do século XIX, 




quando os físicos de vários países realizaram estudos experimentais detalhados das forças 




elétrica e magnética. Uma das descobertas mais importantes foi a de que as forças elétricas e 




magnéticas estão relacionadas: Uma carga elétrica em movimento provoca uma força sobre 




ímãs, e um ímã em movimento provoca uma força sobre cargas elétricas. O primeiro a perceber 




que havia alguma conexão foi o físico dinamarquês Hans Christian Ørsted. Enquanto se 




preparava para uma palestra que faria na universidade em 1820, Oersted observou que a 




corrente elétrica da bateria que ele estava usando desviou a agulha de uma bússola que estava 




próxima. Ele logo percebeu que a eletricidade em movimento criava uma força magnética, e 




cunhou o termo "eletromagnetismo". Poucos anos mais tarde, o cientista britânico Michael 




Faraday raciocinou que - expresso em termos modernos - se uma corrente elétrica podia causar 




um campo magnético, um campo magnético devia ser capaz de produzir uma corrente elétrica. 




Ele demonstrou esse efeito em 1831. Quatorze anos mais tarde, Faraday também descobriu uma 




conexão entre eletromagnetismo e luz, quando mostrou que o magnetismo intenso podia afetar a natureza da luz polarizada. 




Faraday tinha pouca educação formal. Ele havia nascido em uma família pobre de um ferreiro, 




perto de Londres e deixou a escola aos treze anos para trabalhar como garoto de recados e 




encadernador em uma livraria. Lá, ao longo dos anos, ele aprendeu ciência através da leitura 




dos livros que deveria cuidar, e realizando experiências simples e baratas em seu tempo livre. 




Com o tempo, ele conseguiu trabalhar como assistente no laboratório do grande químico Sir 




Humphry Davy. Faraday ficaria no emprego pelos restantes 45 anos de sua vida e, depois da 




morte de Davy, o sucederia. Faraday tinha problemas com a matemática e nunca aprendeu 




muito sobre isso, então foi uma luta para ele conceber um quadro teórico dos fenômenos 




eletromagnéticos estranho que ele observava em seu laboratório. No entanto, ele o fez. 




Uma das maiores inovações intelectuais de Faraday foi a idéia de campos de força. Hoje em 




dias, graças aos livros e filmes sobre alienígenas de olhos esbugalhados e suas naves, a maioria 




das pessoas está familiarizada com o termo, talvez por isso ele deveria ter recebido direitos 




autorais. Mas, nos séculos entre Newton e Faraday um dos grandes mistérios da física era que 




suas leis pareciam indicar que as forças atuam em todo o espaço vazio que separa os objetos 




que interagem. Faraday não gostava disso. Ele acreditava que para mover um objeto, algo tinha 




que entrar em contato com ele. E, assim, ele imaginou o espaço entre as cargas elétricas e ímãs 




como sendo preenchido com tubos invisível que fisicamente realizavam o empurrar e o puxar. 




Faraday chamou àqueles tubos de campo de força. Uma boa maneira de visualizar um campo de 




força é realizar a demonstração de sala aula em que uma placa de vidro é colocada sobre um 




ímã e limalha de ferro é espalhada sobre o vidro. Com alguns toques para superar a fricção, as 




limalhas se movem como se empurradas por um poder invisível e se organizam em um padrão 




de arcos que se estende de um pólo do ímã ao outro. Esse padrão é um mapa da força magnética 




invisível que permeia o espaço. Hoje, acreditamos que todas as forças são transmitidas por 




campos, por isso este é um conceito importante na física moderna - assim como da ficção 




científica. 










Campos de Força 

- O campo de força de um ímã de barra, conforme ilustrado pela reação de 




limalha de ferro. 







Durante várias décadas, nosso entendimento do eletromagnetismo ficou bloqueado, 




correspondendo a não mais do que o conhecimento de algumas leis empíricas: a dica de que a 




eletricidade e o magnetismo estavam intimamente, ainda que misteriosamente, relacionados; a 




noção de que eles tinham algum tipo de conexão com a luz; e o conceito embrionário de 




campos. Pelo menos onze teorias do eletromagnetismo existiam, cada uma deles imperfeita. 




Então, durante um período de vários anos na década de 1860, o físico escocês James Clerk 




Maxwell desenvolveu o pensamento de Faraday em uma estrutura matemática que explicava a 




relação íntima e misteriosa entre a eletricidade, o magnetismo e a luz. O resultado foi um 




conjunto de equações que descrevem tanto a força elétrica quanto a magnética como 




manifestações da mesma entidade física, o campo eletromagnético. Maxwell tinha unificado a 




electricidade e o magnetismo em uma única força. Além disso, ele mostrou que os campos 




eletromagnéticos podem se propagar através do espaço como uma onda. A velocidade daquela 




onda é regida por um número que apareceu em suas equações, que ele calculou a partir de 




dados experimentais que tinham sido medidos alguns anos antes. Para seu espanto, a velocidade 




que ele calculou era igual à velocidade da luz, que então era conhecida experimentalmente até 




uma precisão de 1 por cento. Ele tinha descoberto que a própria luz é uma onda 




eletromagnética! 




Hoje, as equações que descrevem os campos elétricos e magnéticos são chamadas equações de 




Maxwell. Poucas pessoas ouviram falar delas, mas elas provavelmente são as equações de 




maior importância comercial que conhecemos. Não só elas regem o funcionamento de tudo, 




desde electrodomésticos a computadores, mas elas também descrevem outras ondas que a luz, 




tais como microondas, ondas de rádio, luz infravermelha e raios-X. Todas estas diferem de luz 




visível em um único aspecto - seus comprimentos de onda. As ondas de rádio têm 




comprimentos de onda de um metro ou mais, enquanto que a luz visível possui um 







comprimento de onda de algumas dezenas de milionésimos de um metro, e os raios-X têm um 




comprimento de onda mais curto do que uma centena de milionésimos de metro. Nosso sol 




irradia em todos os comprimentos de onda, mas sua radiação é mais intensa nos comprimentos 




de onda que são visíveis para nós. Provavelmente não é por acaso que os comprimentos de onda 




que somos capazes de ver a olho nu sejam aqueles em que o sol irradia mais fortemente: É 




provável que os nossos olhos evoluiram com a capacidade de detectar a radiação 




eletromagnética nessa faixa precisamente porque essa é a faixa de radiação mais disponível 




para eles. Se algum dia encontrarmos seres de outros planetas, eles provavelmente terão a 




capacidade de "ver" radiação em qualquer comprimento de onda que seu próprio sol emite mais fortemente, modulada por fatores tais como as características de bloqueio da luz por poeira e 




gases na atmosfera do seu planeta. Então, os alienígenas que evoluiu na presença de raios-X 




poderão ter uma bela carreira como seguranças de aeroporto. 







Comprimento de onda 

- Microondas, ondas de rádio, luz infravermelha, raios X - e diferentes 




cores de luz - diferem apenas quanto a seus comprimentos de onda. 







As equações de Maxwell ditam que as ondas eletromagnéticas se deslocam a uma velocidade de 




aproximadamente 300.000 km por segundo, ou cerca de 670 milhões de milhas por hora. Mas, 




citar uma velocidade nada significa, a menos que você especifique um quadro de referência 




relativamente ao qual a velocidade é medida. Isso não é algo que você normalmente precisa 




pensar na vida cotidiana. Quando um sinal de limite de velocidade indica 90 km por hora, 




entende-se que sua velocidade é medida em relação à estrada e não ao buraco negro no centro 




da Via Láctea. Mas, mesmo na vida cotidiana, há ocasiões em que você tem que levar em conta 




quadros de referência. Por exemplo, se você carrega uma xícara de chá no corredor de um avião 




em vôo, você pode dizer que sua velocidade é de 2 milhas por hora. Alguém no chão, no 




entanto, poderia dizer que você está se movendo a 572 milhas por hora. A menos que você 




pense que um ou o outro daqueles observadores tem uma alegação melhor da verdade, tenha em 




mente que, porque a Terra orbita o Sol, alguém te observando a partir da superfície daquele 




corpo celeste discordaria de ambos e diria que você está se movendo a cerca de 18 milhas por 







segundo, sem mencionar invejar o seu ar-condicionado. À luz dessas discordâncias, quando 




Maxwell afirmou ter descoberto a "velocidade da luz" brotando de suas equações, a pergunta natural era, em relação a que a velocidade da luz nas equações de Maxwell era medida? 




Não há nenhuma razão para acreditar que o parâmetro de velocidade nas equações de Maxwell 




seja uma velocidade medida em relação à Terra. Suas equações, afinal, aplicam-se a todo o 




universo. Uma resposta alternativa que foi considerado durante algum tempo é que as suas 




equações especificam a velocidade da luz em relação a um meio previamente não-detectado 




permeando todo o espaço, chamado éter luminífero, ou, para encurtar, simplesmente éter, que 




era o termo de Aristóteles para a substância que ele acreditava preencher todo o universo fora 




da esfera terrestre. Este éter hipotético seria o meio através do qual as ondas eletromagnéticas 




se propagam, exatamente como o som se propaga através do ar. Se o éter existisse, haveria um 




padrão absoluto de repouso (ou seja, repouso em relação ao éter) e, portanto, uma forma 




absoluta de se definir também o movimento. O éter forneceria um termo de referência 




preferencial em todo o universo, contra a qual a velocidade de qualquer objeto poderia ser 




medida. Assim, foi postulada a existência do éter com fundamentos teóricos, lançando alguns 




cientistas na busca de uma maneira de estudá-lo, ou, pelo menos, confirmar sua existência. Um 




desses cientistas era o próprio Maxwell. 




Se você corre pelo ar em direção a uma onda sonora, a onda se aproxima de você mais rápido, e 




se você corre para longe, ela se aproxima você de forma mais lenta. Da mesma forma, se 




houvesse um éter, a velocidade da luz variaria dependendo do seu movimento em relação ao 




éter. De fato, se a luz funcionasse da forma que funciona o som, assim como as pessoas em um 




jato supersônico nunca ouvirã qualquer som emitido pela parte traseira do avião, também 




viajantes corendo rapidamente o suficiente através do éter seriam capazes de superar uma onda 




de luz. Trabalhando a partir de tais considerações, Maxwell sugeriu um experimento. Se houver 




um éter, a terra deve estar se movendo através dele, enquanto orbita o sol. E, uma vez que a 




Terra se desloca em uma direção diferente, em janeiro do que, digamos, em abril ou julho, 




dever-se-ia ser capaz de observar uma pequena diferença na velocidade da luz em diferentes 




épocas do ano - veja a figura abaixo. 







Movendo-se através do Éter 

- Se estivéssemos em movimento através do éter, deveríamos ser capazes de detectar esse movimento observando as diferenças sazonais na velocidade da luz. 










Maxwell foi dissuadido de publicar sua idéia em Proceedings of the Royal Society pelo seu 




editor, que não achava que o experimento funcionaria. Mas, em 1879, pouco antes de morrer na 




idade de quarenta e oito anos de de um doloroso câncer de estômago, Maxwell enviou uma 




carta sobre o assunto a um amigo. A carta foi publicada postumamente na revista Nature, onde 




ela foi lida por, entre outros, um físico americano chamado Albert Michelson. Inspirado pela 




especulação de Maxwell, em 1887 Michelson e Edward Morley realizaram uma experiência 




muito sensível concebida para medir a velocidade em que a Terra viaja através do éter. Sua 




idéia era comparar a velocidade da luz em duas direções diferentes, em ângulos retos. Se a 




velocidade da luz fosse um número fixo em relação ao éter, as medidas deveriam ter revelado 




velocidades da luz que variavam consoante a direção do feixe. Mas, Michelson e Morley não 




observaram tal diferença. 




O resultado da experiência de Michelson e Morley está claramente em conflito com o modelo 




de ondas eletromagnéticas viajando através de um éter, e deveria ter provocado o abandono do 




modelo de éter. Mas, o propósito de Michelson tinha sido medir a velocidade da Terra em 




relação ao éter, e não provar ou refutar a hipótese do éter, e o que ele descobriu não o levou a 




concluir que o éter não existia. Nem ninguém mais tirou essa conclusão. Na verdade, o famoso 




físico Sir William Thomson (Lord Kelvin) disse em 1884 que o éter era "a única substância em 




que confiamos em dinâmica. De uma coisa temos certeza, e isso é a realidade e a 




substancialidade do éter luminífero". 




Como você pode acreditar no éter apesar dos resultados da experiência de Michelson-Morley? 




Como já dissemos que acontece muitas vezes, as pessoas tentaram salvar o modelo através de 




acréscimos planejados e ad hoc. Alguns postularam que a terra arrastava o éter juntamente com 




ela, por isso não estava realmente em movimento em relação a ele. O físico holandês Hendrik 




Antoon Lorentz e físico irlandês George Francis Fitzgerald sugeriram que em um quadro em 




que estava se movendo em relação ao éter, provavelmente devido a algum efeito mecânico 




ainda desconhecido, os relógios diminuiriam a velocidade e as distâncias encolheriam, assim 




ainda se mediria a luz com a mesma velocidade. Tais esforços para salvar o conceito de éter 




continuaram por quase 20 anos até a publicação de um trabalho notável por um funcionário 




jovem e desconhecido do escritório de patentes em Berna, Albert Einstein. 




Einstein tinha 26 anos em 1905, quando publicou seu artigo "Zur bewegter Elektrodynamik 




Körper" ("Sobre a Eletrodinâmica dos Corpos em Movimento"). Nela, ele fez a simples 




suposição de que as leis da física e em particular a velocidade da luz deveriam parecer ser as 




mesmas para todos os observadores em movimento uniforme. Essa idéia, ao que parece, exige 




uma revolução na nossa concepção de espaço e tempo. Para ver por quê, imagine dois eventos 




que acontecem no mesmo local, mas em momentos diferentes, em um avião a jato. Para um 




observador no avião, haverá distância zero entre esses dois eventos. Mas, para um segundo 




observador em terra, os eventos serão separados pela distância que o jato já viajou no tempo 







entre os eventos. Isso mostra que dois observadores que se movem em relação um ao outro não 




concordarão sobre a distância entre dois eventos. 




Agora, suponhamos que os dois observadores observam um pulso de luz que viaja a partir da 




cauda da aeronave até seu nariz. Exatamente como no exemplo acima, eles não concordarão 




sobre a distância que a luz percorreu desde a sua emissão na cauda do avião até sua recepção no 




nariz. Como a velocidade é a distância percorrida dividida pelo tempo exigido para tal, isso 




significa que, se eles concordam sobre a rapidez com que o pulso viaja - à velocidade da luz - 




eles não concordarão sobre o intervalo de tempo entre a emissão e a recepção. 







Jato em vôo 

- Se você bate uma bolinha em um jato, um observador à bordo do avião pode 




determinar que ela atinge o mesmo ponto a cada salto, enquanto que um observador no solo 




medirá uma grande diferença nos pontos de salto. 







O que torna isso estranho é que, embora os dois observadores meçam tempos diferentes, eles 




estão assistindo ao mesmo processo físico. Einstein não tentou construir uma explicação 




artificial para isso. Ele tirou a conclusão lógica, ainda que surpreendente, de que a medição do 




tempo necessário, assim como a medição da distância percorrida dependem do observador que 




faz a medição. Esse efeito é uma das chaves para a teoria no trabalho de Einstein de 1905, que 




passou a ser chamada relatividade especial. 




Podemos ver como essa análise poderia se aplicar a dispositivos de cronometragem, se 




considerarmos dois observadores olhando para um relógio. A relatividade especial sustenta que 




o relógio corre mais rápido, dependendo de um observador que está em repouso em relação ao 




relógio. Para os observadores que não estão em repouso em relação ao relógio, o relógio anda 




mais devagar. Se compararmos um pulso de luz viajando da cauda para o nariz do avião ao 




tique de um relógio, vemos que para um observador em terra o tempo corre mais devagar 




porque o feixe de luz tem de percorrer uma distância maior no mesmo quadro de referência. 




Mas, o efeito não depende do mecanismo do relógio; ela vale para todos os relógios, mesmo 




nossos próprios relógios biológicos. 










A dilatação do tempo 

- Relógios em movimento parecem funcionar devagar. Porque isso 




também se aplica a relógios biológicos, pessoas em movimento parecem envelhecer mais 




lentamente, mas não tenha muitas esperanças - a velocidades cotidianas, nenhum relógio 




normal, poderia medir a diferença. 










O trabalho de Einstein mostrou que, como o conceito de repouso, o tempo não pode ser 




absoluto, como pensava Newton. Em outras palavras, não é possível atribuir a cada evento um 




tempo com o qual todos os observadores concordarão. Em vez disso, todos os observadores têm 




suas próprias medidas de tempo, e os tempos medidos por dois observadores que estejam se 




movem em relação uns aos outros não concordarão. As idéias de Einstein vão contra a nossa 




intuição, porque suas implicações não são perceptíveis às velocidades que normalmente 




encontramos na vida cotidiana. Mas, elas têm sido repetidamente confirmadas por experiências. 




Por exemplo, imagine um relógio de referência em repouso no centro da terra, outro relógio na 




superfície da Terra, e um terceiro relógio à bordo de um avião, voando a favor ou contra o 




sentido de rotação da Terra. Com referência ao relógio no centro da Terra, o relógio a bordo do 




avião em movimento para o leste - na direção de rotação da Terra - está se movendo mais 




rápido do que o relógio na superfície da Terra, e por isso deve ser mais lento. Da mesma forma, 




com referência ao relógio no centro da Terra, o relógio a bordo do avião que voa para oeste - 




contra a rotação da Terra - está se movendo mais lentamente que o relógio na superfície, o que 




significa que o relógio funciona mais rápido que o relógio na superfície. E isso é exatamente o 




que foi observado quando, em um experimento realizado em outubro de 1971, um relógio 




atómico muito preciso foi transportado por avião ao redor do mundo. Assim, você poderia 




prolongar a sua vida voando constantemente para o leste ao redor do mundo, mas você pode 




ficar cansado de assistir a todos aqueles filmes das companhias aéreas. No entanto, o efeito é 




muito pequeno, cerca de 180 bilionésimos de um segundo por circuito (e é também um pouco 




diminuído pelos efeitos da diferença de gravidade, mas não precisamos entrar nisso aqui). 




Devido ao trabalho de Einstein, os físicos perceberam que, exigindo que a velocidade da luz seja a mesma em todos os quadros de referência, a teoria de Maxwell sobre eletricidade e 




magnetismo determina que o tempo não pode ser tratado como algo separado das três 




dimensões do espaço. Em vez disso, o tempo e o espaço estão interligados. É algo como 




adicionar de uma quarta direção de futuro/passado à usual esquerda / direita, frente/trás e para 




cima / para baixo. Os físicos chamam esse casamento de espaço e tempo "espaço-tempo", e 




porque o espaço-tempo inclui uma quarta direção, que eles chamam de quarta dimensão. No 




espaço-tempo, o tempo não é mais separada das três dimensões do espaço, e, falando 




livremente, assim como a definição de esquerda / direita, frente / trás, ou para cima / baixo 




depende da orientação do observador, assim também não a direção do tempo varia dependendo 




da velocidade do observador. Observadores que se deslocam a velocidades diferentes 




escolheriam direções diferentes para o tempo no espaço-tempo. A teoria de Einstein da 




relatividade especial foi, portanto, um novo modelo, que se livrou dos conceitos de tempo 




absoluto e repouso absoluto (ou seja, repouso em relação ao éter fixo). 




Einstein logo percebeu que para tornar a gravidade compatível com a relatividade outra 




mudança era necessária. Segundo a teoria da gravitação de Newton, em qualquer dado 




momento os objetos são atraídos uns aos outros por uma força que depende da distância entre 




eles naquele momento. Mas, a teoria da relatividade tinha abolido o conceito de tempo 




absoluto, portanto não havia maneira de definir quando a distância entre as massas devia ser 




medida. Assim, a teoria da gravitação de Newton não era consistente com a relatividade 




especial e tinha que ser modificada. O conflito pode soar como uma mera dificuldade técnica, 




talvez até mesmo um detalhe que pudesse de alguma forma ser contornado sem muita mudança 




na teoria. Como veio a se revelar, nada poderia estar mais longe da verdade. 




Ao longo dos próximos onze anos, Einstein desenvolveu uma nova teoria da gravidade, que ele 




chamou de relatividade geral. O conceito de gravidade na relatividade geral não é nada parecido 




com o de Newton. Em vez disso, ele se baseia na proposta revolucionária de que o espaço-




tempo não é plano, como se presumia anteriormente, mas é curvo e distorcido por sua massa e 




energia. 




Uma boa maneira de se retratar a curvatura é pensar na superfície da Terra. Embora a superfície 




da Terra seja apenas bidimensional (porque existem apenas duas direções ao longo dela, 




digamos norte / sul e leste / oeste), vamos usá-la como nosso exemplo, porque um espaço curvo 




bidimensional é mais fácil de retratar que um espaço curvo quadridimensional. A geometria de 




espaços curvos, tais como a superfície da terra não é a geometria euclidiana com a qual estamos 




familiarizados. Por exemplo, na superfície da Terra, a distância mais curta entre dois pontos - 




que nós conhecemos como uma linha na geometria euclidiana - é o caminho que conecta os 




dois pontos ao longo do que é chamado de um grande círculo. (Um grande círculo é um círculo 




ao longo da superfície da Terra, cujo centro coincide com o centro da terra. O Equador é um 




exemplo de um grande círculo, e assim é todo círculo obtido rotacionando o equador ao longo 




de diferentes diâmetros.)  




Imagine, digamos, que você queria viajar de Nova Iorque a Madrid, duas cidades que estão 




quase na mesma latitude. Se a Terra fosse plana, o caminho mais curto seria viajar diretamente 




para o leste. Se você fizesse isso, você chegaria a Madri após viajar 3.707 milhas. Mas, devido 




à curvatura da Terra, há um caminho que, em um mapa plano parece curvo e, 




consequentemente, mais longo, mas que na verdade é mais curto. Você pode chegar lá em 







3.605 milhas, se seguir a rota do grande-círculo, que é o primeiro ir para o nordeste, então, 




gradualmente virar para o leste, e depois sudeste. A diferença em distância entre as duas rotas é 




devida à curvatura da Terra, e um sinal de sua geometria não-euclidiana. As companhias aéreas 




sabem disso, e mandam seus pilotos seguir rotas do grande-círculo sempre que possível. 




De acordo com as leis do movimento de Newton, objectos tais como balas de canhão, croissants 




e planetas movem-se em linha reta, a menos que sejam influenciado por uma força, por 




exemplo, a gravidade. Mas, a gravidade na teoria de Einstein não é uma força como as outras, 




pelo contrário, é uma conseqüência do fato de que a massa distorce o espaço-tempo, criando 




curvatura. Na teoria de Einstein, os objetos se movem em geodésicas, que são as coisas mais 




próximas de linhas retas em um espaço curvo. As linhas são geodésicas sobre a superfície 




plana, e grandes círculos são geodésicas sobre a superfície da terra. Na ausência de matéria, as 




geodésicas em espaço-tempo de quatro dimensões correspondem a linhas no espaço 




tridimensional. Mas, quando matéria está presente, distorcendo o espaço-tempo, as trajetórias 




dos corpos no espaço tridimensional correspondente se curva de uma forma que na teoria 




newtoniana era explicada pela atração da gravidade. Quando o espaço-tempo não é plano, os 




caminhos dos objetos parecem se dobrar, dando a impressão de que uma força está atuando 




sobre eles. 







Geodésica 

- A menor distância entre dois pontos na superfície da Terra aparece curva quando 




traçada em um mapa plano - algo a se ter em mente, se alguma vez for passar por um teste de 




sobriedade 







. 




A teoria geral da relatividade de Einstein reproduz a relatividade especial quando a gravidade 




está ausente, e ela faz quase as mesmas previsões que a teoria da gravitação de Newton no 




ambiente de gravidade fraco do nosso sistema solar - mas não exatamente. Na realidade, se a 




relatividade geral não fosse levada em conta em sistemas GPS de navegação por satélite, os 




erros de posições globais poderiam se acumular a uma taxa de cerca de dez quilômetros a cada 




dia! No entanto, a real importância da relatividade geral não é a sua aplicação em dispositivos 




que lhe orientam até novos restaurantes, mas sim que é um modelo muito diferente do universo, 




que prevê novos efeitos, tais como ondas gravitacionais e buracos negros. E assim, a 




relatividade geral transformou a física em geometria. A tecnologia moderna é sensível o 




suficiente para nos permitir realizar muitos testes sensíveis da relatividade geral, e ela passou 




em cada um deles. 




Embora ambos tenham revolucionado a física, a teoria do eletromagnetismo de Maxwell e a teoria da gravitação de Einstein - a relatividade geral - são, ambas, como a física do próprio 




Newton, teorias clássicas. Ou seja, eles são modelos em que o universo tem uma única história. 




Como vimos no último capítulo, em nível atômico e subatômico estes modelos não estão de 




acordo com as observações. Ao invés, temos que usar a teoria quântica na qual o universo pode 




ter qualquer história possível, cada uma delas com sua própria intensidade ou amplitude de 




probabilidade. Para cálculos práticos que envolvem o mundo cotidiano, podemos continuar a 




utilizar as teorias clássicas, mas se quisermos entender o comportamento dos átomos e 




moléculas, precisamos de uma versão quântica da teoria do eletromagnetismo de Maxwell; e se 




quisermos entender o universo primordial, quando toda a matéria e energia do universo estavam 




espremidas em um pequeno volume, temos de ter uma versão quântica da teoria da relatividade 




geral. Precisamos também de tais teorias, pois se estamos buscando um entendimento 




fundamental da natureza, não seria coerente se algumas das leis fossem quânticas, enquanto 




outras fossem clássicas. Temos, portanto, que encontrar versões quânticas de todas as leis da 




natureza. Tais teorias são chamadas teorias de campos quânticos. 




As forças da natureza conhecidas podem ser divididas em quatro classes: 




1.  

Gravidade 

. Esta é a mais fraca das quatro, mas é uma força de longo alcance, e atua sobre todo o universo como uma atração. Isto significa que para grandes corpos, as forças 




gravitacionais se somam todas e podem dominar todas as outras forças. 




2.  

Eletromagnetismo 

. Este também é de longo alcance e é muito mais forte que a gravidade, 




mas atua apenas sobre partículas com uma carga elétrica, sendo de repulsão entre cargas de 




mesmo sinal, e de atração entre cargas de sinal oposto. Isto significa que as forças elétricas 




entre grandes corpos se anulam mutuamente, mas em escalas de átomos e moléculas elas 




dominam. As forças eletromagnéticas são responsáveis por toda a química e a biologia. 




3.  

Força nuclear fraca

. Isso causa radioatividade e desempenha um papel vital na formação dos elementos nas estrelas e no universo inicial. No entanto, nós não entramos em contato com essa 




força em nossa vida cotidiana. 




4.  

Força nuclear forte

. Esta força mantém unidos os prótons e nêutrons dentro do núcleo de um átomo. Ela também mantém unidos os prótons e nêutrons entre si, o que é necessário porque 




eles são feitos de partículas ainda menores, os quarks que mencionamos no Capítulo 3. A força 




forte é a fonte de energia para o sol e a energia nuclear, mas, assim como acontece com a força 




fraca, não temos contato direto com ela. 




A primeira força para a qual foi criada uma versão quântica foi o eletromagnetismo. A teoria 




quântica do campo eletromagnético, chamada eletrodinâmica quântica, ou QED para encurtar, 




foi desenvolvida em 1940 por Richard Feynman e outros, e tornou-se um modelo para todas as 




teorias quânticas de campos. Como já dissemos, de acordo com as teorias clássicas, as forças 




são transmitidas peor campos. Mas, nas teorias quânticas de campos, os campos de força são 




retratados como sendo feitos de várias partículas elementares chamadas bósons, que são 




partículas portadoras de força que voam para trás e para frente entre as partículas de matéria, 




transmitindo as forças. As partículas de matéria são chamadas férmions. Elétrons e quarks são 




exemplos de férmions. O fóton, ou partícula de luz, é um exemplo de um bóson. É o bóson que 




transmite a força eletromagnética. O que acontece é que uma partícula de matéria, como um 




elétron, por exemplo, emite um bóson, ou partícula de força e recua a partir dela, assim como 




um canhão recua depois de disparar uma bala. A partícula de força, em seguida, colide com outra partícula de matéria e é absorvida, alterando o movimento daquela partícula. De acordo 




com a QED, todas as interações entre partículas carregadas - partículas que sentem a força 




eletromagnética - são descritas em termos de intercâmbio de fótons. 




As previsões da QED foram testadas e descobriu-se que correspondem a resultados 




experimentais com grande precisão. Mas, realizar os cálculos matemáticos exigidos pela QED 




pode ser difícil. O problema, como veremos adiante, é que quando você adiciona a este quadro 




de intercâmbio de partículas a exigência quânticas de que se incluam todas as histórias em que a 




interação pode ocorrer - por exemplo, todas as formas como as partículas de força podem ser 




intercambiadas - a matemática torna-se complicada. Felizmente, junto com a invenção da noção 




de histórias alternativas - a maneira de pensar sobre as teorias quânticas descrita no último 




capítulo - Feynman também desenvolveu um método gráfico puro de contabilizar as diferentes 




histórias, um método que é aplicado hoje, não apenas à QED, mas a todas as teorias quânticas 




de campo. 




O método gráfico de Feynman fornece uma maneira de visualizar cada termo da soma de 




histórias. Essas imagens, chamadas diagramas de Feynman, são uma das ferramentas mais 




importantes da física moderna. Em QED a soma de todas as histórias possíveis pode ser 




representada como um diagramas de somas de Feynman como este abaixo, que representa 




algumas das maneiras como é possível para dois elétrons se espalhar afastando-se do outro 




através da força eletromagnética. Nestes diagramas, as linhas sólidas representam elétrons e as 




linhas onduladas representam fótons. O tempo é entendido como progredindo de baixo para 




cima, e os lugares onde as linhas se juntam correspondem a fótons sendo emitidos ou 




absorvidos por um elétron. O Diagrama (A) representa os dois elétrons aproximando-se um do 




outro, trocando um fóton, e em seguida continuando seu caminho. Essa é a maneira mais 




simples em que dois elétrons podem interagir eletromagneticamente, mas devemos considerar 




todas as histórias possíveis. Daí, precisamos também incluir diagramas como (B). Esse 




diagrama também retrata duas linhas entrando - os elétrons que se aproximam - e duas linhas 




saindo - os eletros dispersos - mas neste diagrama os elétrons trocam dois fótons antes de ir 




embora. Os diagramas retratados são apenas algumas das possibilidades, na verdade, há um 




número infinito de diagramas, que devem ser matematicamente considerados. 










Diagramas de Feyman 

- Esses diagramas se referem a um processo em que dois elétrons 




dispersam-se um a partir do outro. 







Os diagramas de Feynman não são apenas uma forma elegante de retratar e categorizar a forma 




como as interações podem ocorrer. Os diagramas de Feynman vêm com regras que permitem 




que você leia, a partir das linhas e vértices em cada diagrama, uma expressão matemática. A 




probabilidade, digamos, de que os elétrons que entram, com algum dado impulso inicial, 




acabarão voando para longe com algum impulso final particular é obtida através da soma das 




contribuições de cada diagrama de Feynman. Isso pode exigir algum trabalho, porque, como já 




dissemos, há um número infinito deles. Além disso, embora se atribua uma energia e impulso 




definido aos elétrons que entram e os elétrons que saem, as partículas nos circuitos fechados no 




interior do diagrama podem ter qualquer energia e impulso. Isso é importante porque na 




formação da soma de Feynman, é preciso somar não só todos os diagramas, mas também todos 




os valores de energia e impulso. 




Os diagramas de Feynman oferecem aos físicos uma enorme ajuda para visualizar e calcular as 




probabilidades dos processos descritos pela QED. Mas eles não curam uma doença importante 




sofrida pela teoria: Quando você adiciona as contribuições a partir do número infinito de 




diferentes histórias, você obtém um resultado infinito. (Se os termos sucessivos em uma soma 




infinita decrescem rápido o suficiente, é possível que o a soma seja finito, mas isso, 




infelizmente, não acontece aqui.) Em particular, quando os diagramas de Feynman são 




somados, a resposta parece implicar que o elétron tem uma massa e carga infinitos. Isso é um 




absurdo, porque podemos medir a massa e a carga e eles são finitos. Para lidar com essas 




infinidades, foi desenvolvido um procedimento chamado renormalização. 




O processo de renormalização envolve subtrair quantidades que são definidas como infinitas e 




negativas de tal forma que, com a contabilização matemática cuidadosa, a soma dos valores 




negativos infinitos e os valores positivos infinitos que surgem na teoria quase se anulam, 




deixando um pequeno remanescente, os valores finitos observados da massa e carga. Estas 







manipulações podem soar como o tipo de coisas que fazem com que você obtenha uma nota de 




reprovação em um exame de matemática da escola, e a renormalização é, na verdade, como 




parece, matematicamente duvidosa. Uma conseqüência disso é que os valores obtidos por este 




método para a massa e carga do elétron podem ser qualquer número finito. Isso tem a vantagem 




de que os físicos podem escolher as infinidades negativas de uma maneira que dá a resposta 




certa, mas a desvantagem é que a massa e a carga do elétron, portanto, não podem ser previstos 




a partir da teoria. Mas, uma vez que tenhamos fixado a massa e a carga do elétron desta forma, 




podemos empregar a QED para fazer muitas outras previsões muito precisas, que concordam 




todas muito de perto com a atenção, por isso a renormalização é um dos ingredientes essenciais 




da QED. Um triunfo precoce da QED, por exemplo, foi a previsão correta do chamado 




deslocamento de Lamb, uma pequena mudança na energia de um dos estados do átomo de 




hidrogênio descoberto em 1947. 







Diagramas de Feynman 

- Richard Feynman dirigia uma famosa van com os diagramas de 




Feynman pintados sobre ela. Esta descrição do artista foi feita para mostrar os diagramas 




discutidos acima. Apesar de Feynman ter morrido em 1988, a van ainda está por aí - em 




armazém próximo à Caltech no sul da Califórnia. 







O sucesso da renormalização na QED incentivou tentativas de buscar teorias quânticas de 




campos que descrevem as outras três forças da natureza. Mas, a divisão de forças naturais em 




quatro classes é provavelmente artificial e uma consequência da nossa falta de compreensão. As 




pessoas têm então procurado uma teoria de tudo que unificará as quatro classes em uma única 




lei que seja compatível com a teoria quântica. Este seria o Santo Graal da física. 




Uma indicação de que aquela unificação é a abordagem certa veio a partir da teoria da força 




fraca. A teoria quântica de campo que descreve a própria força fraca não pode ser 




renormalizada, ou seja, ela tem infinitos que não podem ser cancelados por subtração de um 




número finito de quantidades, tais como massa e carga. No entanto, em 1967, Abdus Salam e 




Steven Weinberg cada um independentemente propôs uma teoria na qual o eletromagnetismo 




foi unificado com a força fraca, e descobriram que a unificação curava a praga dos infinitos. A força unida é chamada força eletrofraca. A sua teoria pode ser renormalized, e ela previu três 




novas partículas chamadas W +, W-, e Z0. Evidência da Z0 foi descoberta no CERN em 




Genebra em 1973. Salam e Weinberg foram agraciados com o Prêmio Nobel em 1979, embora 




as partículas W e Z não fossem diretamente observadas até 1983. 




A força forte pode ser renormalizada sozinha em uma teoria chamada CDQ, ou cromodinâmica 




quântica. De acordo com a CDQ, o próton, o nêutron, e muitas outras partículas elementares da 




matéria são feitas de quarks, que possuem uma notável propriedade que os físicos vieram a 




chamar de cor (daí o termo "cromodinâmica", embora as cores de quarks sejam apenas rótulos úteis - não há nenhuma conexão com a cor visível). Os Quarks vêm em três das chamadas 




cores: vermelho, verde e azul. Além disso, cada quark tem um parceiro anti-partícula, e as cores 




dessas partículas são chamados de anti-vermelho, anti-verde e anti-azul. A idéia é que apenas as 




combinações sem cor líquida pode existir como partículas livres. Há duas maneiras de 




conseguir tais combinações neutras de quarks. Uma cor e suas anti-cores se cancelam, assim, 




um quark e um anti-quark formam um par incolor, uma partícula instável chamada méson. 




Além disso, quando todas as três cores (ou anti-cores) são misturadas, o resultado não tem cor 




líquida. Três quarks, um de cada cor, formam partículas estáveis chamada bárions, das quais 




são exemplos os prótons e os nêutrons (e três anti-quarks formam as anti-partículas dos 




bárions). Os prótons e os nêutrons são os bárions que compõem o núcleo dos átomos e são a 




base para toda a matéria normal no universo. 




A QCD também tem uma propriedade chamada liberdade assintótica, a que nos referimos, sem 




nomeá-la, no capítulo 3. Liberdade assintótica significa que as forças forte entre quarks são 




pequenas quando os quarks estão juntos, mas aumentam se eles são afastados, um pouco como 




se fossem unidos por tiras de borracha. A liberdade assintótica explica por que não vemos 




quarks isolados na natureza, e não foi possível produzi-los em laboratório. Assim, embora não 




possamos observar quarks individuais, aceitamos o modelo, porque ele funciona tão bem para 




explicar o comportamento de prótons, nêutrons e outras partículas da matéria. 




Depois de unir as forças fraca e eletromagnética, os físicos na década de 1970 procuraram uma 




maneira de incluir a força forte naquela teoria. Há uma série das chamadas grandes teorias 




unificadas ou GUTs que unificam as forças forte com a força fraca e o eletromagnetismo, mas 




elas, em sua maiorea prevêem que os prótons, a coisa de que somos feitos, devem se deteriorar, 




em média, após cerca de 1032 anos . Essa é uma vida muito longa, dado que o universo tem 




apenas cerca de 1.010 anos de idade. Mas em física quântica, quando dizemos que a vida média 




de uma partícula é 1032 anos, não queremos dizer que a maioria das partículas vive cerca de 




1.032 anos, algumas um pouco mais e outras um pouco menos. Em vez disso, o que queremos 




dizer é que, a cada ano, a partícula tem um 1 em 1032 chances de decair. Como resultado, se 




você observa um tanque contendo 1.032 prótons por apenas alguns anos, você deveria ver 




alguns dos prótons decair. Não é difícil demais construir tal tanque, uma vez que 1032 prótons 




estão contidas em apenas mil toneladas de água. Os cientistas realizaram tais experiências. 




Acontece que detectar decadências e diferenciá-las de outros eventos causados pelos raios 




cósmicos que continuamente chovem sobre nós do espaço não é uma questão fácil. Para 




minimizar o ruído, as experiências são realizadas dentro de lugares profundos, como a mina da 




empresa Kamioka Mining and Smelting, 3.281 pés debaixo de uma montanha no Japão, que 




está de alguma forma um pouco protegida dos raios cósmicos. Como resultado de observações 







em 2009, pesquisadores concluíram que, se os prótons decaem, o tempo de vida do próton é 




maior que cerca de 1034 anos, o que é uma má notícia para as grandes teorias unificadas. 







Bárions e mésons 

- Dize-se que Bárions e Mésons seriam feitos de quarks unidos pela força 




forte. Quando essas partículas colidem, elas podem trocar quarks, mas os quarks individuais 




não podem ser observados. 







Uma vez que evidências observacionais anteriores também tinham falhado em dar suporte às 




GUTs; a maioria dos físicos adotou uma teoria ad hoc chamada de modelo padrão, que 




compreende a teoria unificada das forças eletrofraca e a CDQ como uma teoria das forças 




fortes. Mas, no modelo padrão, as forças eletrofraca forte atuam separadamente e não são 




verdadeiramente unificadas. O modelo padrão tem muito sucesso e concorda com todas as 




evidências de observação atuais, mas em última análise é insatisfatório, pois, além de não 




unificar as forças eletrofraca e forte, ele não inclui a gravidade. 




Pode ter-se provado difícil combinar a força forte com as forças eletromagnética e fraca, mas 




esses problemas nada são se comparados ao problema de fundir a gravidade com os outros três, 




ou mesmo criar uma teoria quântica independente da gravidade. A razão pela qual uma teoria 




quântica da gravidade tem-se se mostrado tão difícil de criar tem a ver com o princípio da 




incerteza de Heisenberg, que discutimos no capítulo 4. Não é óbvio, mas resulta que no que diz 




respeito a este princípio, o valor de um campo e sua taxa de variação desempenham o mesmo 




papel que a posição e a velocidade de uma partícula. Ou seja, quanto mais exatamente uma é 




determinada, menos precisa pode ser a outra. Uma conseqüência importante disso é que não há 




tal coisa como espaço vazio. Isso porque o espaço vazio significa que tanto o valor de um 




campo quanto sua taxa de mudança são exatamente iguais a zero. (Se a taxa de mudança do 




campo não fosse zero, o espaço não permaneceria vazio.) Uma vez que o princípio da incerteza 




não permite que os valores de ambos, o campo e a taxa de variação sejam exatos, o espaço 




nunca está vazio. Ele pode ter um estado de energia mínima, chamado vácuo, mas este estado 




está sujeito ao que são chamados tremores quânticos, ou flutuações do vácuo - partículas e 




campos tremendo para dentro e para fora da existência. 













"Simplesmente desenhar um quadrado em torno dela, eu receio,  

não 

 a torna uma teoria 




unificada ". 







Pode-se pensar as flutuações do vácuo como pares de partículas que aparecem juntos em algum 




momento, afastam-se e, em seguida, se juntam e se aniquilam. Em termos de diagramas de 




Feynman, elas correspondem a circuitos fechados. Essas partículas são denominadas partículas 




virtuais. Ao contrário de partículas reais, as partículas virtuais não podem ser observadas 




diretamente com um detector de partículas. No entanto, seus efeitos indiretos, tais como 




pequenas mudanças na energia das órbitas de elétrons podem ser medidas, e estão de acordo 




com as previsões teóricas até um grau notável de precisão. O problema é que as partículas 




virtuais têm energia, e porque há um número infinito de pares virtuais, elas teriam uma 




quantidade infinita de energia. Segundo a relatividade geral, isso significa que elas curvariam o 




universo até um tamanho infinitamente pequeno, o que obviamente não acontece! 




Esta praga de infinidades é semelhante ao problema que ocorre nas teorias das forças forte, 




fraca e eletromagnética, salvo que naqueles casos a renormalização remove as infinidades. Mas 




os circuitos fechados nos diagramas de Feynman para a gravidade produzem infinidades que 




não podem ser absorvidas pelo renormalização porque na relatividade geral não há parâmetros 




renormalizáveis suficientes (tais como os valores de massa e carga) para remover todas as 




infinidades quânticas da teoria. Ficamos, portanto, com uma teoria da gravidade, que prevê que 




certas quantidades, tais como a curvatura do espaço-tempo são infinitas, o que não é uma forma 




de administrar um universo habitável. Isso significa que a única possibilidade de obtenção de 




uma teoria sensata seria que todas as infinidades de alguma forma se cancelassem, sem recorrer 




à renormalização. 




Em 1976, uma possível solução para esse problema foi encontrada. Ela é chamada 




supergravidade. O prefixo "super" não foi adicionado porque os físicos achavam que era 




"super" que esta teoria da gravitação quântica pudesse realmente funcionar. Em vez disso, 




"super" se refere a um tipo de simetria que a teoria possui, chamada supersimetria. 




Em física, diz-se que um sistema tem uma simetria se suas propriedades não são afetadas por 




uma certa transformação, tal como rotação no espaço ou assumindo sua imagem espelhada. Por 




exemplo, se você virar um donut, ele tem exatamente a mesma aparência (a menos que ele 




tenha uma cobertura de chocolate, nesse caso é melhor comê-lo simplesmente). Supersimetria é 




um tipo mais sutil de simetria que não pode ser associado a uma transformação do espaço 




comum. Uma das implicações importantes da supersimetria é que as partículas de força e 




partículas de matéria e, portanto, força e matéria, são apenas duas facetas de uma mesma coisa. 




Em termos práticos, isso significa que cada partícula de matéria, tal como um quark, deve ter 




uma partícula parceira que é uma partícula de força, e cada partícula de força, como um fóton, 




deve ter uma partícula parceira que é uma partícula de matéria. Isto tem o potencial de resolver 




o problema de infinidades pois verifica-se que as infinidades de circuitos fechados de partículas 




de força são positivas, enquanto que as infinidades de circuitos fechados de partículas de 




matéria são negativas, então as infinidades na teoria resultantes de partículas de força e suas 




partículas de matéria parceiras tendem a se cancelar. Infelizmente, os cálculos necessários para 




descobrir se haveria alguma infinidades que não tenha sido cancelada em supergravidade eram 




tão longos e difíceis e tinham um tamanho potencial de erro que ninguém estava preparado para 




realizá-los. A maioria dos físicos acreditava, no entanto, que a supergravidade era 




provavelmente a resposta certa para o problema de unificar a gravidade com as outras forças. 




Você poderia pensar que a validade da supersimetria seria uma coisa fácil de verificar - 




simplesmente examinar as propriedades das partículas existentes e ver se elas têm pares. 




Nenhuma de tais partículas parceiras foi observada. Mas, vários cálculos que os físicos 




realizaram indicam que as partículas parceiras correspondentes às partículas que observamos 




deviam ter mil vezes mais massa que um próton, se não até mesmo mais pesada. Isso é pesado 




demais para que tais partículas possam ser vistas em qualquer experiências até hoje, mas há 




esperança de que tais partículas eventualmente sejam criadas no Grande Colisor de Hádrons, 




em Genebra. 




A idéia de supersimetria foi a chave para a criação de supergravidade, mas o conceito tinha 




realmente se originado anos antes com os teóricos estudando uma teoria incipiente chamada 




teoria das cordas. De acordo com a teoria das cordas, partículas não são pontos, mas padrões de 




vibração que têm comprimento, mas sem altura ou largura - como pedaços de corda 




infinitamente finos. As teorias de cordas também levam a infinidades, mas acredita-se que, na 




versão correta todas elas se cancelarão. Elas têm outra característica incomum: Eles são 




consistentes somente se o espaço-tempo tiver dez dimensões, em vez das habituais quatro. Dez 




dimensões pode parecer interessante, mas elas podem causar problemas reais se você esquecer 




onde estacionou o carro. Se eles estiverem presentes, por que não notamos essas dimensões 




extras? De acordo com a teoria das cordas, elas são curvadas para cima em um espaço de 




tamanho muito reduzido. Para visualizar isso, imagine um plano bidimensional. Chamamos o 




plano bidimensional porque você precisa de dois números (por exemplo, coordenadas 




horizontal e vertical) para localizar qualquer ponto nele. Outro espaço bidimensional é a 




superfície de um canudo. Para localizar um ponto naquele espaço, você precisa saber onde ao 




longo do comprimento do canudo está o ponto, e também onde ao longo de sua dimensão 




circular. Mas, se o canudo é muito fino, você obteria uma posição muito aproximada 




empregando apenas a coordenada que corre ao longo do comprimento do canudo, assim você pode ignorar a dimensão circular. E se o canudo medisse um milionésimo de milionésimo de 




milhonésimo de milionésimo de polegada de diâmetro, você absolutamente não perceberia a 




dimensão circular. Essa é a imagem de corda que teóricos têm das extra-dimensões - elas são 




altamente curvadas ou enrolads, em uma escala tão pequena que não as vemos. Em teoria das 




cordas as dimensões extras são enroladas no que é chamado espaço interno, em oposição ao 




espaço tridimensional que experimentamos na vida cotidiana. Como veremos, esses estados 




internos não são apenas dimensões ocultas varrida para debaixo do tapete - elas têm significado 




físico importante. 




Além da questão das dimensões, a teoria das cordas sofria de uma outra questão embaraçosa: 




Parecia haver pelo menos cinco teorias diferentes e milhões de maneiras como as dimensões 




extras podiam ser enroladas para cima, o que era um embaraço de possibilidades para aqueles 




que defendem que a teoria das cordas era a teoria exclusiva de tudo. Então, por volta de 1994, 




as pessoas começaram a descobrir dualidades - que diferentes teorias de cordas e maneiras 




diferentes de se enrolar para cima as dimensões extras são simplesmente maneiras diferentes de 




descrever o mesmo fenômeno em quatro dimensões. Além disso, eles descobriram que 




supergravidade também está relacionada com as outras teorias neste sentido. Os teóricos das 




cordas estão agora convencidos de que as cinco diferentes teorias das cordas e supergravidade 




são apenas abordagens diferentes de uma teoria mais fundamental, dada uma delas válida em 




diferentes situações. 




Essa teoria mais fundamental é chamada teoria-M, conforme mencionamos anteriormente. 




Ninguém parece saber o que o "M" representa, mas pode ser "mestre", "milagre", ou "mistério". 




Parece ser todos os três. As pessoas ainda estão tentando decifrar a natureza da teoria M, mas 




isso pode não ser possível. Pode ser que a expectativa tradicional do físico de uma única teoria 




da natureza seja insustentável, e não existe fórmula única. Pode ser que para descrever o 




universo, temos que empregar teorias diferentes em situações diferentes. Cada teoria pode ter 




sua própria versão da realidade, mas de acordo com o realismo dependente de modelo, isto é 




aceitável desde que as teorias estejam de acordo em suas previsões, sempre que se 




sobreponham, ou seja, quando ambas podem ser aplicadas. 




Mesmo que a teoria-M exista como uma única formulação ou apenas como uma rede, nós 




conhecemos algumas de suas propriedades. Primeiro, a teoria-M tem onze dimensões espaço-




tempo, e não dez. Os teóricos das cordas suspeitavam há muito tempo que a previsão de dez 




dimensões precisava ser ajustada, e trabalhos recentes mostraram que uma dimensão de fato 




tinha sido esquecida. Além disso, a teoria-M pode conter não apenas cordas vibrantes, mas 




também partículas em ponto, membranas bidimensionais, bolhas tri-dimensionais, e outros 




objetos que são mais difíceis de imaginar e ocupam ainda mais dimensões do espaço, até nove. 




Esses objetos são chamados de p-branas (onde p vale um número de zero a nove). 










Canudos e Linhas 

- Um canudo é bidimensional, mas se o seu diâmetro for pequeno o 




suficiente - ou se ele for visto de alguma distância - ele parecerá ser unidimensional, como 




uma linha. 







E que tal o enorme número de maneiras de enrolar para cima as dimensões minúsculas? Na 




teoria M aquelas dimensões extras de espaço não podem ser enroladas apenas de qualquer 




maneira. A matemática da teoria restringe a maneira pela qual as dimensões do espaço interno 




podem ser enroladas. A forma exata do espaço interno determina tanto os valores das 




constantes físicas, tais como a carga do elétron, quanto a natureza das interações entre 




partículas elementares. Em outras palavras, ela determina as leis aparentes da natureza. Por 




"aparente" queremos dizer as leis que observamos em nosso universo - as leis das quatro forças e os parâmetros tais como massa e carga que caracterizam as partículas elementares. Mas as leis 




mais fundamentais são as da Teoria-M. 




As leis da teoria-M, por conseguinte, permitem universos diferentes, com diferentes leis 




aparentes, dependendo de como o espaço interno está enrolado. A Teoria-M tem soluções que 




permitem diferentes espaços internos, talvez até 10500, o que significa que permite a 10.500 




universos diferentes, cada um com suas próprias leis. Para se ter uma idéia de quantos ou o que 




seja, pense nisso: Se alguns ser pudesse analisar as leis previstas para cada um desses universos 




em apenas um milésimo de segundo e tivesse começado a trabalhar nele no big bang, neste 




momento aquele ser teria estudado apenas 1.020 deles. E isso sem pausas para um cafezinho. 




Séculos atrás, Newton demonstrou que as equações matemáticas podiam fornecer uma 




descrição assombrosamente exata de como os objetos interagem, tanto na terra quanto no céu. 




Os cientistas foram levados a crer que o futuro do universo inteiro poderia ser definido se nós 




soubéssemos a teoria correta e tivéssemo bastante poder de computação. Depois, veio a 




incerteza quântica, o espaço curvo, os quarks, as cordas, e dimensões extra, e o resultado 




líquido do seu trabalho é 10500 universos, cada um com diferentes leis, das quais apenas uma 




delas corresponde ao universo como o conhecemos. A esperança original dos físicos de 




produzir uma teoria única que explicasse as leis aparentes de nosso universo como única 




conseqüência possível de algumas hipóteses simples pode ter que ser abandonada. Onde isso 




nos deixa? Se a teoria-M permite 10.500 conjuntos de leis aparentes, como foi que nós 




acabamos neste universo, com as leis que são evidentes para nós? E sobre aqueles outros 




mundos possíveis? 




6

ESCOLHENDO O NOSSO UNIVERSO 




 




SEGUNDO O POVO BOSHONGO da África Central, no início havia apenas escuridão, água e 




o grande deus Bumba. Um dia, Bumba com dor de estômago, vomitou o sol. Com o tempo, o 




sol secou um pouco da água, deixando a terra. Mas, Bumba ainda estava com dor, e vomitou 




alguma coisa mais. Lá em cima veio a lua, as estrelas, e então alguns animais: o leopardo, o 




crocodilo, a tartaruga e, finalmente, o homem. Os Maias do México e da América Central 




contam sobre um tempo semelhante antes da criação, quando tudo o que existia era o mar, o 




céu, e o Criador. Na lenda maia, o Criador, infeliz porque não havia ninguém para elogiá-lo, 




criou a terra, montanhas, árvores, e a maioria dos animais. Mas, os animais não podiam falar, e 




então ele decidiu criar o homem. Primeiro, ele os fez de lama e terra, mas eles só falavam 




bobagem. Ele os deixou dissolver e tentou novamente, desta vez fazendo pessoas de madeira. 




Essas pessoas eram maçantes. Ele decidiu destruí-las, mas elas fugiram para a floresta, sofrendo 




lesões pelo caminho que as alteraram ligeiramente, criando o que hoje conhecemos como 




macacos. Depois desse fiasco, o Criador, finalmente chegou a uma fórmula que funcionou, e 




construiu os primeiros seres humanos a partir de milho branco e amarelo. Hoje, fazemos o 




etanol de milho, mas até agora não se compara ao feito do Criador de construir as pessoas que o 




bebem. 




Mitos de criação como estes tentam, todos, responder às perguntas que abordamos neste livro: 




Por que existe um universo, e por que o universo é do jeito que é? Nossa habilidade em lidar 




com tais questões tem crescido constantemente ao longo dos séculos, desde os gregos antigos e 




mais profundamente ao longo do século passado. Armados com as informações dos capítulos 




anteriores, estamos agora prontos para oferecer uma possível resposta para estas perguntas. 




Uma coisa que pode ter sido evidente até mesmo nos primeiros tempos era que o Universo ou 




era uma criação muito recente, ou então os seres humanos vêm existindo por apenas uma 




pequena fração da história cósmica. Isso porque a raça humana vem melhorando tão 




rapidamente no conhecimento e na tecnologia que se as pessoas tivessem estado por aqui em 




torno de milhões de anos, a raça humana estaria muito mais adiantada em seu domínio. 




Segundo o Antigo Testamento, Deus criou Adão e Eva em apenas seis dias da criação. O Bispo 




Ussher, primaz de toda a Irlanda de 1625 a 1656, colocou a origem do mundo, ainda mais 




precisamente, às nove da manhã de 27 de outubro de 4004 aC. Temos uma opinião diferente: 




que os seres humanos são uma criação recente, mas que o universo em si começou muito antes, 




cerca de 13,7 bilhões de anos atrás. 




A primeira evidência real científica de que o universo teve um início veio em 1920. Como 




dissemos no Capítulo 3, aquela era uma época em que a maioria dos cientistas acreditava em 




um universo estático, que sempre existiu. A prova em contrário foi indireta, com base nas 




observações de Edwin Hubble feitas com o telescópio de 100 polegadas do Monte Wilson, nas 




colinas acima de Pasadena, Califórnia. Ao analisar o espectro de luz que elas emitem, Hubble 




determinou que quase todas as galáxias estão se para longe de nós, e quanto mais longe elas 




estão, mais rápido elas se movem. Em 1929, ele publicou uma lei relativa à sua taxa de retirada 




até sua distância de nós, e concluiu que o universo está se expandindo. Se isso for verdade, 




então o universo deve ter sido menor no passado. De fato, se extrapolamos para o passado 




distante, toda a matéria e a energia no universo teria estado concentrada em uma região muito 







pequena da densidade e temperatura inimagináveis, e se voltarmos longe o suficiente, haveria 




um momento em que tudo começou - acontecimento que hoje chamamos big bang. 




A idéia de que o universo está se expandindo envolve um pouco de sutileza. Por exemplo, não 




queremos dizer que o universo está se expandindo de forma que, digamos, pode-se expandir a 




própria casa, derrubando uma parede e posicionando um novo banheiro, onde uma vez estava 




um carvalho majestoso. Ao invés de ampliar o próprio espaço, é a distância entre dois pontos 




dentro do universo que está aumentando. Essa idéia surgiu na década de 30 em meio a muita 




polêmica, mas uma das melhores maneiras de visualiza-la ainda é uma metáfora enunciada em 




1931 pelo astrônomo Arthur Eddington da Cambridge University. Eddington visualizava o 




universo como a superfície de um balão em expansão, e todas as galáxias como pontos nessa 




superfície. Esta imagem ilustra claramente a razão pela qual galáxias distantes se afastam muito 




mais rapidamente do que as suas vizinhas. Por exemplo, se o raio do balão dobrar a cada hora, 




então a distância entre duas galáxias no balão dobraria a cada hora. Se em algum momento 




foram duas galáxias estão a 1 centímetro de distância, uma hora mais tarde, estariam a 2 




centímetros de distância, e elas pareceriam estar se movendo em relação umas às outras a uma 




velocidade de 1 centímetro por hora. Mas, se eles começaram com 2 centímetros de distância, 




uma hora mais tarde elas estariam separadas por 4 centímetros e pareceriam estar se afastando 




umas das outras a um ritmo de 2 centímetros por hora. É exatamente isso que Hubble 




descobriu: quanto mais longe uma galáxia, mais rápido ela está se afastando de nós. 




É importante perceber que a expansão do espaço não afeta o tamanho dos objetos materiais, tais 




como galáxias, estrelas, maçãs, átomos, ou outros objetos mantidos juntos por algum tipo de 




força. Por exemplo, se nós circulamos um aglomerado de galáxias no balão, aquele círculo não 




se expandiria à medida que o balão se expandisse. Pelo contrário, porque as galáxias estão 




ligadas por força gravitacional, o círculo e as galáxias dentro dele manteriam seu tamanho e 




configuração, à medida que o balão aumentasse de tamanho. Isto é importante porque podemos 




detectar a expansão somente se nossos instrumentos de medição tiverem tamanhos fixos. Se 




tudo fosse livre para se expandir, então nossos critérios, nossos laboratórios, e assim por diante 




se expandiriam todos proporcionalmente e nós não notaríamos qualquer diferença. 







Universo Balão 

- As galáxias distantes afastam-se de nós como se o cosmos estivesse, todo ele 




na superfície de um balão gigante 







Que o universo estava se expandindo era novidade para Einstein. Mas, a possibilidade de que as 




galáxias estivessem se afastando umas das outras tinha sido proposta alguns anos antes do 




trabalho publicado por Hubble em bases teóricas decorrentes das próprias equações de Einstein. 




Em 1922, o físico e matemático russo Alexander Friedmann investigou o que aconteceria em 




um modelo de universo baseado em dois pressupostos que simplificavam a matemática: que o 




universo parecia idêntico em todas as direções, e que ele teria esta aparência a partir de todos os 




pontos de observação. Sabemos que a primeira suposição de Friedmann não é exatamente 




verdade - o universo, felizmente, não é uniforme em todos os lugares! Se olharmos para cima 




em uma direção, podemos ver o sol; em outra, a lua ou uma colônia de morcegos hematófagos 




em migração. Mas, o universo realmente parece ser praticamente o mesmo em todas as direções 




quando visto em uma escala que é muito maior - maior até mesmo que a distância entre 




galáxias. É algo como olhar de cima para baixo para uma floresta. Se você estiver perto o 




suficiente, poderá distinguir as folhas individuais, ou, pelo menos as árvores, e os espaços entre 




elas. Mas, se você está tão alto que se você estender o braço e olhar seu polegar e este abrange 




um quilômetro quadrado de árvores, a floresta parecerá ser uma sombra uniforme de verde. 




Poderíamos dizer que, nessa escala, a floresta é uniforme. 




Com base em seus pressupostos Friedmann foi capaz de descobrir uma solução para as 




equações de Einstein em que o universo se expandia da forma que Hubble logo descobriria ser 




verdadeiro. Em particular, o modelo de universo de Friedmann começa com tamanho zero e se 




expande até que a atração gravitacional diminui e, eventualmente, faz com que ele se feche 




sobre si mesmo. (Há, ao que parece, dois outros tipos de soluções para as equações de Einstein, 




que também satisfazem as hipóteses do modelo de Friedmann, uma corresponde a um universo 




em que a expansão continua para sempre, embora ele desacelere um pouco, e outra a um 




universo em que a taxa de expansão diminui tendendo a zero, mas nunca o alcança 




efetivamente.) Friedmann morreu alguns anos depois de produzir este trabalho, e suas idéias 




permaneceram muito desconhecidas até a descoberta de Hubble. Mas, em 1927, um professor 




de física e padre católico chamado Georges Lemaître propôs uma idéia semelhante: Se você 




traçar a história do universo de volta ao passado, ele se torna cada vez menor até chegar a uma 




evento de criação - que hoje chamamos de big bang. 




Nem todos gostaram da imagem do big bang. De fato, o termo "big bang" foi cunhado em 1949 




pelo astrofísico de Cambridge, Fred Hoyle, que acreditava em um universo que se expandia 




para sempre, e atribuia ao termo uma natureza descrição irônica. As primeiras observações 




diretas de apoio a idéia só vieram em 1965, com a descoberta de que existe um fundo tênue de 




micro-ondas através do espaço. Esta radiação de microondas cósmicas de fundo, ou CMBR é a 




mesma que a de seu forno de microondas, mas muito menos poderosa. Você pode observar o 




RCF em si ao ajustar sua televisão para um canal não utilizado - alguma porcentagem do 




chuvisco que você vê na tela será causada por ela. A radiação foi descoberta acidentalmente por 




dois cientistas do Bell Labs tentando eliminar esta estática de sua antena de microondas. A 




princípio, eles pensaram que a estática poderia ser proveniente do excremento de pombos 




empoleirados em seus aparelhos, mas acabou-se descobrindo que o problema deles tinha uma 




origem mais interessante - a CMBR é a radiação remanescente do universo inicial muito quente 




e denso que teria existido por um breve período após o Big Bang. À medida que o universo se 




expandiu, ele se resfriou até que a radiação tornou-se apenas o fraco remanescente que observamos hoje. Atualmente, estas microondas poderiam aquecer a sua comida para apenas 




cerca de -270 graus centígrados - 3 graus acima do zero absoluto, e não é muito útil para 




arrebentar pipoca. 




Os astrônomos também encontraram outras impressões digitais, dando apoio à imagem do big 




bang de um universo inicial pequueno e quente. Por exemplo, durante o primeiro minuto mais 




ou menos, o universo teria sido mais quente que o centro de uma estrela típica. Durante esse 




período, todo o universo teria funcionado como um reator de fusão nuclear. As reações teriam 




cessado quando o universo se expandiu e esfriou o suficiente, mas a teoria prevê que este 




deveria ter deixado um universo composto principalmente de hidrogênio, mas também de cerca 




de 23 por cento de hélio, com traços de lítio (todos os elementos mais pesados foram criados 




mais tarde, no interior das estrelas). O cálculo está em bom acordo com as quantidades de hélio, 




hidrogênio e lítio que observamos. 




Medições da abundância de hélio e o CMBR apresentaram provas convincentes em favor da 




imagem do big bang do universo inicial, mas embora se possa pensar a imagem do Big Bang 




como uma descrição válida dos primeiros tempos, é errado tomar o big bang literalmente, ou 




seja, pensar a teoria de Einstein como fornecendo uma imagem verdadeira da origem do 




universo. Isso se deve ao fato de que a relatividade geral prediz que existe um momento no 




tempo em que a temperatura, densidade e curvatura do universo são todas infinitas, uma 




situação que os matemáticos chamam de singularidade. Para um físico, isto significa que a 




teoria de Einstein se rompe naquele ponto e, portanto, não pode ser usada para prever como o 




universo começou, apenas como ele se desenvolveu mais tarde. Assim, embora possamos 




utilizar as equações da relatividade geral e as nossas observações do céu para aprender sobre o 




universo em uma idade muito jovem, não é correto carregar a imagem do Big Bang todo o 




caminho de volta até o começo. 




Vamos chegar brevemente até a questão da origem do universo, mas primeiro algumas palavras 




sobre a primeira fase da expansão. Os físicos a chamam de inflação. A menos que você tenha 




vivido no Zimbabué, onde a inflação da moeda ultrapassou recentemente 200 milhões por 




cento, o termo pode não soar muito explosivo. Mas de acordo até mesmo com estimativas 




conservadoras, durante a inflação cosmológica, o universo se expandiu por um fator de 




1.000.000.000.000.000.000.000.000.000.000 em .00000000000000000000000000000000001 




segundo. Era como se uma moeda de 1 centímetro de diâmetro, de repente, explodisse dez 




milhões de vezes até a largura da Via Láctea. Isso pode parecer violar a relatividade, que 




determina que nada pode se mover mais rápido que a luz, mas aquele limite de velocidade não 




se aplica à expansão do próprio espaço. 




A idéia de que tal episódio de inflação possa ter ocorrido foi proposta pela primeira vez em 




1980, com base em considerações que vão além da teoria de Einstein da relatividade geral e 




levam em conta aspectos da teoria quântica. Como não temos uma teoria quântica completa da 




gravidade, os detalhes ainda estão sendo trabalhados, e os físicos não têm certeza exatamente de 




como a inflação aconteceu. Mas, de acordo com a teoria, a expansão provocada pela inflação 




não seria completamente uniforme, conforme previsto pela imagem tradicional do big bang. 




Estas irregularidades produziria variações minúsculas de temperatura da CMBR em diferentes 




direções. As variações são pequenas demais para terem sido observadas na década de 1960, mas 




elas foram descobertos em 1992 pelo satélite COBE, da Nasa, e, posteriormente, medidas pelo 




seu sucessor, o satélite WMAP lançado em 2001. Como resultado, agora estamos confiantes de que a inflação realmente aconteceu. 




Ironicamente, apesar de pequenas variações na CMBR serem evidências da inflação, uma razão 




para que a inflação seja um conceito importante é a uniformidade quase perfeita da temperatura 




da CMBR. Se você tornar uma parte de um objeto mais quente que seus arredores e depois 




esperar, o ponto quente vai se tornar mais frio e seus arredores mais quentes até que a 




temperatura do objeto seja uniforme. Da mesma forma, seria de se esperar que o universo 




acabasse por ter uma temperatura uniforme. Mas, este processo leva tempo, e se a inflação não 




tivesse ocorrido, não teria havido tempo suficiente na história do universo para que o calor em 




regiões amplamente separadas se igualasse, supondo-se que a tal velocidade de transferência de 




calor seja limitada pela velocidade da luz. Um período de expansão muito rápida (muito mais 




rápida que a velocidade da luz) corrige isso porque teria havido tempo suficiente para que a 




equalização acontecesse no universo pré-inflacionário pequeno inicial. 




A inflação explica a explosão do Big Bang, pelo menos no sentido de que a expansão que ele 




representa foi muito mais extrema que a expansão prevista pela teoria tradicional do big bang 




da relatividade geral, durante o intervalo de tempo em que a inflação ocorreu. O problema é 




que, para que nossos modelos teóricos da inflação funcionem, o estado inicial do universo tinha 




que ser criado de uma forma muito especial e altamente improvável. Assim, a teoria da inflação 




tradicionais resolve um conjunto de questões, mas cria outro - a necessidade de um estado 




inicial muito especial. Essa questão da hora-zero é eliminada na teoria da criação do universo 




que estamos prestes a descrever. 




Já que não podemos descrever a criação empregando a teoria de Einstein da relatividade geral, 




se quisermos descrever a origem do universo, a relatividade geral tem de ser substituída por 




uma teoria mais completa. Seria de se esperar que a necessidade de uma teoria mais completa, 




mesmo que a relatividade geral não se tenha rompido, porque a relatividade geral não leva em 




conta a estrutura da matéria em pequena escala, que é regida pela teoria quântica. Nós 




mencionamos no capítulo 4, que para a maioria dos efeitos práticos, a teoria quântica não 




possui muita relevância para o estudo da estrutura em grande escala do universo, porque a 




teoria quântica se aplica à descrição da natureza em escalas microscópicas. Mas, se você voltar 




suficientemente longe no tempo, o universo era tão pequeno como o tamanho de Planck, um 




bilhão de trilhões de trilhonésimo de centímetro, que é a escala na qual a teoria quântica tem de 




ser levada em conta. Assim, embora nós ainda não tenhamos uma teoria quântica completa da 




gravidade, sabemos que a origem do universo foi um evento quântico. Como resultado, assim 




como nós combinamos a teoria quântica e a relatividade geral - pelo menos provisoriamente - 




para derivar a teoria da inflação, se queremos voltar ainda mais e compreender a origem do 




universo, temos de combinar o que sabemos sobre a relatividade geral com a teoria quântica. 




Para ver como isso funciona, precisamos entender o princípio de que a gravidade deforma o 




espaço e o tempo. A deformação do espaço é mais fácil de visualizar que a deformação do 




tempo. Imagine que o universo é a superfície plana de uma mesa de bilhar. A superfície da 




mesa é um espaço plano, pelo menos em duas dimensões. Se você rolar a bola sobre a mesa ela 




vai se deslocar em uma linha reta. Mas se a mesa se deformar ou ficar amassada em alguns 




lugares, conforme a ilustração abaixo, então a bola vai fazer uma curva. 










Dobra espacial 

- Matéria e energia dobram o espaço, alterando o caminho de objetos. 







É fácil ver como a mesa de bilhar é deformada, neste exemplo, porque ela está se curvando em 




uma terceira dimensão externa que podemos ver. Já que não pode dar um passo atrás fora de 




nosso próprio espaço-tempo para visualizar sua deformação, a deformação do espaço-tempo em 




nosso universo é mais difícil de imaginar. Mas a curvatura pode ser detectada, mesmo se você 




não puder sair e vê-lo a partir da perspectiva de um espaço maior. Ela pode ser detectada dentro 




do próprio espaço. Imagine uma micro-formiga confinada à superfície da mesa. Mesmo sem a 




capacidade de sair da mesa, a formiga poderia detectar a deformação mapeando 




cuidadosamente as distâncias. Por exemplo, a distância ao redor de um círculo no espaço plano 




é sempre um pouco mais que três vezes a distância em seu diâmetro (o múltiplo real é π). Mas, 




se a formiga corta um círculo que engloba o poço na mesa retratado acima, ele descobriria que a 




distância para atravessar é maior que a esperada, superior a um terço da distância em torno dela. 




De fato, se o poço fosse profundo o suficiente, a formiga descobriria que a distância ao redor do 




círculo é menor do que a distância através dele. O mesmo é verdadeiro em relação à 




deformação em nosso universo - ela estica ou comprime a distância entre pontos do espaço, 




alterando sua geometria, ou forma, de uma maneira que seja mensurável a partir do interior do 




universo. A deformação de tempo estica ou comprime intervalos de tempo, de maneira análoga. 










Deformaçao do espaço-tempo 

- Matéria e energia deformam o tempo e fazem com que a 




dimensão tempo se "misture" com as dimensões do espaço. 







Armado com essas idéias, vamos voltar à questão do início do universo. Nós podemos falar 




separadamente de espaço e tempo, como fizenos neste debate, em situações que envolvem 




baixas velocidades e gravidade fraca. Em geral, porém, tempo e espaço podem se tornar 




entrelaçados, e assim o seu alongamento e compressão também envolvem uma certa quantidade 




de mistura. Essa mistura é importante no início do Universo e a chave para a compreensão do 




início do tempo. 




A questão do começo do tempo é um pouco como a questão da borda do mundo. Quando as 




pessoas pensavam que o mundo era plano, pode se ter perguntado se o mar transbordava sobre a 




sua borda. Isso foi testado experimentalmente: Pode-se ir ao redor do mundo e não cair. O 




problema do que acontece na orla do mundo foi resolvido quando as pessoas perceberam que o 




mundo não era uma chapa plana, mas uma superfície curva. O tempo, no entanto, parecia ser 




um modelo moderno de ferrovia. Se tivesse havido um início, teria que ter havido alguém (ou 




seja, Deus) para colocar os trens em movimento. Embora a teoria geral da relatividade de 




Einstein unificasse tempo e espaço como espaço-tempo e envolvesse uma certa mistura espaço 




e tempo, este ainda era diferente do espaço, e nenhum deles tinha um começo e um fim, mas 




continuavam para sempre. No entanto, uma vez que adicionamos os efeitos da teoria quântica à 




teoria da relatividade, em casos extremos a deformação pode ocorrer de tal forma grande que o 




tempo se comporta como outra dimensão do espaço. 




No início do universo - quando o universo era pequeno o suficiente para ser governado tanto 




pela relatividade geral quanto pela teoria quântica - havia efetivamente quatro dimensões de 




espaço e nenhuma de tempo. Isso significa que, quando falamos do "início" do universo, 




estamos evitando a questão sutil de que quando olhamos para trás em direção ao universo muito 




inicial, o tempo como nós o conhecemos não existe! Temos de aceitar que nossas idéias 




comuns de tempo e espaço não se aplicam ao universo em seu início. Isso está além de nossa 




experiência, mas não além da nossa imaginação, ou da nossa matemática. Se no início do universo todas as quatro dimensões se comportam como espaço, o que acontece com o início do 




tempo? 




A constatação de que o tempo pode se comportar como uma outra direção do espaço significa 




que se pode livrar do problema do tempo tendo um começo, de maneira semelhante à de que 




nos livramos da borda do mundo. Suponhamos que o início do universo era como o Pólo Sul da 




Terra, com graus de latitude desempenhando o papel do tempo. À medida que se move para o 




norte, os círculos de latitude constante, representando o tamanho do universo, se expandiriam. 




O universo começaria como um ponto no Pólo Sul, mas o Pólo Sul é muito parecido com 




qualquer outro ponto. Perguntar o que aconteceu antes do início do universo seria uma pergunta 




sem sentido, porque não há nada ao sul do pólo sul. Neste quadro o espaço-tempo não tem 




limite - as mesmas leis da natureza se sustentam no Pólo Sul assim como em outros lugares. De 




forma análoga, quando combinamos a teoria geral da relatividade com a teoria quântica, a 




questão do que aconteceu antes do início do universo torna-se sem sentido. Essa idéia de que 




histórias devem ser superfícies fechadas sem limites é chamada de condição sem-limite. 




No correr dos séculos, muitos, incluindo Aristóteles, acreditavam que o universo deve ter 




sempre existido, a fim de evitar a questão de como ele foi criado. Outros acreditavam que o 




universo teve um começo, e usavam isso como um argumento para a existência de Deus. A 




constatação de que o tempo se comporta como o espaço apresenta uma nova alternativa. Ela 




remove a antiga objeção de o universo ter tido um começo, mas também significa que o início 




do universo era governado pelas leis da ciência, e não precisam ser postas em movimento por 




algum deus. 




Se a origem do universo foi um evento quântico, este deve ser descrito com precisão pela soma 




das histórias de Feynman. Para aplicar a teoria quântica ao universo inteiro - onde os 




observadores fazem parte do sistema que está sendo observado - no entanto, é complicado. No 




Capítulo 4, vimos como as partículas de matéria disparadas contra uma tela com duas fenda 




poderia produzir padrões de interferência, do mesmo modo que ondas de água. Feynman 




mostrou que isso acontece porque uma partícula não tem uma única história. Isto é, enquanto se 




move de seu ponto de partida até algum ponto final B, ela não tem um caminho definido, mas, 




ao mesmo tempo, toma cada caminho possível ligando os dois pontos. Deste ponto de vista, a 




interferência não é nenhuma surpresa porque, por exemplo, a partícula pode viajar através das 




duas fendas ao mesmo tempo, e interferir consigo mesma. Aplicado ao movimento de uma 




partícula, o método de Feynman nos diz que para calcular a probabilidade de qualquer ponto 




final particular, precisamos considerar todas as histórias possíveis que a partícula poderia seguir 




desde seu ponto de partida até ponto final. Também é possível usar métodos de Feynman para 




calcular as probabilidades quânticas para observações do universo. Se eles forem aplicados ao 




universo como um todo, não existe um ponto A, assim somamos todas as histórias que 




satisfazem a condição sem-limite e acabamos com o universo que vemos hoje. 




Nesta visão, o universo surgiu de forma espontânea, partindo de todas as maneiras possíveis. A 




maioria destes corresponde a outros universos. Embora alguns desses universos sejam 




semelhantes ao nosso, a maioria é muito diferente. Eles não são diferentes apenas em detalhes, 




tais como se Elvis realmente morreu jovem, ou se os nabos são comidos na sobremesa, mas, ao 




invés, eles diferem até mesmo em suas aparentes leis da natureza. Na verdade, existem muitos 




universos com muitos conjuntos de leis físicas diferentes. Algumas pessoas fazem um grande 







mistério dessa idéia, às vezes chamada de conceito multiverso, mas estas são apenas expressões 




diferentes da soma das histórias de Feynman. 




Para representar isso, vamos alterar a analogia do balão de Eddington e, ao invés, pensar o 




universo em expansão como a superfície de uma bolha. Nossa imagem da criação espontânea 




quântica do universo é, então, um pouco como a formação de bolhas de vapor em água 




fervente. Muitas pequenas bolhas aparecem e, em seguida, desaparecem novamente. Estas 




representam mini-universos que se expandem, mas se fecham novamente enquanto ainda são de 




tamanho microscópico. Elas representam possíveis universos alternativos, mas não são de 




grande interesse, uma vez que não duram o tempo suficiente para desenvolver as galáxias e 




estrelas, e quanto mais vida inteligente. Algumas dessas pequenas bolhas, no entanto, crescerão 




o suficiente para que estejam a salvo do colapso. Eles continuarão a se expandir em um ritmo 




cada vez maior e formarão as bolhas de vapor que somos capazes de ver. Estas correspondem a 




universos que começam a crescer a uma taxa cada vez maior - em outras palavras, os universos 




em estado de inflação. 







Multiverso 

- Flutuações quânticas levam à criação de pequenos universos a partir do nada. 




Algumas dessas atingem um tamanho crítico, em seguida, expandem-se de forma inflacionária, 




formando galáxias, estrelas, e em pelo menos um caso, seres como nós 







. 




Como dissemos, a expansão provocada pela inflação não seria completamente uniforme. Na 




soma das histórias, há apenas uma história completamente uniforme e regular, e ela terá a maior 




probabilidade, mas muitas outras histórias que são muito ligeiramente irregulares terão 




probabilidades que são quase tão elevadas. É por isso que a inflação prevê ser provável que o 




universo inicial tenha sido ser ligeiramente não-uniforme, correspondente às pequenas 




variações de temperatura observadas na CMBR. As irregularidades no início do universo são 




uma sorte para nós. Por quê? Homogeneidade é bom se você não quer que o creme se separe de 




seu leite, mas um universo uniforme é um universo maçante. As irregularidades no início do 







universo são importantes porque, se algumas regiões tinham uma densidade ligeiramente 




superior às demais, a atração gravitacional da densidade extra atrasaria a expansão daquela 




região em comparação com seu entorno. À medida que a força da gravidade lentamente atrai a 




matéria, ela pode eventualmente provocar seu colapso para formar galáxias e estrelas, o que 




pode levar a planetas e, em pelo menos um caso, a pessoas. Então, olhe atentamente para o 




mapa do céu de microondas. Ele é o modelo para toda a estrutura do universo. Nós somos o 




produto das flutuações quânticas no universo muito inicial. Se fossemos religiosos, poderíamos 




dizer que Deus realmente joga dados. 







O fundo de microondas 

- Esta imagem do Céu foi criado a partir de sete anos de dados do 




WMAP lançado em 2010. Eles revelam variações de temperatura - mostram como as 




diferenças de cor - que remontam a 13,7 bilhões de anos atrás. A flutuação na foto corresponde 




a diferenças de temperatura de menos de um milésimo de grau na escala Celsius. No entanto, 




elas foram as sementes que cresceram para se tornar galáxias. Crédito: NASA / WMAP 




Science Team. 







Esta idéia leva a uma visão do universo, que é profundamente diferente do conceito tradicional, 




obrigando-nos a ajustar a forma como pensamos sobre a história do universo. A fim de fazer 




previsões em cosmologia, é preciso calcular as probabilidades de estados diferentes de todo o 




universo, no momento presente. Em física, normalmente se presume algum estado inicial de um 




sistema e evolui-se à frente no tempo empregando equações matemáticas pertinentes. Dado o 




estado de um sistema em um dado momento, tenta-se calcular a probabilidade de que o sistema 




estará em algum estado diferente em um momento posterior. A suposição usual na cosmologia 




é que o universo tem uma história simples e definida. Pode-se usar as leis da física para calcular 




como essa história se desenvolve com o tempo. Chamamos isso de abordagem "de baixo para 




cima" da cosmologia. Mas já que precisamos levar em conta a natureza quântica do universo, 




conforme expressa pela soma das histórias de Feynman, a amplitude de probabilidade de que o 




universo está agora em um estado específico é obtida através da soma das contribuições de 




todas as histórias que satisfaçam a condição sem-limites e terminem no estado em questão. Em 




cosmologia, em outras palavras, não se deve seguir a história do universo de baixo para cima, 




porque isso pressupõe que há uma única história, com um ponto inicial e evolução bem definidos. Em vez disso, deve-se traçar as histórias de cima para baixo, para trás a partir do 




momento presente. Algumas histórias serão mais prováveis que outras, e a soma será 




normalmente dominada por uma única história que começa com a criação do universo e 




culmina no estado em questão. Mas, haverá diferentes histórias para diferentes estados 




possíveis do universo no momento presente. Isto leva a uma visão radicalmente diferente da 




cosmologia e da relação entre causa e efeito. As histórias que contribuem para a soma de 




Feynman não têm uma existência independente, mas dependem do que está sendo medido. Nós 




criamos a história através de nossa observação, ao invés de a história nos criar. 




A idéia de que o universo não tem uma história única independente do observador pode parecer 




estar em conflito com certos fatos que conhecemos. Deveria haver uma história em que a lua 




fosse feita de queijo Roquefort. Mas, observamos que a lua não é feita de queijo, o que é uma 




má notícia para os ratos. Daí, as histórias em que a lua é feita de queijo não contribuem para o 




atual estado de nosso universo, embora possam contribuir para outros. Isso pode soar como 




ficção científica, mas não é. 




Uma importante implicação da abordagem de cima para baixo é que as leis aparentes da 




natureza dependem da história do universo. Muitos cientistas acreditam que existe uma única 




teoria que explica aquelas leis, bem como as constantes físicas da natureza, tais como a massa 




do elétron ou a dimensionalidade do espaço-tempo. Mas a cosmologia de cima para baixo 




determina que as leis aparentes da natureza são diferentes para diferentes histórias. 




Consideremos a dimensão aparente do universo. De acordo com a teoria-M, o espaço-tempo 




tem dez dimensões espaciais e uma dimensão de tempo. A idéia é que sete das dimensões 




espaciais sejam enroladas para cima, tão pequenas que não as notamos, deixando-nos com a 




ilusão de que tudo o que existe são as três grandes dimensões restantes com as quais estamos 




acostumados. Uma das questões centrais em aberto na teoria-M é: Por que, em nosso universo, 




não existem mais dimensões grandes, e por que todas as dimensões são enroladas para cima? 




Muitas pessoas gostariam de acreditar que existe algum mecanismo que faz com que todas 




menos três das dimensões de espaço enrolem-se espontaneamente. Como alternativa, talvez 




todas as dimensões começaram pequenas, mas por alguma razão incompreensível três 




dimensões espaciais se expandiram e o resto não. Parece, no entanto, que não há qualquer razão 




dinâmica para que o universo pareça quadridimensional. Ao invés, a cosmologia de cima para 




baixo prevê que o número de dimensões espaciais grandes não é fixado por qualquer princípio 




da física. Haverá uma amplitude de probabilidade quântica para cada número de dimensões 




espacias grandes de zero a dez. A soma de Feynman permite todas estas, todas as histórias 




possíveis para o universo, mas a observação de que nosso universo tem três dimensões 




espaciais grandes seleciona na subclasse de histórias que têm a propriedade que está sendo 




observada. Em outras palavras, a probabilidade quântica de que o universo tenha mais ou 




menos que três dimensões espaciais grandes é irrelevante, porque já determinamos que nós 




estamos em um universo com três grandes dimensões espaciais. Então, enquanto a amplitude de 




probabilidade para três dimensões espaciais grandes não é exatamente zero, não importa quão 




pequeno ela seja, comparado à amplitude de probabilidade para outros números de dimensões. 




Seria como perguntar sobre a amplitude de probabilidade de que o papa atual seja chinês. 




Sabemos que ele é alemão, embora a probabilidade seja maior de que ele seja chinês porque há 




mais chineses que alemães. Da mesma forma, sabemos que nosso universo exibe três 




dimensões espaciais grandes, e assim, mesmo que os outros números de dimensões espaciais grandes possa ter uma amplitude de probabilidade maior, estamos interessados apenas em 




histórias com três. 




E sobre as dimensões enroladas para cima? Lembre-se que na teoria-M a forma precisa das 




dimensões enroladas para cima restantes - o espaço interno - determina tantos os valores de 




grandezas físicas, tais como a carga sobre o elétron quanto a natureza das interações entre 




partículas elementares, ou seja, as forças da natureza. As coisas teriam saido caprichadas, se a 




teoria-M tivesse permitido apenas uma forma para a dimensão ondulada, ou talvez algumas, e 




apenas uma das quais poderia ter sido descartada de alguma forma, deixando-nos com apenas 




uma possibilidade para as leis a natureza aparentes. Em vez disso, existem amplitudes de 




probabilidade para talvez até 10.500 diferentes espaços internos, cada um deles levando a 




diferentes leis e valores para as constantes físicas. 




Se se constrói a história do universo de baixo para cima, não há razão para que o universo 




acabasse com o espaço interno para a interação entre partículas que realmente observamos, o 




modelo padrão (de interações de partículas elementares). Mas, na abordagem de cima para 




baixo, aceitamos que existam universos com todos os espaços internos possíveis. Em alguns 




universos, os elétrons têm o peso das bolas de golfe e a força da gravidade é mais forte do que o 




magnetismo. No nosso, aplica-se o modelo padrão, com todos os seus parâmetros. Pode-se 




calcular a amplitude de probabilidade para o espaço interno, que leva ao modelo padrão com 




base na condição sem-limite. Tal como acontece com a probabilidade de existir um universo 




com três dimensões espaciais grandes, não importa quão pequena esta amplitude seja com 




relação a outras possibilidades, pois já observamos que o modelo padrão descreve o nosso 




universo. 




A teoria que descrevemos neste capítulo é testável. Nos exemplos anteriores, enfatizamos que 




as amplitudes de probabilidade relativas para universos radicalmente diferentes, tais como 




aqueles com um número diferente de dimensões espaciais grandes, não importa. As amplitudes 




de probabilidade relativas para universos vizinhos (ou seja, similares), no entanto, são 




importantes. A condição sem-limite implica em que a amplitude de probabilidade seja maior 




para as histórias em que o universo começa completamente suave. A amplitude é reduzida para 




universos que são mais irregulares. Isso significa que o universo primitivo teria sido quase 




suave, mas com pequenas irregularidades. Conforme vimos, podemos observar estas 




irregularidades como pequenas variações nas microondas vindas de diferentes direções no céu. 




Descobriu-se que elas estão de acordo exatamente com as exigências gerais da teoria da 




inflação; no entanto, medidas mais precisas são necessárias para diferenciar completamente a 




teoria de cima para baixo de outras, e apoiá-la ou refutá-la. Isto poderia ser realizado muito bem 




por satélites no futuro. 




Centenas de anos atrás, as pessoas pensavam que a terra era única, e situada no centro do 




universo. Hoje sabemos que existem centenas de bilhões de estrelas em nossa galáxia, uma 




grande porcentagem delas com sistemas planetários, e centenas de bilhões de galáxias. Os 




resultados descritos neste capítulo, indicam que o nosso universo em si é também um de 




muitos, e que suas leis aparentes não são determinadas de maneira exclusiva. Isso deve ser 




decepcionante para quem esperava que uma teoria definitiva, uma teoria de tudo, poderia prever 




a natureza da física do cotidiano. Não podemos prever características discretas, tais como o 




número de dimensões espaciais grandes ou o espaço interno que determina as quantidades 




físicas que observamos (por exemplo, a massa e a carga do elétron e outras partículas elementares). Em vez disso, usamos estes números para selecionar quais histórias contribuem 




para a soma de Feynman. 




Parece que estamos em um momento crítico na história da ciência, em que devemos modificar 




nossa concepção dos objetivos e do que torna uma teoria física aceitável. Parece que os 




números fundamentais, e até mesmo a forma das leis aparentes da natureza não são exigidas 




pela lógica ou princípio físico. Os parâmetros são livres para assumir muitos valores, e as leis 




de assumir qualquer forma que conduza a uma teoria matemática auto-consistente, e elas 




assumem valores diferentes e formas diferentes em universos diferentes. Isso pode não 




satisfazer nossos desejos humano de ser especial ou de descobrir uma embalagem pura para 




conter todas as leis da física, mas esta parece ser a maneira da natureza. 




Parece haver uma vasta paisagem de universos possíveis. No entanto, como veremos no 




próximo capítulo, os universos em que pode existir vida como a nossa são raros. Vivemos em 




um em que a vida é possível, mas se o universo fosse apenas ligeiramente diferente, seres como 




nós não poderiam existir. O que devemos a fazer deste ajuste fino? É isso evidência de que o 




universo, afinal, foi projetado por um criador benevolente? Ou será que a ciência oferece outra 




explicação? 




 




7

O MILAGRE APARENTE 










OS CHINESES CONTAM DE UM TEMPO durante a dinastia Hsia (ca. 2205—ca. 1782 aC), 




quando nosso ambiente cósmico mudou repentinamente. Dez sóis apareceram no céu. O povo 




na terra sofreu enormemente com o calor, por isso o imperador ordenou um famoso arqueiro 




que derrubasse os sóis extras. O arqueiro foi recompensado com uma pílula que tinha o poder 




de torná-lo imortal, mas sua esposa a roubou. Por esse delito, ela foi banida para a lua. 




Os chineses estavam certos em pensar que um sistema solar com dez sóis não é compatível com 




a vida humana. Hoje sabemos que, embora talvez oferecendo excelentes oportunidades para 




bronzeamento, qualquer sistema solar com vários sóis provavelmente nunca permitiria que a 




vida se desenvolvesse. As razões não são tão simples quanto o calor escaldante imaginado na 




lenda chinesa. Na verdade, um planeta poderia enfrentar uma temperatura agradável enquanto 




orbita estrelas múltiplas, pelo menos por um tempo. Mas, o aquecimento uniforme por longos 




períodos de tempo, uma situação que parece ser necessária para a vida, seria pouco provável. 




Para entender por que, vamos ver o que acontece no mais simples tipo de sistema de estrelas 




múltiplas, um com dois sóis, que é chamado sistema binário. Cerca de metade de todas as 




estrelas no céu pertencem a tais sistemas. Mas, até mesmo a simples sistemas binários podem 




manter apenas determinados tipos de órbitas estáveis, do tipo mostrado abaixo. Em cada uma 




dessas órbitas provavelmente haveria um tempo em que o planeta seria quente demais ou frio 




demais para sustentar a vida. A situação é ainda pior para agrupamentos contendo muitas 




estrelas. 




Nosso sistema solar tem outra propriedades "de sorte" sem as quais sofisticadas formas de vida nunca poderiam ter evoluído. Por exemplo, as leis de Newton permitem que órbitas planetárias 




sejam círculos ou elipses (elipses são círculos amassado, mais largos longo de um eixo mais 




estreitos ao longo de outro). O grau em que tal elipse é amassada é descrito pelo que se chama 




de sua excentricidade, um número entre zero e um. Uma excentricidade próxima de zero 




significa que a figura se assemelha a um círculo, enquanto que uma excentricidade próxima de 




um significa que é muito achatada. Kepler foi perturbado pela idéia de que os planetas não se 




movem em círculos perfeitos, mas a órbita da Terra tem uma excentricidade de apenas cerca de 




2 por cento, o que significa que é quase circular. Como se vê, este foi um golpe de muita sorte. 










Órbitas Binárias

 - Planetas que orbitam sistemas de estrelas binárias, provavelmente terão 




clima inóspito, em algumas estações quentes demais para viver, em outras, frios demais. 







Padrões climáticos sazonais na terra são determinados principalmente pela inclinação do eixo 




da terra de rotação em relação ao plano de sua órbita em torno do sol. Durante o inverno no 




Hemisfério Norte, por exemplo, o Pólo Norte se inclina para longe do sol. O fato de que a terra 




está mais próximo do Sol nessa época - apenas 91.5 milhões milhas de distância, ao contrário 




de cerca de 94.5 milhões de milhas longe do sol no início de julho - tem um efeito 




insignificante sobre a temperatura em comparação com o efeito de sua inclinação. Mas em 




planetas com uma grande excentricidade de órbita, a variação da distância até o Sol 




desempenha um papel muito maior. Em Mercúrio, por exemplo, com uma excentricidade de 20 




por cento, a temperatura é mais de 200 graus Fahrenheit mais quent na maior aproximação do 




planeta ao Sol (periélio) do que quando está mais distante do Sol (afélio). De fato, se a 




excentricidade da órbita da terra fosse próxima de um, nossos oceanos entraria em ebulição 




quando chegássemos ao ponto mais próximo ao sol, e congelaríamos quando chegássemos mais 




longe, fazendo com que nem férias de inverno ou de verão fossem muito agradáveis. Grandes 




excentricidades orbitais não são propícias à vida, por isso estamos com sorte de ter um planeta 




para o qual a excentricidade da órbita é próxima de zero. 







Excentricidades

 - A excentricidade é uma medida de quão próxima está a elipse de um 




círculo. Órbitas circulares são propícias para a vida, enquanto que órbitas muito alongadas 




resultam em grandes flutuações sazonais de temperatura. 







Também temos sorte na relação da massa do nosso sol em relação à nossa distância dele. Isso 




porque a massa de uma estrela determina a quantidade de energia que ela emite. As maiores 




estrelas têm uma massa cerca de uma centena de vezes a do nosso Sol, enquanto que as 




menores têm cerca de cem vezes menos massa. E, no entanto, assumindo que a distância Terra-




sol seja dada, se nosso sol tivesse apenas 20 por cento mais ou menos de massa, a Terra seria 




mais fria do que Marte hoje em dia, ou mais quente que Vênus hoje em dia. 







Tradicionalmente, dada qualquer estrela, os cientistas definem a zona habitável como a região 




estreita ao redor da estrela em que as temperaturas são de tal ordem que a água líquida possa 




existir. A zona habitável é às vezes chamada "zona Cachinhos Dourados", porque a exigência de que a água líquida exista significa que, como Cachinhos Dourados, o desenvolvimento de 




vida inteligente exige que as temperaturas do planeta sejam "corretas". A zona habitável do nosso sistema solar, mostrada na foto acima, é minúscula. Felizmente para aqueles de nós que 




são formas de vida inteligentes, a terra caiu dentro dela! 







Zona Cachinhos Dourados 

- Se Cachinhos Dourados estivesse amostrando planetas, ela 




consideraria somente aqueles dentro da Zona Verde apropriado para a vida. A estrela amarela 




representa o nosso próprio sol. As estrelas mais brancas são maiores e mais quentes; as mais 




vermelhas são menores e mais frias. Planetas mais próximps de seus sóis do que a zona Verde 




seria, demasiado quentes para a vida, e os planetas além dela frios demais. O tamanho da zona 




hospitável é menor para estrelas mais frias. 







Newton acreditava que o nosso sistema solar estranhamente habitável não "surgiu do caos pelas 




simples leis da natureza." Ao invés disso, ele sustentava, a ordem no universo foi "criada por Deus em primeiro lugar e conservada por ele até hoje no mesmo estado e condição." É fácil 




entender por que se poderia pensar isso. As muitas ocorrências improváveis que conspiraram 




para permitir nossa existência, e o projeto propício à vida humana seriam realmente estranhos 




se o nosso fosee o único sistema solar no Universo. Mas, em 1992 veio a primeira observação 




confirmada de um planeta orbitando uma estrela diferente do nosso sol. Sabemos agora de 




centenas de tais planetas, e poucos duvidam que existem muitos outros entre os muitos bilhões 




de estrelas em nosso universo. Isso torna as coincidências de nossas condições planetárias - o 




Sol único, a combinação favorável de distância Terra-Sol e massa solar - muito menos notável e 




as provas muito menos convincentes de que a Terra foi cuidadosamente projetada apenas para 




agradar a nós, seres humanos. Existem planetas de todos os tipos. Alguns - ou pelo menos um - 




suporta vida. Obviamente, quando os seres de um planeta que sustenta a vida examinam o 




mundo à sua volta, eles são levados a achar que o seu ambiente satisfaz as condições de que necessitam para existir. 




É possível transformar essa última afirmação em um princípio científico: A nossa própria 




existência impõe regras que determinam de onde e em que momento é possível para nós 




observar o universo. Ou seja, o fato de estarmos restritos às características do tipo de ambiente 




em que nos encontramos. Este princípio é chamado de princípio antrópico fraco. (Veremos em 




breve porque o adjetivo "fraco" é adicionado.) Um termo melhor do que o "princípio antrópico" 




teria sido "princípio de seleção", porque a princípio se refere à forma como o nosso próprio conhecimento de nossa existência impõe regras que selecionam, entre todos os ambientes 




possíveis, apenas aqueles ambientes com as características que permitem a vida. 




Embora possa soar como uma filosofia, o princípio antrópico fraco pode ser utilizado para fazer 




previsões científicas. Por exemplo, quantos anos tem o universo? Como veremos em breve, 




para que nós existamos, o universo deve conter elementos tais como carbono, que são 




produzidos através do cozimento de elementos mais leves dentro das estrelas. O carbono 




precisa ser espalhado através do espaço na explosão de uma supernova, e, eventualmente, 




condensar-se como parte de um planeta em um sistema solar de nova geração. Em 1961, o 




físico Robert Dicke argumentou que o processo leva cerca de 10 bilhões de anos, assim o fato 




de estarmos aqui significa que o universo deve ter pelo menos esta idade. Por outro lado, o 




universo não pode ser muito antigo que 10 bilhões de anos, uma vez que no futuro distante, 




todo o combustível das estrelas terá sido utilizado, e exigimos estrelas quentes para o nosso 




sustento. Assim, o universo deve ter cerca de 10 bilhões de anos. Esta não é uma previsão 




muito precisa, mas é verdadeira - de acordo com os dados atuais o Big Bang ocorreu cerca de 




13,7 bilhões de anos atrás. 




Como era o caso da idade do universo, as previsões antrópica geralmente produzem um 




intervalo de valores para um determinado parâmetro físico, ao invés de localizá-lo 




precisamente. Isso ocorre porque a nossa existência, embora ela possa não exigir um 




determinado valor de algum parâmetro físico, muitas vezes ela depende de que tais parâmetros 




não variem muito em relação ao local onde nós realmente os encontramos. Nós, além disso, 




esperamos que as condições reais em nosso mundo estejam tipicamente dentro da faixa 




antropicamente permitida. Por exemplo, se apenas uma modesta excentricidade orbital, 




digamos, entre zero e 0,5, permitirá a vida, então uma excentricidade de 0,1 não deveria nos 




surpreender porque, entre todos os planetas no universo, uma percentagem razoável 




provavelmente têm órbitas com excentricidades tão pequenas. Mas, se ocorresse que a terra se 




movesse em um círculo quase perfeito, com excentricidade, digamos, de 0,00000000001, isso 




faria da Terra um planeta muito especial, e nos motivaria a tentar explicar porque estamos 




vivendo em um habitat tão anômalo. Essa idéia é às vezes chamada de princípio da 




mediocridade. 




As coincidências felizes relativas à forma das órbitas planetárias, a massa do sol, e assim por 




diante são chamadas ambientais porque elas surgem do acaso de nosso ambiente. e não de um 




erro nas leis fundamentais da natureza. A idade do universo é também um fator ambiental, uma 




vez que há um tempo mais cedo e mais tarde na história do universo, mas temos de viver nesta 




época, porque é a única época que favorece a vida. Coincidências ambientais são fáceis de 




entender, porque o nosso é apenas um habitat cósmico entre muitos que existem no universo, e 




nós, obviamente, precisamos existir em um habitat que sustente a vida. 




O princípio antrópico fraco não é muito controvertido. Mas, existe uma forma mais forte que nós vamos discutir aqui, embora ela seja vista com desdém entre alguns físicos. O princípio 




antrópico forte sugere que o fato de que nós existimos impõe restrições não apenas ao nosso 




ambiente, mas à forma e conteúdo possíveis das próprias leis da natureza. A idéia surgiu porque 




não são só as características peculiares do nosso sistema solar que parecem estranhamente 




propícias ao desenvolvimento da vida humana, mas também as características de nosso 




universo inteiro, e que é muito mais difícil de explicar. 




O conto de como o universo primordial de hélio, hidrogênio e um pouco de lítio evoluiu para 




um universo que abriga, pelo menos, um mundo com vida inteligente como nós, é um conto de 




muitos capítulos. Conforme mencionado anteriormente, as forças da natureza tinha que ser tais 




que elementos mais pesados - especialmente carbono - pudessem ser produzidos a partir dos 




elementos primordiais, e permanecerem estáveis por pelo menos bilhões de anos. Esses 




elementos pesados foram formados nas fornalhase que chamamos estrelas, de modo que as 




forças tiveram primeiro que permitir que as estrelas e galáxias se formassem. Aquelas 




cresceram a partir de sementes de pequenas faltas de homogeneidade no início do Universo, 




que era quase completamente uniforme, mas felizmente continha variações de densidade de 




cerca de 1 parte em 100.000. No entanto, a existência de estrelas e a existência no interior 




dessas estrelas de elementos de que somos feitos, não é suficiente. A dinâmica das estrelas teve 




que ser tal que algumas acabariam por explodir, e, além disso, explodir precisamente de uma 




forma que pudesse desembolsar os elementos mais pesados pelo espaço. Além disso, as leis da 




natureza tinham de ditar que esses restos poderiam se recondensar em uma nova geração de 




estrelas, rodeadas por planetas incorporando os elementos pesados recém-formados. Assim 




como certos acontecimentos na Terra primitiva tiveram de ocorrer para permitir que nos 




desenvolvessemos, também o era cada elo dessa cadeia necessária para a nossa existência. Mas, 




no caso dos eventos resultando na evolução do universo, tais desenvolvimentos eram regidos 




pelo equilíbrio das forças fundamentais da natureza, e são aqueles cuja interação tinha que ser 




correta para que nós existíssemos. 




Um dos primeiros a reconhecer que isso poderia implicar em uma boa dose de acaso foi Fred 




Hoyle, na década de 50. Hoyle acreditava que todos os elementos químicos tinham sido 




originalmente formados a partir do hidrogênio, que ele considerava a verdadeira substância 




primordial. O hidrogênio tem o núcleo atômico mais simples, consistindo em apenas um 




próton, isoladamente ou em combinação com um ou dois nêutrons. (Diferentes formas de 




hidrogênio, ou qualquer núcleo, com o mesmo número de prótons, mas diferente número de 




nêutrons são chamados isótopos.) Hoje sabemos que o hélio e lítio, átomos cujos núcleos 




contêm dois ou três prótons, também eram primordialmente sintetizados, em quantidades muito 




menores, quando o universo tinha cerca de 200 segundos de idade. A vida, por outro lado, 




depende de elementos mais complexos. O carbono é o mais importante deles, a base de toda a 




química orgânica. 




Embora se possa imaginar que organismos "vivos", tais como computadores inteligentes 




produzidos a partir de outros elementos, tais como o silício, é duvidoso que a vida pudesse ter 




evoluído espontaneamente na ausência de carbono. As razões para isso são técnicas, mas têm a 




ver com a maneira única em que o carbono se combina com outros elementos. O dióxido de 




carbono, por exemplo, é gasoso à temperatura ambiente, e biologicamente muito útil. Uma vez 




que o silício é o elemento diretamente abaixo do carbono na tabela periódica, ele tem 







propriedades químicas semelhantes. No entanto, o dióxido de silício, quartzo, é muito mais útil 




em uma coleção de pedras do que nos pulmões de um organismo. Ainda assim, talvez formas 




de vida pudessem evoluir que se alimentam de silício e ritmicamente torcem suas caudas em 




piscinas de amônia líquida. Mesmo esse tipo de vida exótica não poderia evoluir a partir apenas 




de elementos primordiais, porque esses elementos podem formar somente dois compostos 




estáveis - hidreto de lítio, que é um sólido cristalino incolor, e gás de hidrogênio, nenhum deles 




um composto susceptível de se reproduzir ou mesmo de se apaixonar. Além disso, permanece o 




fato de que somos uma forma de vida de carbono, e que levanta a questão de como fram criados 




o carbono, cujo núcleo contém seis prótons e os outros elementos pesados em nossos corpos. 




A primeira etapa ocorre quando estrelas mais antigas começam a acumular hélio, que é 




produzido quando dois núcleos de hidrogênio colidem e se fundem. Esta fusão é como as 




estrelas criam a energia que nos aquece. Dois átomos de hélio podem, por sua vez, colidir para 




formar o berílio, um átomo cujo núcleo contém quatro prótons. Uma vez que o berílio tenha-se 




formado, ele poderia, em princípio fundir-se com um terceiro núcleo de hélio para formar 




carbono. Mas, isso não acontece, porque o isótopo de berílio que é formado decai quase 




imediatamente de volta a núcleos de hélio. 




A situação muda quando uma estrela começa a ficar sem hidrogênio. Quando isso acontece, o 




núcleo da estrela entra em colapso até que sua temperatura central sobe para cerca de 100 




milhões de graus Kelvin. Nestas condições, os núcleos encontram-se com tanta freqüência que 




alguns núcleos de berílio colidem com um núcleo de hélio antes de terem tido a chance de 




decair. O berílio pode então fundir-se com o hélio para formar um isótopo de carbono que é 




estável. Mas este carbono ainda está muito longe de formar agregados ordenados de compostos 




químicos do tipo que pode desfrutar de uma taça de Bordeaux, fazer malabarismo com pinos de 




boliche em chamas, ou fazer perguntas sobre o universo. Para que tais seres humanos existam, 




o carbono precisa mover-se do interior da estrela para vizinhanças mais amigáveis. Isso, como 




já dissemos, é conseguido quando a estrela, no final do seu ciclo de vida, explode como uma 




supernova, expulsando carbono e outros elementos pesados que mais tarde se condensam em 




um planeta. 







Processo Alfta Triplo

 - O Carbono é criado no interior das estrelas a partir de colisões de três núcleos de hélio, um evento que seria muito improvável, se não fosse uma propriedade 




especial das leis da física nuclear. 




 




Este processo de criação de carbono é chamado de processo alfa triplo porque "partícula alfa" é outro nome para o núcleo do isótopo do hélio envolvido, e porque o processo exige que três 




deles (eventualmente) se fundam. A física usual prevê que a taxa de produção de carbono 




através do processo alfa triplo deve ser muito pequena. Observando isso, em 1952, Hoyle 




previu que a soma das energias de um núcleo de berílio e um núcleo de hélio deve ser quase 




exatamente a energia de um certo estado quântico dos isótopos de carbono formados, uma 




situação chamada de ressonância, que aumenta enormemente a taxa de de uma reação nuclear. 




Na época, nenhum nível de energia desse tipo era conhecido, mas com base na sugestão de 




Hoyle, William Fowler da Caltech o procurou e o encontrou, fornecendo um apoio importante 




para a visão de Hoyle sobre como núcleos complexos foram criados. 




Hoyle escreveu: "Eu não acredito que qualquer cientista que tenha analisado as provas deixaria 




de tirar a conclusão de que as leis da física nuclear foram deliberadamente projetadas em 




relação às conseqüências que elas produzem no interior das estrelas." Naquela época, ninguém 




sabia física nuclear suficiente para compreender a magnitude do acaso que resultou nestas leis 




físicas exatas. Mas, ao investigar a validade do princípio antrópico forte, em anos recentes, os 




físicos começaram a se perguntar como teria sido o universo se as leis da natureza fossem 




diferentes. Hoje, podemos criar modelos em computador que nos informam como a taxa da 




reação alfa triplo depende da força das forças fundamentais da natureza. Tais cálculos mostram 




que uma mudança tão pequena quanto 0,5 por cento na intensidade da força nuclear forte, ou 4 




por cento da força elétrica, destruiria ou quase todo o carbono ou todo o oxigênio em cada 




estrela e, portanto, a possibilidade de vida como nós a conhecemos. Altere estas regras do nosso 




universo só um pouquinho, e as condições de nossa existência desaparecem! 




Examinando os modelos de universo que geramos quando as teorias da física são alteradas de 




certas formas, pode-se estudar o efeito de mudanças na lei física de uma maneira metódica. 




Acontece que não é só os pontos fortes da força nuclear forte e da força eletromagnética que 




são feitos sob encomenda para nossa existência. A maioria das constantes fundamentais em 




nossas teorias parece afinada no sentido de que se elas foram alterados por montantes modestos, 




o universo seria qualitativamente diferente, e em muitos casos, inadequados para o 




desenvolvimento da vida. Por exemplo, se a outra força nuclear, a força fraca, fosse muito mais 




fraca, no início do universo todo o hidrogênio no cosmos teria se transformado em hélio, e, 




portanto, não haveria estrelas normais; se fosse muito mais forte, supernovas explodindo não 




ejetariam seus envelopes externos e, daí falhariam em semear o espaço com os elementos 




pesados que os planetas exigem para promover a vida. Se os prótons fossem 0,2 por cento mais 




pesados, eles se decomporiam em nêutrons, desestabilizando os átomos. Se a soma das massas 




dos tipos de quarks que compõem um próton fossem alterados em tão pouco quanto 10 por 




cento, haveria muito menos núcleos atômicos estáveis dos quais somos feitos; na realidade, a 




soma das massas dos quarks parece mais ou menos otimizada para a existência do maior 




número de núcleos estáveis. 




Se se supõe que algumas centenas de milhões de anos em órbita estável são necessárias para a 




vida planetária evoluir, o número de dimensões do espaço também é corrigido por nossa 




existência. Isso ocorre porque, de acordo com as leis da gravidade, é somente em três 




dimensões que as órbitas elípticas estáveis são possíveis. As órbitas circulares são possíveis em 




outras dimensões, mas aquelas, como Newton temia, são instáveis. Em qualquer uma, exceto 




em três dimensões, mesmo uma pequena perturbação, como aquela produzida pela força dos outros planetas, enviaria um planeta para fora de sua órbita circular e faria com que ele fosse 




em espiral ou para longe do sol, e então nós queimaríamos ou congelaríamos. Além disso, em 




mais de três dimensões a força gravitacional entre dois corpos diminuiria mais rapidamente do 




que acontece em três dimensões. Em três dimensões a força gravitacional cai para ¼ do seu 




valor se a distância for dobrada. Em quatro dimensões ela cairia para ⅛, em cinco dimensões 




que cairia para, e assim por diante. Como resultado, em mais de três dimensões, o sol não seria 




capaz de existir num estado estável, com a sua pressão interna equilibrando a força da 




gravidade. Ele ou desmoronaria ou entraria em colapso para formar um buraco negro, e 




qualquer uma das duas coisas arruinaria o seu dia. Na escala atômica, as forças elétricas se 




comportariam da mesma maneira que as forças gravitacionais. Isso significa que os elétrons em 




átomos ou escapariam oou cairiam em espiral para dentro do núcleo. Em nenhum dos casos os 




átomos como os conhecemos seriam possíveis. 




O surgimento de estruturas complexas capazes de suportar observadores inteligentes parece ser 




muito frágil. As leis da natureza formam um sistema que é extremamente afinado, e muito 




pouco nas leis da física pode ser alterado sem destruir a possibilidade do desenvolvimento da 




vida como a conhecemos. Se não fosse por uma série de coincidências surpreendentes nos 




detalhes precisos da lei física, ao que parece, os seres humanos e outras formas de vida 




semelhantes jamais teria surgido. 




A mais impressionante coincidência de sintonia fina envolve a chamada constante cosmológica 




nas equações de Einstein da relatividade geral. Como já dissemos, em 1915, quando formulou a 




teoria, Einstein acreditava que o Universo era estático, isto é, nem estava se expandindo nem 




contraindo. Uma vez que toda a matéria atrai outra matéria, ele introduziu em sua teoria uma 




nova força antigravitacional para combater a tendência do universo a se contrair sobre si 




mesmo. Essa força, ao contrário de outras forças, não veio de uma fonte específica, mas estava 




incorporada no próprio tecido do espaço-tempo. A constante cosmológica descreve a 




intensidade dessa força. 




Quando foi descoberto que o universo não era estático, Einstein eliminou a constante 




cosmológica de sua teoria e chamou a inclusão dela o maior erro de sua vida. Mas, em 1998, 




observações de supernovas distantes mostraram que o universo está se expandindo a uma taxa 




em aceleração, um efeito que não é possível sem algum tipo de força de repulsão que atue em 




todo o espaço. A constante cosmológica foi ressuscitada. Uma vez que nós sabemos agora que 




seu valor não é zero, a questão permanece, porque ela tem o valor que tem? Os físicos criaram 




argumentos explicando como ela poderia surgir devido a efeitos da mecânica quântica, mas o 




valor que eles calculam é cerca de 120 ordens de magnitude (1 seguido de 120 zeros) mais forte 




do que o valor real, obtido através da observação de supernovas. Isso significa que ou o 




raciocínio empregado para o cálculo estava errado, ou então existe algum outro efeito que 




milagrosamente cancela todos, exceto uma fração inimaginavelmente pequena do número 




calculado. A única coisa que é certa é que, se o valor da constante cosmológica fosse muito 




maior do que é, o nosso universo teria explodido antes de que galáxias pudessem se formar e, - 




uma vez mais - a vida como a conhecemos seria impossível. 




O que podemos fazer com essas coincidências? Sorte na forma e natureza precisa das leis 




físicas fundamentais é um tipo de sorte diferente da sorte que encontramos em fatores 




ambientais. Não pode ser tão facilmente explicada, e tem implicações muito mais físicas e 




filosóficas. Nosso universo e suas leis parecem ter um design que é ao mesmo tempo feito sob medida para nos apoiar e, se quisermos existir, deixa pouco espaço para alterações. Isso não é 




facilmente explicado, e levanta a questão natural de porque é dessa forma. 




Muitas pessoas gostariam que usássemos essas coincidências como prova do trabalho de Deus. 




A idéia de que o universo foi projetado para acomodar a humanidade aparece em teologias e 




mitologias que datam de milhares de anos até ao presente. Nas narrativas mito-históricas do 




povo Maya Vuh, os deuses proclamam: "Não receberemos glória nem honra de tudo o que 




criamos e formamos até que os seres humanos existem, dotados de sensibilidade." Um texto 




típico egípcio datado de 2000, aC declara, "Homens, o gado de Deus, foram bem tratados. Ele 




[o deus sol] fez o céu e a terra em seu benefício". Na China, o filósofo taoísta Lieh-Yü K'ou (c. 




400 aC) expressou a idéia através de uma personagem em um conto que diz: "O céu faz com 




que os cinco tipos de grãos cresçam, e traz as tribos de penas e barbatanas, especialmente em 




nosso benefício." 




Na cultura ocidental, o Antigo Testamento contém a idéia de um desígnio providencial na 




história da criação, mas a visão cristã tradicional também foi bastante influenciada por 




Aristóteles, que acreditava que "em um mundo inteligente natural que funciona de acordo com 




algum projeto deliberado." O teólogo cristão medieval, Tomás de Aquino empregava as idéias 




de Aristóteles sobre a ordem na natureza para defender a existência de Deus. No século XVIII, 




outro teólogo cristão foi tão longe quanto dizer que as lebres têm rabos brancos para que seja 




fácil para nós a atirar nelas. Uma ilustração mais moderna do ponto de vista cristão foi dado há 




alguns anos, quando o Cardeal Christoph Schönborn, arcebispo de Viena, escreveu: "Agora, no 




início do século 21, diante de afirmações científicas como o neo-darwinismo e a hipótese do 




multiverso [muitos universos] em cosmologia, inventados para evitar a esmagadora evidência 




de propósito e desígnio encontrados na ciência moderna, a Igreja Católica novamente defenderá 




a natureza humana, proclamando que o desígnio imanente na natureza é real." Em cosmologia, 




a esmagadora evidência de propósito e desígnio à qual o cardeal estava se referindo é o 




aperfeiçoamento das leis físicas que descrevemos acima. 




O ponto decisivo na rejeição científico de um universo centrado no homem era o modelo de 




Copérnico do sistema solar, em que a Terra já não ocupava uma posição central. Ironicamente, 




a visão de mundo do próprio Copérnico era antropomórfica, até o ponto de que ele nos conforta 




salientando que, apesar de seu modelo heliocêntrico, a Terra está quase o centro do universo: 




"Embora [a terra] não esteja no centro do mundo, no entanto, a distância [até aquele centro] é 




como nada em especial quando comparada à das estrelas fixas." Com a invenção do telescópio, 




observações no século XVII, tais como o fato de que o nosso não é o único planeta orbitado por 




uma lua, deu peso ao princípio de que não temos posição privilegiada no universo. Nos séculos 




seguintes, quanto mais descobríamos sobre o universo, mais parecia que o nosso planeta era 




provavelmente apenas uma variedade do jardim de planetas. Mas a descoberta relativamente 




recente de extrema afinação de muitas de tantas leis da natureza poderia levar, pelo menos 




alguns de nós, de volta à velha ideia de que este grande projeto é o trabalho de algum grande 




projetista. Nos Estados Unidos, porque a Constituição proíbe o ensino da religião nas escolas, 




esse tipo de idéia é chamado design inteligente, com o entendimento não declarado, mas 




implícito de que o projetista é Deus. 




Esta não é a resposta da ciência moderna. Vimos no Capítulo 5 que o nosso universo parece ser 




um entre muitos, cada um com diferentes leis. Essa idéia do multiverso não é um conceito 




inventado para explicar o milagre da sintonia fina. Ele é uma consequência da condição sem-limite, assim como o são muitas outras teorias da cosmologia moderna. Mas, se for verdade, 




então o princípio antrópico forte pode ser considerado efetivamente equivalente ao fraco, 




colocando a afinação da lei física em pé de igualdade com os fatores ambientais, pois isso 




significa que o nosso habitat cósmico - agora todo o universo observável - é apenas um entre 




muitos, assim como nosso sistema solar é um entre muitos. Isso significa que, da mesma forma 




que as coincidências ambientais do nosso sistema solar foram tornadas normais pela percepção 




de que bilhões de tais sistemas existem, a afinações nas leis da natureza pode ser explicada pela 




existência de múltiplos universos. Muitas pessoas ao longo dos tempos têm atribuído a Deus, a 




beleza e a complexidade da natureza, que por sua vez parecia não ter explicação científica. Mas, 




assim como Darwin e Wallace explicaram como o projeto aparentemente miraculoso de formas 




de vida podia aparecer sem intervenção de um ser supremo, o conceito de multiverso pode 




explicar o ajuste fino das leis físicas sem a necessidade de um criador benevolente que fez o 




universo em nosso benefício . 




Einstein fez, certa vez, a seu assistente Ernst Straus a pergunta "Será que Deus tinha qualquer opção, quando ele criou o universo?" No final do século XVI, Kepler estava convencido de que 




Deus tinha criado o universo de acordo com um princípio matemático perfeito. Newton 




demonstrou que as mesmas leis que se aplicam aos céus aplicam-se na terra, e desenvolveu 




equações matemáticas para expressar aquelas leis que eram tão elegantes que inspiraram fervor 




quase religioso entre muitos cientistas do século XVIII, que pareciam ter a intenção de usá-las 




para demonstrar que Deus era um matemático. 




Desde Newton, e especialmente desde Einstein, o objetivo da física tem sido encontrar 




princípios matemáticos simples do tipo que Kepler imaginou, e com eles criar uma teoria 




unificada de tudo o que levaria em conta para todos os detalhes da matéria e as forças que 




observamos na natureza. No final do século XIX e início do século XX, Maxwell e Einstein 




Unidos unificaram as teorias da eletricidade, magnetismo e luz. Na década de 70, o modelo 




padrão foi criado, uma única teoria das forças nucleares forte e fraca, e a força eletromagnética. 




A teoria das cordas e a teoria-M, então, surgiram em uma tentativa de incluir a força restante, a 




gravidade. O objetivo era encontrar não apenas uma teoria única que explicasse todas as forças, 




mas também aquela que explicaria os números fundamentais sobre os quais temos falado, tais 




como o poder das forças e as massas e cargas das partículas elementares. Conforme Einstein 




colocou, a esperança era ser capaz de dizer que "a natureza é de tal modo constituída que é 




possível logicamente estabelecer tais leis fortemente determinadas que, dentro dessas leis 




somente constantes racional e completamente determinadas ocorreriam (não constantes, 




portanto, cujo valor numérico pudesse ser alterado sem destruir a teoria)." Seria improvável que uma teoria única tenha a sintonia fina que nos permite existir. Mas, se à luz dos recentes 




avanços interpretamos o sonho de Einstein como sendou o de uma teoria única que explica este 




e outros universos, com seus espectros inteiros de leis diferentes, então a teoria-M pode ser 




aquela teoria. Mas a teoria-M é única, ou exigida por um princípio lógico simples? Podemos 




responder à pergunta,  

por que a teoria-M

? 




 




8

O PROJETO MONUMENTAL 




 




NESTE LIVRO, DESCREVEMOS como regularidades no movimento dos corpos celestes 




como o sol, a lua, os planetas sugeriram que eles eram regidos por leis fixas, em vez de estar 




sujeita às excentricidades arbitrárias e caprichos de deuses e demônios. Primeiramente, a 




existência de tais leis só se tornou aparente em astronomia (ou astrologia, que era considerado a 




mesma coisa). O comportamento das coisas na terra é tão complicado e sujeito a tantas 




influências, de modo que as primeiras civilizações eram incapazes de discernir qualquer padrão 




claro ou as leis que regem estes fenômenos. Aos poucos, porém, novas leis foram descobertas 




em outras áreas além da astronomia, e isto levou à idéia do determinismo científico: Deve haver 




um conjunto completo de leis que, dado o estado do universo em um momento específico, 




poderia especificar como ele se desenvolverá a partir daquele momento. Essas leis devem se 




manter em todos os lugares e em todos os momentos; caso contrário elas não seriam leis. Não 




poderia haver exceções ou milagres. Deuses ou demônios não poderiam intervir no 




funcionamento do universo. 




No momento em que o determinismo científico foi proposto pela primeira vez, as leis de 




Newton de movimento e gravidade eram as únicas leis conhecidas. Nós descrevemos como 




essas leis foram ampliadas por Einstein na sua teoria da relatividade geral, e como outras leis 




foram descobertos para reger outros aspectos do universo. 




As leis da natureza nos dizem como o universo funciona, mas eles não respondem o por quê? 




Perguntas que foram feitas no início deste livro: 




 

Por que existe algo ao invés de nada? 




 

Por que existimos? 




 

Por este conjunto específico de leis e não algum outro? 




Alguns diriam que a resposta a estas perguntas é que existe um Deus que escolheu criar o 




universo dessa maneira. É razoável perguntar quem ou o que criou o universo, mas se a resposta 




é Deus, então a questão foi apenas desviada para quem criou Deus. Neste ponto de vista é 




aceito que existe alguma entidade que não precisa de criador, e esta entidade é chamado de 




Deus. Isso é conhecido como o argumento da primeira causa para a existência de Deus. 




Afirmamos, no entanto, que é possível responder a estas questões puramente dentro do reino da 




ciência, e sem invocar qualquer ser divino. 




Conforme a idéia de realismo dependente de modelo introduzida no Capítulo 3, nosso cérebro 




interpreta a entrada de dados de nossos órgãos sensoriais criando um modelo do mundo 




externo. Formamos conceitos mentais da nossa casa, árvores, outras pessoas, a eletricidade que 




flui de tomadas de parede, átomos, moléculas e outros universos. Estes conceitos mentais são a 




única realidade que podemos conhecer. Não há nenhum teste da realidade independente de 




modelo. Dai resulta que um modelo bem construído cria uma realidade própria. Um exemplo 




que pode nos ajudar a pensar sobre as questões da realidade e da criação é o Jogo da Vida, 




inventado em 1970 por um jovem matemático de Cambridge chamado John Conway. 




A palavra "jogo" no Jogo da Vida é um termo enganador. Não há vencedores e perdedores, na 




verdade, não há jogadores. O Jogo da Vida não é realmente um jogo, mas um conjunto de leis 







que governam o universo bidimensional. É um universo determinista: Depois de definir uma 




configuração inicial, ou condição inicial, as leis determinam o que acontece no futuro. 




O mundo imaginado por Conway é uma matriz quadrada, como um tabuleiro de xadrez, mas 




estendendo-se infinitamente em todas as direções. Cada quadrado pode estar em um dos dois 




estados: vivo (mostrado em verde) ou morto (mostrado em preto). Cada quadrado tem oito 




vizinhos: os vizinhos acima, abaixo, à esquerda e à direita e quatro vizinhos na diagonal. O 




Tempo neste mundo não é contínuo, mas avança em passos discretos. Dado qualquer arranjo 




das quadrados mortos e vivos, o número de vizinhos vivos determina o que acontece a seguir, 




de acordo com as seguintes leis:  




1.  Um quadrado vivo com dois ou três vizinhos vivos sobrevive (sobrevivência). 




2.  Um quadrado morto com exatamente três vizinhos vivos torna-se uma célula viva 




(nascimento). 




3.  Em todos os outros casos, a célula morre ou continua morta. No caso de um quadrado 




vivo ter zero ou um vizinho, diz-se que deve morrer de solidão; se ele tem mais de três 




vizinhos, diz-se que vai morrer de superlotação. 




Isso é tudo que existe para isso: Dada qualquer condição inicial, essas leis geram geração após 




geração. Um quadrado vivendo isolado ou dois quadrados vivos adjacentes morrem na próxima 




geração, porque eles não têm vizinhos suficientes. Três quadrados vivos ao longo da diagonal 




vivem um pouco mais. Após o primeira momento o quadrado final morre, deixando apenas o 




quadrado do meio, que morre na geração seguinte. Qualquer linha diagonal de quadrados 




"evapora" exatamente dessa maneira. Mas, se três quadrados vivos são colocados 




horizontalmente em uma linha, mais uma vez o centro tem dois vizinhos e sobrevive, enquanto 




os dois quadrados da ponta morrem, mas neste caso as células imediatamente acima e abaixo da 




célula central experiemental um nascimento. A linha, portanto, transforma-se em uma coluna. 




De maneira semelhante, na próxima geração a coluna volta se transformar em uma fileira, e 




assim por diante. Tais configurações oscilantes são chamadas pisca-piscas. 







Pisca-piscas 

- Pisca-piscas são um tipo simples de objeto composto no Jogo da Vida  







Se três quadrados vivos são colocados na forma de um L, ocorre um novo comportamento. Na 




geração seguinte o quadrado embalado pelo L dará à luz, levando a um bloco de 2 × 2. O bloco 




pertence a um tipo de padrão chamado de natureza morta porque ele vai passar de geração em 




geração inalterado. Muitos tipos de padrões existentes que se transformam nas primeiras 




gerações, mas logo se transformam em uma natureza morta, ou morrem, ou voltam à sua forma 




original e então repetem o processo. 













Evolução para uma natureza morta 

- Alguns objetos compostos no Jogo da Vida evoluem 




para uma forma em que as regras ditam que nunca vai mudar. 










Há também padrões chamados planadores, que se transformam em outras formas e, após 




algumas gerações, voltam à sua forma original, mas em uma posição em um quadrado abaixo 




ao longo da diagonal. Se você observa a estes em desenvolvimento ao longo do tempo, eles 




parecem rastejar ao longo da matriz. Quando este planadores colidem, comportamentos 




curiosos podem ocorrer, dependendo da forma de cada planador no momento da colisão. 







Planadores 

- Planadores se transformam através dessas formas intermediárias, em seguida, 




retornam à sua forma original, deslocados por um quadrado na diagonal. 







O que torna esse universo interessante é que, embora a "física" fundamental do universo seja simples, a "química" pode ser complicada. Ou seja, existem objetos compostos em diferentes escalas. Na menor escala, a física fundamental nos diz que há apenas quadrados vivos e 




quadrados mortos. Em uma escala maior, há planadores, piscas, e blocos de naturezas-mortas. 




Em uma escala ainda maior, existem objetos ainda mais complexos, tais como canhões de 




planadores: padrões estacionários que, periodicamente, dão à luz novos planadores que deixam 




o ninho e correm para baixo na diagonal. 




Se observar o universo do jogo da vida por algum tempo, em qualquer escala particular, você 




poderá deduzir as leis que regem os objetos naquela escala. Por exemplo, na escala de objetos 




apenas alguns quadrados adiante você pode ter leis tais como "blocos nunca se movem", 




"Planadores se movem na diagonal", e várias leis para o que acontece quando os objetos se 




chocam. Você pode criar uma física inteira, em qualquer nível de objetos compostos. As leis 




implicariam em entidades e conceitos que não têm lugar entre as leis originais. Por exemplo, 







não existem conceitos tais como "chocar-se" ou "mover-se" na legislação original. Estes descrevem apenas a vida e a morte de cada um dos quadrados estacionários. Como acontecem 




em nosso universo, no Jogo da Vida a sua realidade depende do modelo que você emprega. 







Configuração inicial do Canhão de Planadores 

- Um canhão de planadores tem 




aproximadamente dez vezes o tamanho de um planador  







Conway e seus estudantes criaram este mundo, porque eles queriam saber se um universo com 




regras fundamentais tão simples como estas que eles definiram poderia conter objetos 




complexos o suficiente para se replicar. No mundo do Jogo da Vida, existem objetos compostos 




que, depois de apenas seguir as leis daquele mundo por algumas gerações, darão origem a 




outros de sua espécie? Não só Conway e seus alunos foram capazes de demonstrar que isso é 




possível, mas eles até mesmo demonstraram que tal objeto seria, em certo sentido, inteligente! 




O que queremos dizer com isso? Para sermos precisos, eles mostraram que os grandes 




conglomerados de quadrados que se autoreplicam são "máquinas de Turing universais". Para 




nossos propósitos, isso significa que para qualquer cálculo, que um computador em nosso 




mundo físico pode, em princípio, realizar, se a máquina recebesse a entrada de dados 




apropriada - ou seja, fornecido o ambiente de mundo adequado ao Jogo da Vida - então 




algumas gerações depois, a máquina estaria em um estado tal que um resultado poderia ser lido 




que corresponderia ao resultado daquele cálculo do computador. 










O canhão de planadores após 116 gerações 

- Com o tempo, o canhão de planadores muda de 




forma, emite um planador, e depois retorna à sua forma e posição originais. Ele repete o 




processo ad infinitum. 







Para ter uma ideia de como isso funciona, considere o que acontece quando os planadores são 




disparados contra um bloco de 2 × 2 quadrados vivos. Se os planadores se aproximam da 




maneira correta, o bloco, que tinha sido estacionário, move-se em direção ou para longe da 




fonte dos planadores. Desta forma, o bloco pode simular uma memória de computador. Na 




verdade, todas as funções básicas de um computador moderno, tais como portas AND e OR, 




também podem ser criadas a partir de planadores. Desta forma, assim como os sinais elétricos 




são empregados em um computador físico, fluxos de planadores podem ser utilizados para 




enviar e processar informação. 




No Jogo da Vida, como em nosso mundo, os padrões de auto-reprodução são objetos 




complexos. Uma estimativa, com base no trabalho inicial do matemático John von Neumann, 




coloca o tamanho mínimo de um padrão de auto-replicação no Jogo da Vida Humana a dez 




trilhões de quadrados - aproximadamente o número de moléculas em uma única célula humana. 




Pode-se definir os seres vivos como sistemas complexos de tamanho limitado que são estáveis e 




que se reproduzem. Os objetos descritos acima satisfazem as condições de reprodução, mas 




provavelmente não são estáveis: Uma pequena perturbação de fora provavelmente destruiria o 




delicado mecanismo. No entanto, é fácil imaginar que leis um pouco mais complicadas 




permitiriam sistemas complexos com todos os atributos da vida. Imagine uma entidade desse 




tipo, um objeto em um mundo do tipo Conway. Tal objeto responderia a estímulos ambientais 




e, portanto, pareceria tomar decisões. Será que tal vida seria consciente de si mesma? Seria ela 




auto-consciente? Esta é uma questão sobre a qual a opinião está profundamente dividida. 




Algumas pessoas afirmam que o auto-conhecimento é algo único dos seres humanos. Ele lhes 




dá o livre arbítrio, a capacidade de escolher entre diferentes cursos de ação. 




Como alguém pode dizer se um ser tem livre arbítrio? Se alguém encontra um alienígena, como se pode dizer se é apenas um robô ou se ele tem uma mente própria? O comportamento de um 




robô seria totalmente determinado, ao contrário de um ser com livre arbítrio. Assim, poder-se-




ia, em princípio, detectar um robô como um ser cujas ações podem ser previstas. Como 




dissemos no capítulo 2, esta pode ser incrivelmente difícil se o ser for grande e complexo. Não 




podemos sequer resolver exatamente as equações para três ou mais partículas que interagem 




entre si. Uma vez que um alienígena do tamanho de um ser humano poderia conter cerca de mil 




trilhões de trilhões de partículas, mesmo que o alienígena fosse um robô, seria impossível 




resolver as equações e prever o que iria fazer. Teríamos, portanto, que dizer que qualquer ser 




complexo tem livre arbítrio - não como um elemento fundamental, mas como uma teoria 




efetiva, uma admissão de nossa incapacidade de fazer os cálculos que nos permitiriam prever 




suas ações. 




O exemplo do Jogo da Vida de Conway mostra que mesmo um conjunto de leis muito simples 




pode produzir características complexas, semelhantes às da vida inteligente. Deve haver muitos 




conjuntos de leis com esta propriedade. O que escolhe as leis fundamentais (por oposição às 




leis aparentes) que regem nosso universo? Como acontece no universo de Conway, as leis de 




nosso universo determinam a evolução do sistema, dado o estado a qualquer momento. No 




mundo de Conway, somos os criadores - escolhemos o estado inicial do universo especificando 




objetos e suas posições no início do jogo. 




Em um universo físico, as contrapartidas de objetos tais como planadores no Jogo da Vida são 




órgãos isolados de matéria. Qualquer conjunto de leis que descreva um mundo contínuo como o 




nosso terá um conceito de energia, que é uma quantidade conservada, significando que não 




muda com o correr do tempo. A energia do espaço vazio será uma constante, independente 




tanto do tempo quando da posição. Pode-se subtrair essa energia constante do vácuo, medindo a 




energia de qualquer volume de espaço em relação ao do mesmo volume de espaço vazio, de 




modo que também podemos chamar a constante de zero. Uma exigência que qualquer lei da 




natureza deve satisfazer é que ela dita que a energia de um corpo isolado cercado por espaço 




vazio é positiva, o que significa que se tem de trabalhar para montar o corpo. Isso ocorre porque 




se a energia de um corpo isolado fosse negativa, ela poderia ser criada em um estado de 




movimento, de modo que a sua energia negativa fosse exatamente equilibrada pela energia 




positiva devido ao seu movimento. Se isso fosse verdade, não haveria razão para que os corpos 




não pudessem aparecer em qualquer lugar e em todo lugar. O espaço vazio seria, portanto, 




instável. Mas, se a criação de um corpo isolado exige energia, tal instabilidade não pode 




acontecer, porque, como já dissemos, a energia do universo deve permanecer constante. Isso é 




que é preciso para tornar o universo localmente estável - faze-lo de modo que as coisas não 




apareçam simplesmente em todos os lugares a partir do nada. 




Se a energia total do universo deve ser sempre zero, e a criação de um corpo exige energia, 




como pode todo um universo ser criado do nada? É por isso que deve haver uma lei como a 




gravidade. Como a gravidade é atrativa, a energia gravitacional é negativa: Precisa-se trabalhar 




para separar um sistema mantido unido gravitacionalmente, tal como a terra e a lua. Esta 




energia negativa podem equilibrar a energia positiva necessária para criar a matéria, mas não é 




assim tão simples. A energia gravitacional negativa da Terra, por exemplo, é menos de um 




bilionésimo da energia positiva das partículas de matéria das quais é feita a Terra. Um corpo 




como uma estrela terá mais energia gravitacional negativa, e quanto menor for (quanto mais 




perto as diferentes partes dele estão entre si), maior será é a energia gravitacional negativa. 




Mas, antes que se possa tornar maior que a energia positiva da matéria, a estrela entrará em 




colapso como um buraco negro, e os buracos negros possuem energia positiva. É por isso que 




um espaço vazio é estável. Órgãos tais como estrelas ou buracos negros não podem 




simplesmente aparecer do nada. Mas um universo inteiro pode. 




Como a gravidade dá forma ao espaço e ao tempo, ela permite que o espaço-tempo seja 




localmente estável, mas globalmente instável. Na escala do universo inteiro, a energia positiva 




da matéria pode ser equilibrada pela energia gravitacional negativa e, portanto, não há restrição 




à criação de universos inteiros. Porque existe uma lei como a gravidade, o universo pode e se 




criará partir do nada, da forma descrita no Capítulo 6. A criação espontânea é a razão pela qual 




existe algo ao invés de nada, por que o universo existe e por que nós existimos. Não é 




necessário invocar Deus para iluminar o papel de toque azul, e colocar o universo em 




movimento. 




Por que as leis fundamentais são como as descrevemos? A teoria definitiva deve ser consistente 




e deve prever resultados finitos para as quantidades que podemos medir. Vimos que precisa 




haver uma lei como a gravidade, e vimos no Capítulo 5 que para uma teoria da gravidade prever 




quantidades finitas, a teoria deve ter o que é chamado supersimetria entre as forças da natureza 




e da matéria sobre a qual elas atuam. A Teoria-M é a teoria supersymmetric da gravidade mais 




geral. Por estas razões, a teoria-M é o único candidato a teoria completa do universo. Se ele é 




finito - e isso ainda tem que ser provado - ele será um modelo de um universo que cria a si 




mesmo. Devemos fazer parte deste universo, porque não há outro modelo consistente. 




A Teoria-M é a teoria unificada que Einstein esperava encontrar. O fato de que nós, seres 




humanos - que somos meras coleções de partículas fundamentais da natureza - fomos capazes 




de chegar tão perto de uma compreensão das leis que regem a nós e ao nosso universo é um 




grande triunfo. Mas, talvez o verdadeiro milagre seja que as considerações abstratas da lógica 




levam a uma teoria original que prevê e descreve um vasto universo cheio da variedade incrível 




que vemos. Se a teoria for confirmada pela observação, será a conclusão bem-sucedida de uma 




pesquisa que remonta a mais de 3.000 anos. Termos encontrado o projeto monumental. 




 




GLOSSÁRIO 




Abordagem de baixo para cima

 • em cosmologia, uma ideia que se baseis no pressuposto de 




que há uma única história do universo, com um ponto de partida bem definido, e que o estado 




do universo hoje é uma evolução daquele início. 




Abordagem de cima para baixo

 • a abordagem da cosmologia em que se traça a história do 




universo de "cima para baixo", ou seja, para trás a partir do momento presente. 




Amplitude de probabilidade 

• em uma teoria quântica, um número complexo, cujo valor 




absoluto ao quadrado fornece uma probabilidade. 




Antimatéria

 • cada partícula de matéria tem uma antipartícula correspondente. Se eles se 




encontrarem, elas se aniquilam mutuamente, deixando pura energia. 




Átomo

 • a unidade básica da matéria comum, constituído por um núcleo com prótons e 




nêutrons, rodeado por elétrons em órbita. 




Bárion

 • um tipo de partícula elementar, como o próton ou o nêutron, que é composta de três 




quarks. 




Big Bang

 • o início denso e quente do universo. A teoria do Big Bang postula que cerca de 13,7 




bilhões de anos atrás a parte do Universo que podemos ver hoje tinha apenas alguns milímetros 




de diâmetro. Hoje, o universo é imensamente maior e mais frio, mas podemos observar os 




vestígios daquele período inicial na radiação cósmica de fundo em microondas que permeia 




todo o espaço. 




Bóson

 • uma partícula elementar que carrega força. 




Buraco Negro

 • uma região do espaço-tempo que, devido à sua imensa força gravitacional, é 




separado do resto do universo. 




Condição sem-limite

 • a exigência de que as histórias do universo sejam superfícies fechadas 




sem um limite. 




Constante cosmológica • um parâmetro nas equações de Einstein, que dá ao espaço-tempo uma 




tendência inerente de expansão. 




Elétron

 • uma partícula elementar da matéria que tem uma carga negativa e é responsável pelas propriedades químicas dos elementos. 




Espaço-tempo

 • um espaço matemático cujos pontos devem ser especificados tanto por 




coordenadas de tempo quando espaciais 




Fase

 • uma posição no ciclo de uma onda. 




Férmion

 • uma partícula de matéria do tipo elementar. 




Física Clássica

 • qualquer teoria da física em que se presume que o universo tem uma história única, bem definida. 




Força eletromagnética

 • a segunda mais forte entre as quatro forças da natureza. Ela atua entre partículas com cargas elétricas. 




Força nuclear forte

 • a mais forte entre as quatro forças da natureza. Esta força mantém os prótons e nêutrons dentro do núcleo de um átomo. Ela também mantém unidos os prótons e 




nêutrons entre si, o que é necessário porque eles são feitos de partículas ainda menores, os 




quarks. 




Força nuclear fraca

 • uma das quatro forças da natureza. A força franca é rponsável pela 




radioatividade e desempenha um papel vital na formação dos elementos nas estrelas e no 




universo inicial. 




Fóton

 • um bóson que carrega a força eletromagnética. Uma partícula quântica da luz. 




Galáxia

 • um grande sistema de estrelas, matéria interestelar e matéria escura que é mantido 




juntos pela gravidade. 




Gravidade

 • a mais fraca das quatro forças da natureza. É o meio pelo qual os objetos que 




possuem massa atraem-se mutuamente. 




Histórias alternativas

 • uma formulação da teoria quântica em que a probabilidade de qualquer observação ser interpretada a partir de todas as histórias possíveis que poderiam ter levado 




àquela observação. 




Leis aparentes

 • as leis da natureza que observamos em nosso universo—as leis das quatro 




forças e os parâmetros tais como massa e carga que caracterizam as particulas elementares—em 




contraste com as leis mais fundamentais da Teoria-M que permite diferentes universos com leis 




diferentes. 




Liberdade assintótica

 • uma propriedade da força forte que faz com que ela se torne mais 




fraco a curtas distâncias. Assim, embora os quarks estejam ligados em núcleos pela força forte, 




eles podem se mover dentro de núcleos quase como se eles não sentisse, qualquer força. 




Méson

 • um tipo de partícula elementar que é composto de um quark e um anti-quark. 




Multiverso

 • um conjunto de universos. 




Neutrino

 • uma partícula elementar leve que é afetada apenas pela força nuclear fraca e pela 




gravidade. 




Neutron

 • um tipo de bárion eletricamente neutro que, com o próton constitui o núcleo de um 




átomo. 




Princípio Antrópico

 • a idéia de que podemos tirar conclusões sobre as leis aparentes da física com base no fato de que nós existimos. 




Princípio da incerteza de Heisenberg

 • uma lei da teoria quântica afirmando que certos pares 




de propriedades físicas não podem ser conhecidos simultaneamente com precisão arbitrária. 




Próton

 • um tipo de bárion com carga positiva que, com o nêutron constitui o núcleo de um 




átomo. 




Quark

 • uma partícula elementar com carga elétrica fracionária que sente a força forte. Prótons e nêutrons são compostos, cada um, de três quarks. 




Renormalização

 • uma técnica matemática desenvolvida para dar sentido a infinitos que 




surgem em teorias quânticas. 




Singularidade

 • um ponto no espaço-tempo no qual uma quantidade física torna-se infinita. 




Supergravidade

 • uma teoria da gravidade que tem um tipo de simetria chamada supersimetria. 




Supersimetria

 • um tipo mais sutil de simetria que não pode ser associado a uma 




transformação do espaço comum. Uma das implicações importantes da supersimetria é que as 




partículas de força e partículas de matéria e, portanto, força e matéria, são apenas duas facetas 




de uma mesma coisa. 




Teoria das cordas

 • Teoria da física em que partículas são descritas como padrões de vibração que têm comprimento, mas sem altura ou largura - como pedaços de corda infinitamente finos. 




Teoria quântica 

• uma teoria em que os objetos não têm história única definida. 




Teoria-M

 • uma teoria fundamental da física que é um candidato a teoria de tudo.

AGRADECIMENTOS 




O UNIVERSO TEM UM PROJETO, e assim tem um livro. Mas, ao contrário do Universo, um 




livro não aparece espontaneamente do nada. Um livro exige um criador, e este papel não cai 




apenas sobre os ombros de seus autores. Então, em primeiro lugar, gostaríamos de agradecer 




aos nossos editores, Beth Rashbaum e Ann Harris, por sua paciência quase infinita. Eles foram 




nossos alunos quando precisamos de alunos, nossos professores quando precisamos de 




professores e nossos incentivadores quando precisamos de incentivo. Eles se ativeram ao 




manuscrito, e o fizeram com bom ânimo, se a discussão girava em torno da colocação de uma 




vírgula ou a impossibilidade de incorporação de uma superfície de curvatura negativa 




assimétricamente no espaço plano. Também gostaríamos de agradecer a Mark Hillery, que 




gentilmente leu grande parte do manuscrito e ofereceu contribuições valiosas; à Carole 




Lowenstein, que tanto fez para ajudar com a decoração de interiores; a David Stevenson, que 




orientou a realização da capa, e a Loren Noveck, cuja atenção aos detalhes nos salvou de alguns 




erros que não gostaríamos de ter visto serem transformados em letra impressa. A Peter 




Bollinger: muita gratidão por trazer arte à ciência em suas ilustrações, e por seu empenho em 




assegurar a exatidão de todos os detalhes. E a Sidney Harris: Obrigado por seus cartuns 




maravilhosos, e sua grande sensibilidade para as questões enfrentadas pelos cientistas. Em 




outro universo, você poderia ter sido um físico. Agradecemos também aos nossos agentes, Al 




Zuckerman e Susan Ginsburg, por seu apoio e incentivo. Se há duas mensagens que eles 




consistentemente ofereceram, elas são: "Já é hora de terminar o livro" e "Não se preocupe sobre quando você terminará, você vai chegar lá um dia." Eles foram sábios o suficiente para saber 




quando dizer o que. E, finalmente, nossos agradecimentos à assistente pessoal de Stephen, 




Judith Croasdell, seu assessor de computação, Sam Blackburn e Joan Godwin. Eles ofereceram 




não só apoio moral, mas um apoio prático e técnico sem o qual não poderiamos ter escrito este 




livro. Além disso, eles sempre souberam onde encontrar os melhores bares.

SOBRE OS AUTORES 




STEPHEN HAWKING foi Professor Lucasiano de Matemática da Universidade de Cambridge 




por 30 anos, e tem sido o destinatário de inúmeros prêmios e honrarias, incluindo, mais 




recentemente, a Medalha Presidencial da Liberdade. Seus livros para o leitor em geral incluem 




o clássico Uma História Breve do Tempo, a coleção de ensaios Black Holes and Baby 




Universes, O Universo em uma Casca de Noz, e Uma História Mais Vreve do Tempo. 




LEONARD MLODINOW é um físico da Caltech e autor do best-seller The Drunkard's Walk: 




How Randomness Rules Our Lives, Euclid's Window: The Story of Geometry from Parallel 




Lines to Hyperspace, Feynman’s Rainbow: A Search for Beauty in Physics and in Life, and A 




Briefer History of Time. Ele também escreveu para Star Trek: The Next Generation. Ele mora 




em South Pasadena, Califórnia.
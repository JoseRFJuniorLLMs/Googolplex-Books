À memória de Paul Adrien Maurice 

Dirac




1902-1984




 




Acho que posso dizer com segurança que ninguém entende 

mecânica quântica




Richard 

Feynman

Agradecimentos




Aos funcionários da Oxford University Press, por sua ajuda na preparação dos manuscritos para impressão, e principalmente à Shelley Cox, por uma série de comentários úteis sobre a primeira versão.




John Polkinghorne




Queens’ College 




Cambridge

Prefácio




A descoberta da teoria quântica moderna em meados da década de 1920 trouxe à tona a maior revisão da forma de pensar a natureza do mundo físico desde a época de Isaac 

Newton. O que fora considerado a arena de processos claros e determinados mostrou-se, em suas raízes subatômicas, nebuloso e intermitente em seu comportamento. Comparadas a essa mudança revolucionária, as maiores descobertas da relatividade geral e especial parecem não ser mais interessantes do que variações sobre temas clássicos. De fato, Albert 

Einstein, que fora o progenitor da teoria da relatividade, considerou a 

mecânica quântica moderna tão em desacordo com seu gosto metafísico que permaneceu implacavelmente contrário a ela até o fim da vida. Não é exagero algum pensar na teoria quântica como uma das realizações intelectuais mais incríveis do século XX e em sua descoberta como a representação de uma verdadeira revolução em nossa compreensão do processo físico.




Sendo assim, a apreciação de ideias quânticas não deve ser uma particularidade dos físicos teóricos. Embora a articulação completa da teoria exija o uso de sua linguagem natural, a matemática, muitos de seus conceitos básicos podem ser acessíveis ao leitor comum que está preparado para se dar ao trabalho de acompanhar a história de uma descoberta notável. Este pequeno livro foi escrito tendo esse leitor em mente. Seu texto principal não contém nenhuma equação matemática. Um breve apêndice destaca algumas ideias matemáticas simples que trarão um maior esclarecimento àqueles capazes de digerir uma carne um pouco mais dura. (Seções relevantes desse apêndice aparecem com referência cruzada em negrito no texto principal.)




A teoria quântica mostrou ser imensamente fértil durante os mais de 75 anos de sua investigação desde as descobertas iniciais. Ela é atualmente aplicada com confiança e êxito na discussão de quarks e glúons (os candidatos contemporâneos para os constituintes básicos da matéria nuclear), embora essas entidades sejam, no mínimo, 100 milhões de vezes menores do que os átomos, cujo comportamento foi motivo de interesse dos primeiros quânticos. Ainda assim, permanece um profundo paradoxo. A epígrafe deste livro contém algo do exagero da expressão que caracterizou o discurso do grande físico quântico de segunda geração, Richard 

Feynman, mas é certamente verídico que, embora saibamos realizar os cálculos, não 

entendemos

 a teoria de modo tão completo como deveríamos. Veremos, a seguir, que importantes questões interpretativas permanecem sem solução. Elas exigirão, para eventual resolução, não só observação física, como também decisão 

metafísica.




Quando jovem, tive o privilégio de aprender teoria quântica sendo aluno de Paul 

Dirac, enquanto ele ministrou seu famoso curso em Cambridge. O material das suas aulas correspondia de perto ao tratamento dado em seu livro seminal, 

Os princípios de 

mecânica quântica

, um dos verdadeiros clássicos da publicação científica do século XX. Além de ser o maior físico teórico que já conheci pessoalmente, 

Dirac tinha pureza de espírito e comportamento modesto (ele nunca enfatizava, nem minimamente, as próprias imensas contribuições aos princípios básicos desse tema), o que o tornou uma figura inspiradora e um tipo de santo-cientista. Humildemente dedico este livro à sua memória.

Capítulo 1




Rachaduras clássicas




O primeiro florescer da ciência física moderna atingiu seu auge em 1687, com a publicação da obra 

Principia

, de Isaac 

Newton. Desde então, a mecânica estabeleceu-se como uma disciplina madura, capaz de descrever os movimentos das partículas de uma maneira clara e determinista. Essa nova ciência parecia ser tão completa que, por volta do fim do século XVIII, o maior dos sucessores de 

Newton, Pierre Simon 

Laplace, pôde fazer sua célebre declaração de que um ser, equipado com poderes ilimitados de cálculo e suprido de conhecimento completo das disposições de todas as partículas em algum instante do tempo, poderia usar as equações de 

Newton para prever o futuro e retrodizer com igual certeza o passado do universo inteiro. De fato, essa alegação mecanicista um tanto assustadora trazia consigo uma grande suspeita de orgulho arrogante. Em primeiro lugar, os seres humanos não se veem como máquinas autômatas. Além disso, por mais imponentes que fossem, sem sombra de dúvida, as realizações de 

Newton, elas não abarcavam todos os aspectos do mundo físico que eram conhecidos na época. Restavam questões não resolvidas que ameaçavam a crença na autossuficiência total da síntese newtoniana. Por exemplo, havia a questão da verdadeira natureza e origem da lei da gravitação universal do inverso do quadrado que Sir Isaac descobrira. Tratava-se de um assunto sobre o qual o próprio 

Newton declinara formular uma hipótese. Havia, ainda, a questão não solucionada da natureza da 

luz. Nesse caso, 

Newton permitiu-se um grau de latitude especulativa. Em 

Opticks

, ele se inclinou à visão de que um feixe de 

luz era composto de uma corrente de partículas minúsculas. Esse tipo de teoria corpuscular estava em harmonia com a tendência de 

Newton de ver o mundo físico em termos atomísticos.




A natureza da 

luz




Foi somente no século XIX que houve um progresso real no entendimento da natureza da 

luz. Bem no começo do século, em 1801, 

Thomas Young apresentou evidências bastante convincentes para o fato de que a 

luz tinha a característica de um movimento de onda, uma especulação que fora feita mais de um século antes por 

Christiaan Huygens, contemporâneo holandês de 

Newton. As principais observações feitas por Young concentraram-se nos efeitos do que hoje chamamos de 

fenômenos de interferência. Um exemplo básico é a existência de bandas alternadas de 

luz e escuridão, o que, por mais irônico que seja, foram exibidas pelo próprio Sir Isaac em um fenômeno chamado de anéis de 

Newton. Efeitos desse tipo são característicos de 

ondas e surgem da seguinte forma: o modo como dois trens de 

ondas se combinam depende de como suas oscilações se inter-relacionam. Se estão no mesmo ritmo (em fase, como dizem os físicos), então a crista coincide de maneira construtiva com a crista, resultando em reforço mútuo máximo. Quando isso acontece no caso da 

luz, obtêm-se bandas de brilho. Se, no entanto, os dois conjuntos de 

ondas estão exatamente fora de ritmo (fora de fase), a crista coincide com o vale em uma anulação mutuamente destrutiva, e o resultado é uma banda de escuridão. Assim, a aparência de padrões de interferência de 

luz e escuridão alternadas é uma assinatura inequívoca da presença de 

ondas. As observações de Young pareciam ter resolvido o problema. A 

luz é ondulatória.




No decorrer do século XIX, a natureza do movimento ondulatório associado à 

luz parecia tornar-se clara. Descobertas importantes feitas por Hans Christian Oersted e Michael Faraday mostraram que eletricidade e magnetismo – fenômenos que, à primeira vista, pareciam muito distintos em sua natureza – estavam, na verdade, intimamente ligados entre si.




O modo como podiam ser combinadas para dar uma teoria consistente de eletromagnetismo veio a ser determinado por James Clerk 

Maxwell – um homem de tamanha genialidade que pode ser incluído, com justiça, no mesmo patamar de Isaac 

Newton. As célebres equações de 

Maxwell, ainda a base fundamental da teoria eletromagnética, foram estabelecidas em 1873 em seu 

Tratado sobre eletricidade e magnetismo

, um dos maiores clássicos da literatura científica. 

Maxwell percebeu que essas equações possuíam soluções ondulatórias e que a velocidade dessas 

ondas era determinada em termos de constantes físicas conhecidas. Esta revelou ser a familiar velocidade da 

luz!






















1. Sobreposição de 

ondas: (a) em fase; (b) fora de fase.













Essa descoberta foi considerada o maior triunfo da física do século XIX. O fato de que a 

luz era composta de 

ondas eletromagnéticas parecia ter sido estabelecido da maneira mais sólida possível. 

Maxwell e seus contemporâneos viam essas 

ondas como oscilações em um meio elástico difuso, que veio a ser chamado de éter. Em um artigo de enciclopédia, ele diria que o éter era a entidade mais confirmada de toda a teoria física.




Chamamos a física de 

Newton e 

Maxwell de física clássica. No fim do século XIX, ela havia se tornado um imponente edifício teórico. Não foi nada surpreendente o fato de que nobres homens mais velhos, como Lord Kelvin, chegaram a pensar que todas as ideias da física já eram conhecidas e que tudo o que havia para fazer era organizar os detalhes com maior precisão. Nos últimos quinze anos do século, um jovem na Alemanha, que contemplava uma carreira acadêmica, foi alertado contra o ingresso na física. Seria melhor tomar outro rumo, pois a física estava no fim da estrada, com pouco ou nada de relevante a ser feito. O nome desse jovem era Max 

Planck, que felizmente ignorou o conselho dado.




Na realidade, algumas rachaduras já começavam a aparecer na esplêndida fachada da física clássica. Na década de 1880, os norte-americanos 

Michelson e Morley conduziram alguns experimentos engenhosos com éter para demonstrar o movimento da Terra. A ideia era que, se nesse meio a 

luz fosse realmente 

ondas, então sua velocidade medida deveria depender de como o observador estava movendo-se em relação ao éter. Pense nas 

ondas do mar. Sua velocidade aparente, observada de um navio, altera-se conforme o navio se move a favor ou contra as 

ondas, parecendo menor no primeiro caso do que no segundo. O experimento foi projetado para comparar a velocidade da 

luz em dois sentidos mutuamente perpendiculares. Apenas se a Terra estivesse em repouso coincidente em relação ao éter quando as mensurações foram feitas é que as duas velocidades seriam as mesmas, e essa possibilidade poderia ser excluída repetindo-se o experimento alguns meses depois, quando a Terra estaria movendo-se em um sentido diferente em sua órbita. Na prática, 

Michelson e Morley não conseguiram detectar nenhuma diferença de velocidade. A solução desse problema exigiria a teoria da relatividade especial de 

Einstein, que prescindia completamente de éter. A grande descoberta não é objeto dessa história, embora seja importante observar que a relatividade, mesmo sendo altamente significativa e surpreendente, não abolia as qualidades de clareza e determinismo que a física clássica possuía. É por isso que, no Prefácio, afirmei que a relatividade especial exigia muito menos em termos de um novo raciocínio radical do que a teoria quântica viria a demandar.




Espectros




O primeiro indício da revolução quântica, irreconhecível como tal na época, surgiu em 1885. Ele teve origem nos rascunhos matemáticos de um professor secundário suíço chamado 

Balmer. Ele estava pensando no espectro do hidrogênio, ou seja, o conjunto de linhas coloridas separadas que se encontram quando a 

luz do gás incandescente é dividida ao passar por um prisma. As diferentes cores correspondem a frequências distintas (taxas de oscilação) das 

ondas de 

luz envolvidas. Enquanto brincava com os números sem obter êxito algum, 

Balmer descobriu que essas frequências poderiam ser descritas por uma fórmula matemática bastante simples [1]

1

. Naquele tempo, isso não teria passado de mera curiosidade. 




Mais tarde, as pessoas tentaram entender o resultado de 

Balmer em termos de sua imagem contemporânea do átomo. Em 1897, J. J. Thomson descobriu que a carga negativa em um átomo era carregada por partículas minúsculas, que acabaram recebendo o nome de “elétrons”. Supunha-se que a carga positiva restante era simplesmente espalhada pelo átomo. Essa ideia foi chamada de “modelo de pudim de passas”, com os elétrons exercendo a função das passas, e a carga positiva, a do pudim. As frequências espectrais deveriam corresponder às várias maneiras como os elétrons podem oscilar dentro do “pudim” com carga positiva. Porém, verificou-se ser muito difícil fazer essa ideia funcionar de modo empiricamente satisfatório. Veremos que a verdadeira explicação da estranha descoberta de 

Balmer seria encontrada mais tarde, usando-se um conjunto muito distinto de ideias. Enquanto isso, a natureza dos átomos provavelmente parecia uma questão obscura demais para que esses problemas originassem uma ansiedade coletiva.




A 

catástrofe ultravioleta




Muito mais nitidamente desafiadora e complexa era outra dificuldade, trazida à tona pela primeira vez por Lord 

Rayleigh em 1900, que veio a se chamar “a 

catástrofe ultravioleta”. Ela surgiu da aplicação das ideias de outra grande descoberta do século XIX, a física estatística. Nela, os cientistas estavam tentando desvendar o comportamento de sistemas muito complicados, que tinham formas bem diferentes do que seus movimentos detalhados poderiam assumir. Um exemplo desse sistema seria um gás composto de muitas moléculas distintas, cada uma com seu próprio estado de movimento. Outro exemplo seria a energia radioativa, que poderia ser composta de contribuições divididas entre muitas frequências distintas. Seria praticamente impossível monitorar todos os detalhes do que estava acontecendo em sistemas dessa complexidade, mas, apesar disso, alguns aspectos importantes de seu comportamento geral poderiam ser elaborados. Isso ocorria porque o comportamento físico resulta de uma média aproximada calculada sobre contribuições de muitos estados componentes individuais do movimento. Entre essas possibilidades, o conjunto mais provável predomina porque é, de forma preponderante, o mais provável. Sobre essa base de maximizar a 

probabilidade, Clerk 

Maxwell e 

Ludwig Bolzmann conseguiram demonstrar que é possível calcular com confiabilidade certas propriedades físicas do comportamento geral de um sistema complexo, como a pressão em um gás de determinado volume e temperatura.




Rayleigh aplicou essas técnicas de física estatística à questão de como a energia é distribuída entre diferentes frequências no caso da 

radiação de corpo negro. Um corpo negro é aquele que absorve perfeitamente toda a radiação que recai sobre ele e, a seguir, reemite a radiação por completo. O tema do espectro da radiação em equilíbrio com um corpo negro poderia parecer um tipo de questão um tanto exótica para se formular, porém, na verdade, há aproximações excelentes a corpos negros disponíveis; então, trata-se de um assunto que pode ser investigado de modo experimental e teórico, por exemplo, estudando-se a radiação no interior de um forno especialmente preparado. A questão era simplificada pelo fato de que já se sabia que a resposta dependeria somente da temperatura do corpo, e não de detalhes adicionais de sua estrutura. 

Rayleigh destacou que a aplicação objetiva das ideias comprovadas da física estatística levou a um resultado desastroso. Além de o cálculo não estar de acordo com o espectro medido, ele não fazia sentido algum. Ele previa que uma quantidade infinita de energia estaria concentrada nas frequências mais altas, uma conclusão constrangedora que recebeu o nome de “

catástrofe ultravioleta”. A natureza catastrófica dessa conclusão é bastante evidente: “ultravioleta” é uma maneira de dizer “altas frequências”. O desastre surgiu porque a física estatística clássica prevê que cada grau de liberdade do sistema (nesse caso, cada modo distinto com que a radiação pode ondular) receberá o mesmo volume fixo de energia, uma quantidade que só depende da temperatura. Quanto maior a frequência, maior o número de modos correspondentes de oscilação, com o resultado de que as frequências mais altas roubam tudo, acumulando quantidades ilimitadas de energia. Aqui havia um problema que equivalia a mais do que uma pequena falha na esplêndida fachada da física clássica. Era mais o caso de um buraco aberto no prédio.




Em um ano, Max 

Planck, agora professor de física em Berlim, encontrara uma maneira notável de solucionar o dilema. Ele disse a seu filho que acreditava ter feito uma descoberta de igual significância às de 

Newton. Pode ter parecido uma alegação grandiosa, mas 

Planck estava dizendo nada menos que a pura verdade.




A física clássica considerava que a radiação vertia continuamente para dentro e para fora do corpo negro, semelhante a como a água pode verter de uma esponja. No mundo de suaves mudanças da física clássica, nenhuma outra suposição parecia totalmente plausível. Mesmo assim, 

Planck fez uma proposta contrária, sugerindo que a radiação era emitida ou absorvida de tempos em tempos em pacotes de energia de tamanho definido. Ele especificou que o conteúdo energético de um desses 

quanta

 (como eram chamados os pacotes) seria proporcional à frequência da radiação. A constante de proporcionalidade foi tirada de uma constante universal da natureza, agora conhecida como 

constante de 

Planck. Ela é representada pelo símbolo 

h

. A magnitude de 

h

 é muito pequena em termos de tamanhos correspondentes à experiência cotidiana. É por isso que esse comportamento pontuado de radiação não fora notado antes; uma sequência de pequenos pontos muito próximos assemelha-se a uma linha cheia.




Uma consequência dessa hipótese ousada era que a radiação de alta frequência só poderia ser emitida ou absorvida em eventos que envolvessem um único quantum de energia significativamente alta. Essa grande tarifa energética significava que esses eventos de alta frequência seriam gravemente suprimidos em comparação às expectativas da física clássica. Subjugar dessa forma as altas frequências eliminava a 

catástrofe ultravioleta, além de gerar uma fórmula em detalhada concordância com o resultado empírico.




Era evidente que 

Planck descobrira algo de grande significância. Contudo, exatamente qual era tal significância, nem ele nem outras pessoas tinham certeza no início. Que grau de seriedade deveríamos atribuir aos 

quanta? Eles eram uma característica persistente da radiação ou apenas um aspecto do modo como a radiação interagia com um corpo negro? Afinal, gotas de uma torneira formam uma sequência de 

quanta aquoso, mas fundem-se com o restante da água e perdem sua identidade individual assim que caem na pia.




O 

efeito fotoelétrico




O próximo avanço foi feito por um jovem com tempo de sobra enquanto trabalhava como especialista técnico de terceira classe no escritório de patentes em Berna. Seu nome era Albert 

Einstein. Em 1905, o 

annus mirabilis

 de 

Einstein, ele fez três descobertas fundamentais. Uma delas acabou sendo o próximo passo no desdobramento da história da teoria quântica. 

Einstein pensou sobre as propriedades enigmáticas que vieram à tona com as investigações sobre o 

efeito fotoelétrico [2]. Esse é o fenômeno pelo qual um feixe de 

luz ejeta elétrons de dentro de um metal. Os metais contêm elétrons que são capazes de se mover em seu interior (seu fluxo é o que gera uma corrente elétrica), mas que não têm energia suficiente para escapar completamente do metal. Que o 

efeito fotoelétrico acontecia não era surpresa alguma. A radiação transfere energia para os elétrons aprisionados dentro do metal e, se o ganho for suficiente, um elétron consegue escapar das forças que o confinam. Em um tipo de raciocínio clássico, os elétrons seriam agitados pelo “marulho” das 

ondas de 

luz, e alguns poderiam ser perturbados o bastante para se livrar do metal. Segundo essa representação, o grau em que isso acontecia dependeria da intensidade do feixe, uma vez que determinava seu conteúdo energético, mas não era possível prever nenhuma dependência particular da frequência da 

luz incidente. Na prática, os experimentos mostravam um comportamento exatamente oposto. Abaixo de uma determinada frequência crítica, não havia emissão de elétrons, por mais intenso que o feixe fosse; acima dessa frequência, mesmo um feixe fraco poderia ejetar alguns elétrons.




Einstein viu esse comportamento intrigante tornar-se instantaneamente inteligível se o feixe de 

luz fosse considerado uma corrente de 

quanta persistente. Um elétron seria ejetado porque um desses 

quanta colidira com ele, abrindo mão de toda a sua energia. A quantidade de energia nesse quantum, segundo 

Planck, era diretamente proporcional à frequência. Se a frequência fosse baixa demais, não haveria energia transferida em uma colisão para permitir que o elétron escapasse. Por outro lado, se a frequência excedesse um determinado valor crítico, haveria energia suficiente para que o elétron conseguisse escapar. A intensidade do feixe simplesmente determinava quantos 

quanta ele continha e, portanto, quantos elétrons estavam envolvidos em colisões e eram ejetados. O aumento da intensidade não poderia alterar a energia transferida em uma única colisão. Levando a sério a existência dos 

quanta de 

luz (eles vieram a ser chamados de “fótons”), seria possível explicar o mistério do 

efeito fotoelétrico. O jovem 

Einstein fizera uma descoberta crucial. Na verdade, ele acabou recebendo o Prêmio Nobel por ela. A academia sueca presumivelmente considerou suas outras duas grandes descobertas de 1905 – a relatividade especial e uma demonstração convincente da realidade das moléculas – ainda especulativas demais para ser recompensado de tal forma!




A análise quântica do 

efeito fotoelétrico foi uma grande vitória da física, mas assemelhava-se a uma vitória pírrica. O tema agora passava por uma grave crise. Como todas aquelas brilhantes percepções do século XIX sobre a natureza ondulatória da 

luz seriam conciliadas com essas novas ideias? Afinal, uma onda é algo espalhado, suspenso, enquanto um quantum é semelhante a uma partícula, um tipo de projétil pequeno. Como seria possível que as duas fossem verdadeiras? Durante muito tempo, os físicos simplesmente tiveram de viver com o paradoxo desconfortável da natureza onda/partícula da 

luz. Não teria havido progresso na tentativa de negar as descobertas de Young e 

Maxwell ou de 

Planck e 

Einstein. As pessoas tinham que se agarrar à experiência com a ponta dos dedos intelectuais, mesmo que não conseguissem compreendê-la. Parece que muitos atingiram isso por meio da tática um tanto covarde de desviar o olhar. Com o tempo, no entanto, veremos que a história teve um final feliz.




O átomo nuclear




Enquanto isso, a atenção voltou-se da 

luz para os átomos. Em Manchester, em 1911, Ernest 

Rutherford e alguns colaboradores mais jovens começaram a estudar como alguns projéteis pequenos e com carga positiva, chamados de partículas α, comportavam-se ao se chocar com uma lâmina delgada de ouro. Muitas partículas α conseguiam atravessá-la sendo pouco afetadas; porém, para grande surpresa dos investigadores, algumas eram defletidas de modo considerável. 

Rutherford disse mais tarde que era tão surpreendente como se um projétil de navio de guerra com quinze polegadas tivesse retrocedido ao atingir um lenço de papel. O modelo atômico de pudim de passas não evidenciava o sentido desse resultado. As partículas α deveriam ter passado como um projétil atravessa um bolo. 

Rutherford rapidamente viu que havia apenas uma maneira de resolver esse dilema. A carga positiva dos átomos de ouro, que repeliria as partículas α positivas, não poderia espalhar-se como em um “pudim”, mas deveria estar concentrada no centro do átomo. Um encontro mais próximo com essa carga concentrada conseguiria defletir uma partícula α de maneira substancial. Consultando um antigo livro de mecânica de sua época de estudante universitário na Nova Zelândia, 

Rutherford – um físico experimental maravilhoso que não era grande coisa em matemática – pôde demonstrar que essa ideia, de uma carga central positiva no átomo orbitada por elétrons negativos, adequava-se com perfeição ao comportamento observado. O modelo de pudim de passas imediatamente cedeu lugar ao modelo atômico do “sistema solar”. 

Rutherford e seus colegas haviam descoberto o núcleo atômico.




Isso foi um enorme sucesso, mas parecia, à primeira vista, mais uma vitória de Pirro. Na verdade, a descoberta do núcleo conduziu a física clássica à sua crise mais profunda até então. Se os elétrons em um átomo estão circundando o núcleo, estão continuamente mudando seu sentido de movimento. A teoria eletromagnética clássica exige que, nesse processo, eles irradiem parte de sua energia. Como resultado, devem mover-se de forma constante mais próximo ao núcleo. Essa é uma conclusão verdadeiramente desastrosa, pois implica que os átomos seriam instáveis, já que seus elétrons componentes espiralavam até o colapso em direção ao centro. Além disso, no decorrer dessa desintegração, seria emitido um padrão contínuo de radiação que em nada se parecia com as frequências espectrais agudas da fórmula de 

Balmer. Depois de 1911, o grandioso edifício da física clássica não estava só começando a rachar. Parecia ter sido atingido por um terremoto.




O átomo de 

Bohr




No entanto, assim como ocorreu com 

Planck e a 

catástrofe ultravioleta, havia um físico teórico à mão para vir ao resgate e arrebatar o sucesso das mandíbulas do fracasso ao propor uma nova hipótese ousada e radical. Dessa vez, foi um jovem dinamarquês chamado 

Niels 

Bohr, que estava trabalhando na Manchester de 

Rutherford. Em 1913, 

Bohr fez uma proposta revolucionária [3]. 

A ideia clássica de um processo suave em que a energia vertida para dentro e para fora de um corpo negro foi substituida por Planck pela noção de um processo pontuado em que a energia é emitida ou absorvida como 

quanta. Em termos matemáticos, isso significava que uma quantidade como a energia trocada, que previamente se pensava obter qualquer valor possível, agora era considerada capaz de assumir apenas uma série de valores agudos (1, 2, 3, ... pacotes envolvidos). Os matemáticos diriam que o contínuo fora substituído pelo discreto. 

Bohr via que isso poderia ser uma tendência bastante geral ao novo tipo de física que estava nascendo lentamente. Ele aplicou aos átomos princípios semelhantes que 

Planck aplicara à radiação. Um físico clássico teria suposto que os elétrons circundando um núcleo poderiam fazer isso em órbitas cujos raios conseguiriam assumir qualquer valor. 

Bohr propôs a substituição dessa possibilidade contínua pela exigência discreta de que os raios pudessem assumir somente uma série de valores distintos que fosse possível enumerar (primeiro, segundo, terceiro...). Ele também fez uma sugestão definitiva de como esses possíveis raios eram determinados, usando uma prescrição que envolvia a constante 

h

 de 

Planck. (A proposta relacionava-se ao momentum angular, uma medida do movimento rotatório do elétron que é mensurada nas mesmas unidades físicas que 

h

.)




Duas consequências seguiam-se a essas propostas. Uma era a propriedade altamente desejável de restabelecer a estabilidade dos átomos. Uma vez que um elétron estivesse no estado correspondente ao menor raio permitido (que também era o estado de energia mais baixa), ele não tinha lugar algum para ir e, portanto, nenhuma energia adicional poderia ser perdida. O elétron poderia ter chegado a esse estado mais baixo perdendo energia à medida que se movia de um estado de raio mais alto. 

Bohr presumiu que, quando isso acontecesse, a energia excedente seria irradiada como um único 

fóton. Os cálculos realizados demonstraram que essa ideia levava objetivamente à segunda consequência da corajosa conjectura de 

Bohr: a previsão da fórmula de 

Balmer para linhas espectrais. Após quase trinta anos, essa misteriosa prescrição numérica deixou de ser uma excentricidade inexplicável para se tornar uma propriedade inteligível da nova teoria dos átomos. A agudeza das linhas espectrais foi vista como um reflexo da descontinuidade que estava começando a ser reconhecida como um traço característico do pensamento quântico. O movimento espiral contínuo que teria sido esperado com base na física clássica fora substituído por um salto quântico fortemente descontínuo de uma órbita de um raio permitido para uma órbita de um raio permitido menor.




O átomo de 

Bohr foi um grande triunfo. Porém, ele surgira de um ato de experimentação inspirada com o que ainda era, em muitos aspectos, a física clássica. O trabalho pioneiro de 

Bohr era, na realidade, um conserto substancial, um remendo sobre o edifício fragmentado da física clássica. Tentativas de estender esses conceitos ainda mais logo começaram a enfrentar dificuldades e encontrar inconsistências. A “antiga teoria quântica”, como tais esforços vieram a ser chamados, era uma combinação constrangedora e não reconciliada das ideias clássicas de 

Newton e 

Maxwell com as prescrições quânticas de 

Planck e 

Einstein. O trabalho de 

Bohr foi um passo crucial no desenrolar da história da física quântica, mas não poderia ser mais do que uma área de encontro no caminho da “nova teoria quântica”, uma explicação completamente integrada e consistente dessas estranhas ideias. Antes que ela fosse atingida, havia outro fenômeno importante a ser descoberto, que enfatizava ainda mais a necessidade inevitável de dar conta do raciocínio quântico.




Espalhamento de Compton




Em 1923, o físico norte-americano Arthur Compton investigou o espalhamento de raios X (radiação eletromagnética de alta frequência) de acordo com a matéria. Ele constatou que a radiação espalhada tinha sua frequência alterada. Em uma representação de onda, isso não poderia ser entendido. Aquela constatação implicava que o processo de espalhamento ocorria porque os elétrons presentes nos átomos absorviam e reemitiam energia das 

ondas incidentes e que isso aconteceria sem uma mudança de frequência. Em uma representação de 

fóton, o resultado poderia ser facilmente compreendido. O que estaria envolvido seria uma colisão do tipo “bola de bilhar” entre um elétron e um 

fóton e, no curso disso, o 

fóton perderia parte de sua energia para o elétron. Segundo a prescrição de 

Planck, mudança de energia é o mesmo que mudança de frequência. Assim, Compton pôde elaborar uma explicação quantitativa para suas observações, fornecendo, com isso, a evidência até então mais persuasiva da característica corpuscular da radiação eletromagnética.




As perplexidades às quais a sequência de descobertas discutidas neste capítulo deu origem não ficariam muito tempo sem ser abordadas. Dois anos após o trabalho de Compton, o progresso teórico de um tipo substancial e duradouro acabou sendo feito. A 

luz da nova teoria quântica começava a despontar.













1

 As indicações numéricas entre colchetes referem-se ao Apêndice matemático na p. 116.

Capítulo 2




O nascer da 

luz




Os anos seguintes à proposta pioneira de Max 

Planck foram uma época de confusão e escuridão para a comunidade física. A 

luz era 

ondas; a 

luz era partículas. Modelos tentadoramente exitosos, como o átomo de 

Bohr, traziam a promessa de que uma nova teoria física estava prestes a acontecer, mas a imposição imperfeita desses remendos quânticos nas ruínas da física clássica mostrou que era preciso mais discernimento antes que uma representação consistente pudesse emergir. Quando, por fim, a 

luz despontou, ela o fez com toda a subitaneidade de uma aurora tropical.




Nos anos de 1925 e 1926, a teoria quântica moderna alcançou seu pleno desenvolvimento. Esses 

anni mirabiles

 continuam sendo um episódio de enorme significância na memória folclórica da comunidade da física teórica, ainda recordado com respeito, apesar do fato de que a memória viva não tenha mais acesso a esses tempos heroicos. Quando há agitações contemporâneas sobre aspectos fundamentais da teoria física, algumas pessoas podem dizer: “Tenho a impressão de que 1925 está se repetindo”. Existe um tom melancólico presente nesse comentário. Como disse Wordsworth sobre a Revolução Francesa: “Era maravilhoso, naquela manhã, estar vivo, mas ser jovem era o próprio paraíso!”. De fato, embora muitos avanços importantes tenham sido feitos nos últimos 75 anos, ainda não houve uma segunda vez em que a revisão radical de princípios físicos tenha sido necessária na mesma escala que presenciou o nascimento da teoria quântica.




Dois homens em especial puseram a revolução quântica em marcha, produzindo novas ideias espantosas quase simultaneamente.



















2. A elite da teoria quântica: Conferência de Solvay, 1927.










Mecânica matricial




Um deles era um jovem teórico alemão, Werner 

Heisenberg. Ele vinha trabalhando para entender os detalhes dos 

espectros atômicos. A espectroscopia desempenhara uma importantíssima função no desenvolvimento da física moderna. Um motivo para isso foi que as técnicas experimentais para a 

medição das frequências de linhas espectrais são capazes de grande refinamento, gerando resultados muito exatos, os quais propõem problemas específicos para os teóricos atacarem. Já vimos um exemplo simples no caso do espectro do hidrogênio, com a fórmula de 

Balmer e a explicação dada por 

Bohr em termos de seu modelo atômico. As coisas ficaram mais complicadas desde então, e 

Heisenberg estava interessado em um ataque muito mais amplo e ambicioso das propriedades espectrais em termos gerais. Enquanto se recuperava em Heligoland, uma ilha do Mar do Norte, de uma crise grave de febre do feno, ele fez sua grande descoberta. Os cálculos pareciam bastante complexos, porém, quando o pó da matemática assentava, ficava evidente que o que estava envolvido era a manipulação de entidades matemáticas chamadas de matrizes (arranjos de números que se multiplicam juntos de uma maneira específica). Logo, a descoberta de 

Heisenberg ficou conhecida como 

mecânica matricial. As ideias subjacentes reaparecerão um pouco mais tarde de forma ainda mais geral. Por enquanto, vamos apenas observar que as matrizes diferem de simples números porque, via de regra, elas não comutam. Isso equivale a dizer que, se A e B são duas matrizes, o produto AB e o produto BA não são o mesmo. A ordem da multiplicação importa, diferentemente do que acontece com os números, nos quais dois multiplicados por três e três multiplicados por dois equivalem a seis. Ao que se constatou, essa propriedade matemática das matrizes tem um importante significado físico conectado com quais quantidades poderiam ser medidas simultaneamente na 

mecânica quântica. [No item 4 do Apêndice matemático, há uma generalização matemática que se mostrou necessária para o completo desenvolvimento da teoria quântica.]




Em 1925, as matrizes eram tão matematicamente exóticas para um físico teórico médio como podem ser hoje para o leitor médio não matemático deste livro. Muito mais conhecida dos físicos daquela época era a matemática associada ao movimento de 

ondas (envolvendo equações diferenciais parciais). Usavam-se técnicas que eram padrão na física clássica do tipo que 

Maxwell desenvolvera. Seguindo de perto a descoberta de 

Heisenberg, havia uma versão da teoria quântica com aparência bem distinta, baseada na matemática muito mais simpática das equações de onda.




Mecânica ondulatória




De modo bastante adequado, essa segunda representação da teoria quântica era chamada de 

mecânica ondulatória. Embora sua versão completamente desenvolvida tenha sido descoberta pelo físico austríaco 

Erwin Schrödinger, um movimento na direção certa fora feito um pouco antes no trabalho de um jovem aristocrata francês, o Príncipe Louis de Broglie [5]. Este fez a ousada sugestão de que, se a 

luz ondulante também mostrava propriedades corpusculares, talvez, de modo correspondente, pudéssemos esperar que partículas como os elétrons manifestassem propriedades corpusculares. 

De Broglie pôde moldar essa ideia em termos quantitativos com a generalização da fórmula de 

Planck, que tornara a propriedade corpuscular da energia proporcional à propriedade ondulatória da frequência. 

De Broglie sugeriu que outra propriedade corpuscular, o momentum (uma quantidade física significativa, bem-definida e aproximadamente correspondente à quantidade do movimento persistente de uma partícula), deveria ser analogamente relacionada à outra propriedade corpuscular, o comprimento de onda, com a constante universal de 

Planck sendo novamente a constante relevante de proporcionalidade. Essas equivalências geraram um tipo de minidicionário para traduzir de partículas para 

ondas, e vice-versa. Em 1924, 

De Broglie esquematizou essas ideias em sua tese de doutorado. As autoridades da Universidade de Paris ficaram desconfiadas dessas noções heterodoxas, mas felizmente consultaram 

Einstein. Ele reconheceu a genialidade do jovem e o título acadêmico foi-lhe concedido. Em alguns anos, experimentos conduzidos por Davisson e Germer, nos Estados Unidos, e por 

George Thomson, na Inglaterra, puderam demonstrar a existência de padrões de interferência quando um feixe de elétrons interagia com uma estrutura cristalina, confirmando que os elétrons de fato manifestavam comportamento ondulatório. Louis de Broglie recebeu o Prêmio Nobel de física em 1929. (

George Thomson era filho de J. J. Thomson. Costuma-se dizer que o pai ganhou o Prêmio Nobel por demonstrar que o elétron é uma partícula, enquanto o Prêmio Nobel foi dado ao filho por mostrar que o elétron é uma onda.)




As ideias que 

De Broglie desenvolveu eram baseadas na discussão das propriedades de partículas em livre movimento. Para atingir uma teoria inteiramente dinâmica, uma generalização adicional seria necessária para permitir a incorporação de interações. Esse é o problema que Schrödinger teve êxito em resolver. No início de 1926, ele publicou a famosa equação que agora recebe o seu nome [6]. O caminho para essa descoberta foi a investigação de uma analogia retirada da óptica. 




Embora os físicos do século XIX pensassem na 

luz como sendo composta de 

ondas, eles nem sempre usavam as maduras técnicas de cálculos do movimento ondulatório para calcular o que estava acontecendo. Se o comprimento de onda da 

luz era pequeno em comparação às dimensões que definiam o problema, era possível empregar um método mais simples no geral. Essa era a abordagem da óptica geométrica, que tratava a 

luz como se propagando em raios de linha reta, os quais eram refletidos ou refratados de acordo com regras simples. Os cálculos da escola de física de sistemas elementares de lentes e espelhos são realizados hoje exatamente da mesma forma, sem se preocupar com as complexidades de uma equação de 

ondas. A simplicidade da óptica de raios aplicada à 

luz é semelhante à simplicidade do desenho de trajetórias na mecânica de partículas. Se esta mostrasse ser apenas uma aproximação de uma 

mecânica ondulatória subjacente, Schrödinger argumentava que essa 

mecânica ondulatória poderia ser descoberta revertendo-se o tipo de considerações que haviam conduzido da óptica de 

ondas para a óptica geométrica. Assim, ele descobriu a 

equação de Schrödinger.




Schrödinger publicou suas ideias somente alguns meses após 

Heisenberg ter apresentado sua teoria da 

mecânica matricial para a comunidade da física. Naquela época, Schrödinger tinha 38 anos, dando um contraexemplo impressionante da afirmação, às vezes feita por não cientistas, de que os físicos teóricos fazem seu trabalho verdadeiramente original antes dos 25. A 

equação de Schrödinger é a equação dinâmica fundamental da teoria quântica. É um tipo bem direto de equação diferencial parcial, daquelas que eram conhecidas dos físicos naquele tempo e para as quais eles tinham uma bateria formidável de técnicas de solução matemática. Era muito mais fácil de usar do que os métodos matriciais recém-criados por 

Heisenberg. Ao mesmo tempo, as pessoas podiam pôr mãos à obra aplicando essas ideias a uma variedade de problemas físicos específicos. O próprio Schrödinger conseguiu derivar a fórmula de 

Balmer de sua equação para o espectro do hidrogênio. Esse cálculo mostrou como 

Bohr estivera tão próximo – mas também tão distante – da verdade na reformulação inspirada da antiga teoria quântica. (O momentum angular era importante, porém não exatamente da maneira que 

Bohr havia proposto.) 




Mecânica quântica 




Estava claro que 

Heisenberg e Schrödinger fizeram avanços esplêndidos. Ainda assim, à primeira vista, o modo como eles apresentaram suas novas ideias pareceu tão diferente que não estava evidente se haviam feito a mesma descoberta, expressa de maneira distinta, ou se havia duas propostas rivais na mesa [veja a discussão no item 10 do Apêndice]. Seguiu-se um importante trabalho de esclarecimento, para o qual 

Max Born, em Göttingen, e Paul 

Dirac, em Cambridge, fizeram contribuições especialmente significativas. Logo ficou estabelecido que havia uma única teoria baseada em princípios gerais comuns, cuja articulação matemática poderia assumir uma variedade de formas equivalentes. Esses princípios gerais acabaram por ser expostos de modo mais transparente nos 

Princípios de 

mecânica quântica

, de 

Dirac, publicado pela primeira vez em 1930 e um dos clássicos intelectuais do século XX. O prefácio da primeira edição começa com o enunciado falsamente simples: “Os métodos do progresso na teoria física passaram por uma enorme mudança durante o século atual”. Devemos agora considerar a representação transformada da natureza do mundo físico que essa enorme mudança introduzira.




Aprendi 

mecânica quântica direto da fonte, por assim dizer, pois participei do famoso curso sobre teoria quântica que 

Dirac ministrou em Cambridge por um período de mais de trinta anos. Entre os participantes, havia graduandos do último ano, como eu, e também visitantes graduados que acreditavam, com razão, ser um privilégio ouvir a história novamente, por mais conhecida que pudesse ser para eles em linhas gerais, da boca do homem que fora um de seus maiores protagonistas. As palestras seguiam de perto o padrão do livro de 

Dirac. Uma característica impressionante era a falta total de ênfase da parte do palestrante sobre o que fora sua considerável contribuição pessoal a essas grandes descobertas. Já falei sobre 

Dirac como um tipo de santo científico, sobre a pureza de sua mente e a simplicidade de seu objetivo. As palestras cativavam por sua clareza e pelo majestoso desdobramento de seu argumento, tão satisfatório e aparentemente inevitável como o desenvolvimento de uma fuga de Bach. Elas eram privadas de qualquer tipo de truque retórico, mas próximo ao início 

Dirac permitia-se um gesto levemente teatral.




Ele pegava um pedaço de giz e o quebrava em dois. Posicionando um fragmento em um lado do púlpito e o outro no lado oposto, 

Dirac dizia que classicamente existe um estado em que o pedaço de giz está “aqui” e outro em que o pedaço de giz está “lá”, e essas são as duas únicas possibilidades. Porém, substitua o giz por um elétron e, no mundo quântico, não há apenas estados de “aqui” e “lá”, mas uma vasta quantidade de outros estados que são misturas dessas possibilidades – um pouco de “aqui” e outro tanto de “lá”, todos juntos. A teoria quântica permite a mistura de estados que classicamente seriam excludentes entre si. É essa possibilidade contraintuitiva de adição que distingue o mundo quântico do mundo cotidiano da física clássica [7]. No jargão profissional, essa nova possibilidade é chamada de 

princípio da sobreposição

. 




Fendas duplas e sobreposição




As consequências radicais que se seguem à pressuposição de sobreposição estão bem-ilustradas pelo que se chama de 

experimento da fenda dupla. Richard 

Feynman, o espirituoso físico ganhador do Prêmio Nobel que atraiu a imaginação popular com seus livros com histórias engraçadas, uma vez descreveu esse fenômeno como estando no “coração da 

mecânica quântica”. Ele considerava que era preciso engolir a teoria quântica como um todo, sem se preocupar com o gosto nem se seria possível digeri-la. Isso poderia ser feito engolindo-se o 

experimento da fenda dupla, pois




Na realidade, ele contém o 

único

 mistério. Não podemos fazer o mistério desaparecer “explicando” como ele funciona. Vamos apenas 

dizer

 como ele funciona. Ao dizer como ele funciona, as peculiaridades básicas de toda a 

mecânica quântica serão ditas.




Após essa introdução, o leitor certamente gostará de saber mais sobre esse intrigante fenômeno. O experimento envolve uma fonte de entidades quânticas, digamos um bombardeador de elétrons que dispara um feixe contínuo de partículas. Essas partículas colidem com uma tela em que há duas fendas, A e B. Depois da tela com as fendas, há uma tela de detecção que pode registrar a chegada dos elétrons. Pode ser uma grande placa fotográfica sobre a qual o elétron incidente fará uma marca. O índice de distribuição do bombardeador de elétrons é ajustado para que haja somente um único elétron atravessando o aparato de cada vez. Depois observamos o que acontece.




Os elétrons chegam à tela de detecção um a um, e para cada um deles vemos uma marca correspondente que registra seu ponto de impacto. Isso expressa o comportamento individual do elétron em um modo corpuscular. No entanto, quando um grande número de marcas se acumula na tela de detecção, verificamos que o padrão coletivo criado por elas demonstra a conhecida forma de um efeito de interferência. 




Há um ponto intensamente escuro na tela oposta ao ponto médio entre as duas fendas, correspondente à localização na qual o maior número de marcas de elétrons foi depositado. Em qualquer lado dessa faixa central, existem faixas alternadamente claras e reduzidamente escuras, que correspondem à não chegada e chegada de elétrons nessas posições, respectivamente. Esse padrão de difração (como os físicos chamam tais efeitos de interferência) é uma assinatura inconfundível de elétrons que se comportam de modo ondulatório. 



















3. O 

experimento da fenda dupla.










O fenômeno é um belo exemplo da dualidade onda/partícula do elétron. Os elétrons que chegam um a um têm comportamento corpuscular; o padrão de interferência coletivo resultante é comportamento ondulatório. Contudo, há algo muito mais interessante do que isso a ser dito. Podemos ir um pouco mais a fundo nessa questão fazendo a seguinte pergunta: quando um único elétron indivisível está atravessando o aparato, através de qual fenda ele passa para chegar à tela de detecção? Vamos supor que ele tenha passado pela fenda superior, A. Se esse for o caso, a fenda inferior B seria realmente irrelevante e poderia até ser temporariamente fechada. Porém, somente com A aberta, o elétron provavelmente não chegaria ao ponto médio da tela distante; pelo contrário, seria mais provável que ele acabasse no ponto em frente a A. Já que esse não é o caso, concluímos que o elétron não poderia ter passado por A. Virando o argumento do avesso, concluímos que o elétron também não poderia ter passado por B. O que, então, estava acontecendo? Aquele grande e bom homem, Sherlock Holmes, gostava de dizer que, quando se eliminou o impossível, tudo o que restar tem fundamento, por mais improvável que pareça ser. A aplicação desse princípio holmesiano nos leva à conclusão de que o elétron indivisível 

passou pelas duas fendas

. Em termos de intuição clássica, essa é uma conclusão sem sentido. Todavia, em termos do 

princípio da sobreposição da teoria quântica, ela faz sentido. O estado de movimento do elétron foi a adição dos estados (passando por A) e (passando por B).




O 

princípio da sobreposição implica duas características gerais da teoria quântica. A primeira é que não é mais possível formar uma representação clara do que está acontecendo no decorrer do processo físico. Vivendo como nós no mundo (clássico) cotidiano, é impossível visualizar uma partícula indivisível passando por ambas as fendas. A segunda característica é que não é mais possível prever com exatidão o que acontecerá quando fizermos uma observação. Suponha que modificássemos o 

experimento da fenda dupla colocando um detector próximo a cada uma das fendas, de modo que fosse possível determinar por qual fenda o elétron passou. Acontece que essa modificação do experimento traria duas consequências. Uma delas é que, algumas vezes, o elétron seria detectado próximo à fenda A e, em outras, próximo à fenda B. Seria impossível prever onde ele seria encontrado em qualquer ocasião em especial, mas, durante uma longa série de testes, as probabilidades relativas associadas às duas fendas seriam de 50-50. Isso ilustra a característica geral de que, na teoria quântica, as previsões dos resultados da mensuração são estatísticas por natureza, e não deterministas. A teoria quântica lida com probabilidades, e não com certezas. A outra consequência dessa modificação do experimento seria a destruição do padrão de interferência na tela final. Os elétrons não tenderiam mais ao ponto médio da tela de detecção, mas seriam divididos igualmente entre os que chegassem em frente a A e os que chegassem em frente a B. Em outras palavras, o comportamento encontrado depende do que se está procurando. Uma pergunta corpuscular (qual fenda?) dá uma determinada resposta corpuscular; uma questão ondulatória (apenas sobre o padrão acumulado final na tela de detecção) recebe uma resposta ondulatória.




Probabilidades




Foi 

Max Born, em Göttingen, quem pela primeira vez enfatizou claramente a natureza probabilística da teoria quântica, uma realização pela qual ele receberia seu merecido Prêmio Nobel somente em 1954. O advento da 

mecânica ondulatória suscitou a conhecida questão: 

ondas de quê? Inicialmente, havia alguma disposição para supor que poderia tratar-se de uma questão de 

ondas de matéria, de modo que era o próprio elétron que estava espalhado dessa forma ondulatória. Born logo percebeu que essa ideia não funcionava, porque não dava conta de propriedades corpusculares. Em vez disso, eram 

ondas de 

probabilidade que a 

equação de Schrödinger descrevia. Esse desenvolvimento não agradou a todos os pioneiros, pois muitos retiveram intensamente os instintos deterministas da física clássica. Tanto 

De Broglie quanto Schrödinger ficaram desiludidos com a física quântica quando se depararam com sua natureza probabilística.




A interpretação de probabilidades implicava que as mensurações devem ser ocasiões de mudança instantânea e descontínua. Se um elétron estava em um estado com 

probabilidade dispersa “aqui”, “lá” e, talvez, “em todo lugar”, quando sua posição foi medida e constatou-se que, nessa ocasião, estava “aqui”, então a distribuição de 

probabilidade teve que subitamente mudar, ficando concentrada unicamente na posição realmente medida – “aqui”. Como a distribuição de 

probabilidade deve ser calculada com a 

função de onda, esta também deve mudar descontinuamente, um comportamento que a própria 

equação de Schrödinger não implicava. Esse fenômeno de súbita mudança, chamado de 

colapso do pacote de onda, era uma condição extra que tinha que ser imposta de fora sobre a teoria. Veremos no próximo capítulo que o processo de mensuração continua a originar perplexidades sobre como entender e interpretar a teoria quântica. Em alguém como Schrödinger, a questão evocava mais do que perplexidade. Ela o enchia de aversão, e o físico chegou a dizer que, se soubesse que suas ideias teriam levado a esse “maldito salto quântico”, ele não teria desejado descobrir essa equação!




Observáveis




(Aviso ao leitor: esta seção inclui algumas ideias matemáticas simples que valem o esforço de ser entendidas, mas cuja digestão exigirá alguma concentração. Esta é a única seção do texto principal que arrisca um encontro furtivo com a matemática. Lamento que essa dificuldade seja inevitável para os não matemáticos.)




A física clássica descreve um mundo que é claro e determinado. A física quântica descreve um mundo que é nebuloso e intermitente. Em termos do formalismo (a expressão matemática da teoria), vimos que essas propriedades surgem do fato de que o 

princípio da sobreposição quântica permite a mistura de estados que, na forma clássica, seriam estritamente imiscíveis. Esse simples princípio de aditividade contraintuitiva encontra uma forma natural de expressão matemática em termos dos chamados 

espaços vetoriais [7].




Um vetor no espaço ordinário pode ser imaginado como uma seta, algo de determinado comprimento apontando em determinada direção. As setas podem ser adicionadas simplesmente seguindo uma à outra. Por exemplo, quatro milhas na direção norte, seguida de três milhas na direção leste resulta em cinco milhas em uma direção 37° a leste do norte (veja a Figura 4). 




Os matemáticos podem generalizar essas ideias para espaços com qualquer número de dimensões. A propriedade básica que todos os vetores têm é que podem ser adicionados. Assim, eles representam uma contraparte matemática natural ao 

princípio da sobreposição quântica. Não precisamos nos ocupar dos detalhes aqui, mas, como sempre é bom estar à vontade com a terminologia, vale a pena destacar que uma maneira especialmente sofisticada de espaço vetorial, chamada de 

espaço de Hilbert, oferece o veículo matemático de escolha para a teoria quântica.




Até então, a discussão se concentrou em estados de movimento. Pode-se pensar neles surgindo de modos específicos de preparar o material inicial para um experimento: disparar elétrons de um bombardeador de elétrons; passar 

luz por um determinado sistema óptico; defletir partículas por um conjunto especial de campos elétricos e magnéticos, e assim por diante. Pode-se pensar no estado como sendo “qual é o caso” para o sistema que foi preparado, embora a incapacidade de representação por imagens da teoria quântica signifique que esse assunto não será tão claro e objetivo como o seria na física clássica. Se o físico quiser saber algo com maior precisão (onde exatamente está o elétron?), será necessário fazer uma observação envolvendo uma intervenção experimental no sistema. Por exemplo, o investigador pode querer mensurar alguma quantidade dinâmica em especial, como a posição ou o momentum de um elétron. Surge, então, a questão formal: se o estado é representado por um vetor, como os 

observáveis que podem ser medidos devem ser representados? A resposta se dá em termos de 

operadores que atuam no 

espaço de Hilbert. Assim, o esquema que liga o formalismo matemático à física inclui a especificação de que vetores correspondem a estados e de quais 

operadores correspondem aos 

observáveis [8].






















4. Adição de vetores.













A ideia geral de um operador é de que ele é algo que transforma um estado em outro. Um exemplo simples é dado por 

operadores de rotação. No espaço comum tridimensional, uma rotação de 90° na vertical (no sentido de uma chave de fenda pela direita) transforma um vetor (pense nele como se fosse uma seta) apontando para o leste em um vetor (seta) que aponta para o norte. Uma importante propriedade dos 

operadores é que geralmente não comutam entre si, ou seja, a ordem em que atuam é significativa. Considere dois 

operadores: R

1

, uma rotação de 90° na vertical; R

2

, uma rotação de 90° (novamente pela direita) sobre um eixo horizontal que aponta para o norte. Aplique-os na ordem R

1

 seguido de R

2

 em uma seta apontando para o leste. R

1

 transforma isso em uma seta apontando para o norte, que não é alterada por R

2

. Representamos as duas operações realizadas nessa ordem como o produto R

2

.R

1

, uma vez que os 

operadores, como hebraico e aramaico, são sempre lidos da direita para a esquerda. A aplicação dos 

operadores na ordem contrária primeiro altera a seta voltada para o leste para uma seta que aponta para baixo (efeito de R

2

), que então fica inalterada (efeito de R

1

). Como R

2

.R

1

 acaba ficando com uma seta voltada para o norte e R

1

.R

2

 fica com uma seta apontando para baixo, esses dois produtos são bastante distintos entre si. A ordem importa – as rotações não comutam.




Os matemáticos reconhecerão que as matrizes também podem ser consideradas 

operadores, e a não comutabilidade das matrizes que 

Heisenberg usou é outro exemplo específico dessa propriedade geral dos 

operadores.




Tudo isso parece um tanto abstrato, mas a não comutabilidade se apresenta como a contraparte matemática de uma importante propriedade física. Para ver como isso acontece, é preciso, em primeiro lugar, estabelecer como o formalismo dos 

operadores para 

observáveis está relacionado aos resultados reais de experimentos. Os 

operadores são entidades matemáticas bastante sofisticadas, porém as mensurações são sempre expressas como números não sofisticados, como, por exemplo, 2,7 unidades de qualquer coisa que possa ser. Se a teoria abstrata pretende mostrar o sentido das observações físicas, deve haver uma maneira de associar números (os resultados de observações) com 

operadores (o formalismo matemático). Felizmente, a matemática é capaz de enfrentar esse desafio. As principais ideias são 

autovetores

 e 

autovalores

 [8].

























5. Rotações não comutativas.










Por vezes, um operador que atua sobre um vetor não altera sua direção. Um exemplo seria uma rotação sobre o eixo vertical, que mantém um vetor vertical completamente inalterado. Outro exemplo seria a operação de se alongar no sentido vertical. Isso não mudaria a direção de um vetor vertical, mas alteraria seu comprimento. Se o alongamento tem um efeito de duplicação, o comprimento do vetor vertical é multiplicado por dois. Em linhas gerais, dizemos que, se um operador O transforma um determinado vetor 

v

 em um múltiplo λ de si mesmo, então 

v

 é um autovetor de O com autovalor λ. A ideia básica é que os 

autovalores (λ) oferecem um modo matemático de associar números a um determinado operador (O) e um estado específico (

v

). Os princípios gerais da teoria quântica incluem o ousado requisito de que um autovetor (também chamado de autoestado) corresponderá fisicamente a um estado em que a 

medição da quantidade observável O 

certamente

 dará o resultado λ.




Uma série de consequências significativas decorrem dessa regra. Uma é o oposto, ou seja, como há muitos vetores que não são 

autovetores, haverá muitos estados em que a mensuração de O não dará nenhum resultado específico com certeza. (Matemática à parte, é bastante fácil ver que a sobreposição de dois autoestados de O correspondentes a diferentes 

autovalores resultará em um estado que não pode ser um simples autoestado de O.) Portanto, a mensuração de O em estados desse último tipo resulta em uma variedade de diferentes respostas em diferentes eventos de mensuração. (A conhecida natureza probabilística da teoria quântica manifesta-se novamente.) Qualquer que seja o resultado obtido, o estado resultante deve corresponder a ele; isso equivale a dizer que o vetor deve ser alterado instantaneamente para se tornar o autovetor adequado de O. Essa é a versão sofisticada do 

colapso do pacote de onda.




Outra consequência importante está relacionada a quais mensurações podem ser mutuamente compatíveis, ou seja, feitas ao mesmo tempo. Suponha que seja possível medir O

1

 e O

2

 simultaneamente, com resultados λ

1

 e λ

2

, respectivamente. Fazer isso em uma ordem multiplica o vetor de estado por λ

1

 e depois por λ

2

, enquanto a reversão da ordem das observações simplesmente reverte a ordem em que os λs multiplicam o vetor de estado. Como os λs são apenas números ordinários, essa ordem não importa. Isso implica que O

2

.O

1

 e O

1

.O

2

 atuando sobre o vetor de estado têm efeitos idênticos, de modo que a ordem dos 

operadores não tem relevância. Em outras palavras, mensurações simultâneas só podem ser mutuamente compatíveis para 

observáveis que correspondam a 

operadores que comutem entre si. Ou seja, 

observáveis que não comutam não serão simultaneamente mensuráveis.




Aqui, vemos a nebulosidade característica da teoria quântica se manifestando mais uma vez. Na física clássica, o pesquisador pode medir qualquer coisa que deseje sempre que desejar fazê-lo. O mundo físico está exposto à vista do olho do cientista, que tem o potencial de ver tudo. No mundo quântico, por contraste, a visão do físico está parcialmente encoberta. Nosso acesso ao conhecimento de entidades quânticas é epistemologicamente mais limitado do que a física clássica havia suposto.




Nosso flerte matemático com os 

espaços vetoriais está chegando ao fim. Qualquer leitor que esteja muito confuso deve simplesmente se agarrar ao fato de que, na teoria quântica, apenas 

observáveis cujos 

operadores comutam entre si podem ser medidos simultaneamente.




O 

princípio da incerteza




O que tudo isso significa foi consideravelmente esclarecido por 

Heisenberg em 1927 quando formulou seu famoso 

princípio da incerteza. Ele percebeu que a teoria deveria especificar o que ela permitia ser conhecido por meio de mensuração. A preocupação de 

Heisenberg não era com argumentos matemáticos do tipo que estivemos considerando, mas com “experimentos mentais” idealizados que buscavam explorar o conteúdo físico da 

mecânica quântica. Um desses experimentos mentais envolvia a consideração do que é chamado de microscópio de raios gama.




A ideia é encontrar, em princípio, com que exatidão se pode mensurar a posição e o momentum de um elétron. Segundo as regras da 

mecânica quântica, os 

operadores correspondentes não comutam. Portanto, se a teoria realmente funciona, não deve ser possível saber os valores de posição e o momentum com precisão arbitrária. 

Heisenberg queria entender, em termos físicos, por que isso acontecia. Vamos começar tentando medir a posição do elétron. Em princípio, uma maneira de fazer isso seria irradiar 

luz sobre o elétron e, a seguir, olhar por um microscópio para ver onde ele está. (Lembre-se de que estes são experimentos 

mentais

.) Os instrumentos ópticos têm um poder de resolução limitado, que impõe restrições sobre a exatidão com que os objetos podem ser localizados. Não dá para fazer melhor do que o comprimento de onda da 

luz sendo usado. É claro que uma maneira de aumentar a precisão seria usar comprimentos de onda mais curtos – que é onde entram os raios gama, já que são radiação de frequência muito alta (comprimento de onda curto). Porém, esse estratagema exige um custo, resultante da natureza corpuscular da radiação. Para que o elétron possa ser visto, ele deve defletir pelo menos um 

fóton no microscópio. A fórmula de 

Planck conclui que, quanto maior a frequência, mais energia aquele 

fóton estará carregando. Como consequência, a diminuição do comprimento de onda sujeita o elétron cada vez mais por meio de um distúrbio incontrolável de seu movimento através de sua colisão com o 

fóton. A decorrência disso é que se perde progressivamente o conhecimento de qual será o momentum do elétron após a mensuração de posição. Há uma negociação inescapável entre a crescente exatidão da mensuração de posição e a decrescente precisão de conhecimento sobre o momentum. Esse fato está na base do princípio de incerteza: não é possível ter conhecimento perfeito da posição e do momentum simultaneamente [9]. Em linguagem mais coloquial, pode-se saber onde o elétron está, mas não o que ele está fazendo; ou pode-se saber o que está fazendo sem saber onde ele está. No mundo quântico, o que o físico clássico consideraria conhecimento parcial é o melhor que podemos fazer. 




Esse semiconhecimento é uma característica quântica. Os 

observáveis vêm em pares que epistemologicamente se excluem. Um exemplo desse comportamento encontrado no cotidiano pode ser dado em termos musicais. Não é possível atribuir um instante preciso a quando uma nota foi tocada e saber precisamente qual foi seu tom. Isso ocorre porque a determinação do tom de uma nota requer a análise da frequência do som, e isso exige escutar uma nota por um período que dura diversas oscilações antes de ser possível fazer uma estimativa exata. É a natureza ondulatória do som que impõe essa restrição e, se as questões de mensuração da teoria quântica forem discutidas do ponto de vista da 

mecânica ondulatória, considerações exatamente semelhantes levam de volta ao 

princípio da incerteza.




Há uma interessante história humana por trás da descoberta de 

Heisenberg. Naquela época, ele estava trabalhando no Instituto de Copenhague, cujo diretor era 

Niels 

Bohr. 

Bohr adorava discussões intermináveis, e o jovem 

Heisenberg era um de seus parceiros de conversa favoritos. Na verdade, após algum tempo, as ruminações infinitas de 

Bohr levaram seu colega mais jovem quase à distração. 

Heisenberg ficou contente em aproveitar a oportunidade oferecida pela ausência de 

Bohr, que estava de férias em uma estação de esqui, para continuar seu próprio trabalho e concluir o artigo sobre o 

princípio da incerteza. Depois ele o submeteu às pressas para publicação antes que seu eminente chefe voltasse. Porém, quando 

Bohr retornou, ele detectou um erro que 

Heisenberg cometera. Felizmente, o erro era corrigível, e a correção não afetava o resultado final. Esse pequeno deslize envolvia um erro sobre o poder de resolução de instrumentos ópticos. Acontece que 

Heisenberg já tivera problemas com esse assunto antes. Ele fez sua tese de doutorado orientado por Arnold 

Sommerfeld, um dos principais protagonistas da antiga teoria quântica. Brilhante como teórico, 

Heisenberg não se incomodou muito com o trabalho experimental que também deveria fazer parte de seus estudos. O colega de experimentos de 

Sommerfeld, 

Wilhelm Wien, observara isso. Ele se ressentia da atitude arrogante do jovem e decidiu testá-lo na prova oral: desafiou 

Heisenberg exatamente com uma exigência de derivar o poder de resolução de instrumentos ópticos! Depois da prova, Wien declarou que esse lapso significava que 

Heisenberg seria reprovado. 

Sommerfeld, naturalmente (e com razão), argumentou a favor de uma nota máxima. No final, era preciso chegar a um termo comum, e o futuro ganhador do Prêmio Nobel recebeu seu doutorado, mas com a menor nota possível. 




Amplitudes de 

probabilidade




O modo como as probabilidades são calculadas na teoria quântica ocorre em termos do que se chamam amplitudes de probabilidades. Uma discussão completa seria matematicamente exigente demais, mas há dois aspectos envolvidos dos quais o leitor deve estar ciente. Um deles é que essas amplitudes são números complexos, ou seja, envolvem números ordinários e também 

i

, a raiz quadrada “imaginária” de -1. Na verdade, os números complexos são endêmicos no formalismo da teoria quântica, porque fornecem uma maneira conveniente de representar um aspecto de 

ondas que foi referido no Capítulo 1, durante a discussão de 

fenômenos de interferência. Vimos que a fase de 

ondas depende do fato de que dois conjuntos de 

ondas estejam em fase ou fora de fase (ou qualquer possibilidade intermediária entre essas duas). Matematicamente, os números complexos oferecem um modo conveniente e natural de expressar essas “relações de fase”. No entanto, a teoria precisa tomar cuidado para garantir que os resultados das observações (

autovalores) não sejam contaminados por termos envolvendo 

i

. Isso é feito exigindo-se que os 

operadores correspondentes aos 

observáveis satisfaçam uma determinada condição que os matemáticos chamam de “hermitiana” [8]. 




O segundo aspecto das 

amplitudes de 

probabilidade que precisamos pelo menos mencionar é que, sendo parte do aparato matemático da teoria em discussão, seu cálculo envolve uma combinação de vetores de estado e 

operadores 

observáveis. Como são esses “elementos matriciais” (assim são chamadas essas combinações) que carregam a maior parte da significância física direta, e como eles são formados do que se poderia chamar de “sanduíches” de estado e observável, a dependência do tempo da física pode ser atribuída a uma dependência do tempo presente nos vetores de estado ou a uma dependência do tempo presente nos 

observáveis. Conforme se constatou mais tarde, essa observação oferece a pista para como, apesar de suas aparentes diferenças, as teorias de 

Heisenberg e Schrödinger realmente correspondem à mesma física [10]. Sua aparente desigualdade ocorre porque 

Heisenberg atribui toda a dependência do tempo aos 

operadores, enquanto Schrödinger a atribui inteiramente aos vetores de estado. 




As próprias probabilidades – que, para fazer sentido, devem ser números positivos – são calculadas a partir das amplitudes por um tipo de elevação ao quadrado (chamada de “quadrado do módulo”) que sempre gera um número positivo da amplitude complexa. Há também uma condição de escalonamento (chamada de “normalização”) garantindo que, quando todas as probabilidades são adicionadas, elas devem totalizar 1 (obviamente algo deve acontecer!).




Complementaridade




Durante todo o tempo em que essas descobertas estavam vindo à tona, Copenhague fora o centro onde avaliações eram feitas e vereditos eram dados sobre o que estava acontecendo. Nessa época, 

Niels 

Bohr não estava mais dando contribuições detalhadas a avanços técnicos. Ainda assim, ele permanecia profundamente interessado em questões interpretativas e foi a única pessoa para quem – por sua integridade e seu discernimento – os Jovens Turcos, que estavam de fato escrevendo os artigos pioneiros, enviavam suas descobertas. Copenhague foi a corte do rei-filósofo, para quem as ofertas intelectuais da nova geração da 

mecânica quântica eram trazidas para avaliação e reconhecimento.




Além de seu papel de figura de pai, 

Bohr fez observações perspicazes sobre a nova teoria quântica. Isso assumiu a forma de sua noção de 

complementaridade. A teoria quântica oferecia uma série de modos alternativos de pensamento. Havia as representações alternativas do processo que poderiam ser baseadas na 

medição de todas as posições ou de todos os momenta; a dualidade entre pensar nas entidades em termos de 

ondas ou em termos de partículas. 

Bohr enfatizava que os dois membros desses pares de alternativas deveriam ser considerados com igual seriedade e poderiam, portanto, ser tratados sem contradição porque cada um complementava o outro, em vez de entrar em conflito. Isso acontecia porque eles correspondiam a diferentes – e mutuamente incompatíveis – arranjos experimentais que não podiam ser usados ao mesmo tempo. Ou se projetava um experimento de onda (fendas duplas), em que se fazia uma pergunta ondulatória que receberia uma resposta ondulatória (um padrão de interferência); ou se criava um experimento de partícula (detectar por qual fenda o elétron passou) e, nesse caso, a pergunta corpuscular recebia uma resposta corpuscular (duas áreas de impacto em frente às duas fendas).




É evidente que a 

complementaridade era uma ideia útil, embora de forma alguma resolvesse todos os problemas de interpretação, como será mostrado no próximo capítulo. À medida que 

Bohr ficou mais velho, tornou-se cada vez mais preocupado com questões filosóficas. Sem dúvida, ele era um grande físico, mas parece-me que era nitidamente menos talentoso nessa ocupação tardia. Seus pensamentos eram extensos e nebulosos, e muitos livros foram depois escritos na tentativa de analisá-los, com conclusões que atribuíam a 

Bohr uma variedade de posições filosóficas mutuamente incompatíveis. Talvez ele não tivesse ficado surpreso com isso, pois gostava de dizer que havia 

complementaridade entre ser capaz de dizer algo claramente e o fato de isso ser algo profundo e digno de nota. É certo que a relevância da 

complementaridade para a teoria quântica (onde surge a questão a partir da experiência, e dispomos de um arcabouço teórico geral que a torna inteligível) não garante uma licença para a fácil exportação da noção para outras disciplinas, como se pudesse ser invocada para “justificar” qualquer correspondência paradoxal que alguém imaginasse. Pode-se dizer que 

Bohr chegou perigosamente próximo a isso quando sugeriu que a 

complementaridade poderia lançar 

luz sobre a antiga questão do determinismo e do livre-arbítrio em relação à natureza humana. Vamos adiar uma reflexão filosófica adicional para o capítulo final.




Lógica quântica




Pode-se esperar que a teoria quântica modifique, de modo contundente, as nossas concepções acerca dos termos físicos de posição e momentum. É de todo mais surpreendente que também tenha afetado a maneira como pensamos sobre estas pequenas palavras lógicas: “e” e “ou”.




A lógica clássica, conforme concebida por Aristóteles e por qualquer outra pessoa, baseia-se na lei distributiva da lógica. Se eu lhe disser que Bill tem cabelo ruivo e que ele está em casa ou no bar, você esperará encontrar um Bill ruivo em casa ou um Bill ruivo no bar. Parece ser uma conclusão inofensiva, que formalmente depende da lei aristotélica do terceiro excluído: não há meio-termo entre “em casa” e “não em casa”. Na década de 1930, as pessoas começaram a perceber que as coisas eram diferentes no mundo quântico. Um elétron pode não estar apenas “aqui” e “não aqui”, mas também em qualquer número de outros estados que são sobreposições de “aqui” e “não aqui”. Isso constitui um meio-termo nunca sonhado por Aristóteles. A consequência é que existe uma forma especial de lógica, chamada de 

lógica quântica, cujos detalhes foram elaborados por 

Garret Birkhoff e 

John von Neumann. Ela também é conhecida por lógica de três valores porque, além de “verdadeiro” e “falso”, ainda possibilita a resposta probabilística “talvez”, uma ideia com a qual os filósofos flertaram de modo independente.

Capítulo 3




Perplexidades obscuras




Na época em que a teoria quântica moderna foi descoberta, os problemas físicos que ocupavam o palco principal lidavam com o comportamento de átomos e de radiação. Esse período de descoberta inicial foi seguido, no final da década de 1920 e início da década de 1930, por um longo e exaltado período de pesquisa, conforme as novas ideias eram aplicadas a uma ampla variedade de outros fenômenos físicos. Por exemplo, veremos um pouco mais adiante que a teoria quântica deu um novo e significativo entendimento de como se comportavam os elétrons dentro de sólidos cristalinos. Uma vez ouvi Paul 

Dirac falar sobre esse período de rápido desenvolvimento dizendo que era uma época “em que homens de segunda categoria faziam trabalho de primeira categoria”. Na boca de quase todo mundo, essas palavras teriam sido um comentário humilhante de natureza não muito agradável. Contudo, não era assim com 

Dirac. Durante toda a sua vida, ele manteve um modo simples e prosaico de falar, em que dizia o que pensava com objetividade sem enfeites. Suas palavras simplesmente pretendiam transmitir algo da riqueza do entendimento que fluía daquelas ideias básicas iniciais.




Essa aplicação exitosa das ideias quânticas continuou sem interrupções. Agora usamos a teoria de forma igualmente eficiente para discutir o comportamento de quarks e glúons, uma realização impressionante quando lembramos que esses constituintes de matéria nuclear são, no mínimo, 100 milhões de vezes menores do que os átomos que interessavam aos pioneiros na década de 1920. Os físicos sabem fazer os cálculos e constatam que as respostas continuam a ser corretas com exatidão surpreendente.




Por exemplo, a 

eletrodinâmica quântica (a teoria da interação de elétrons com fótons) gera resultados que estão em conformidade com experimentos com uma precisão correspondente a um erro menor do que a largura de um fio de cabelo em relação à distância entre Los Angeles e Nova York!




Considerada nesses termos, a história quântica é um tremendo conto de sucesso, talvez a maior história de sucesso na história da ciência física. Ainda assim, continua sendo um paradoxo profundo. Apesar da capacidade dos físicos de fazer os cálculos, eles ainda não entendem a teoria. Sérios problemas de interpretação permanecem sem solução, sendo motivo de contínua controvérsia. Essas questões litigiosas referem-se a duas perplexidades em especial: a significância do caráter probabilístico da teoria e a natureza do processo de mensuração.




Probabilidades




As probabilidades também ocorrem na física clássica, cuja origem está na ignorância de alguns detalhes sobre o que está acontecendo. O exemplo paradigmático é uma moeda atirada para cima. Ninguém duvida que a mecânica newtoniana determine como ela deve cair depois de jogada – não há debate sobre a intervenção direta de Fortuna, a deusa da sorte –, mas o movimento é sensível demais ao detalhe preciso e minúsculo de como a moeda foi jogada (não estamos cientes desses detalhes) para que consigamos prever exatamente qual será o resultado. Porém, sabemos que, se a moeda for imparcial, as chances são iguais: 

1

/

2

 para cara e 

1

/

2

 para coroa. Da mesma forma, para um dado de verdade, a 

probabilidade de dar qualquer número é de 

1

/

6

. Se alguém perguntar sobre a 

probabilidade de sair o número um ou dois, simplesmente somamos as probabilidades separadas, o que resulta em 

1

/

3

. Essa regra da adição se mantém porque os processos de lançar o dado que levam a um ou dois são distintos e independentes entre si. Como não têm nenhuma influência um sobre o outro, basta adicionar as possibilidades resultantes. Tudo parece bastante objetivo. Ainda assim, as coisas no mundo quântico são diferentes. 




Primeiro, considere qual seria o equivalente clássico do experimento quântico com elétrons e fendas duplas. Uma analogia do cotidiano seria arremessar bolas de tênis em uma cerca com dois buracos. Haverá uma determinada 

probabilidade de uma bola passar por um buraco e outra 

probabilidade de que ela passe pelo outro buraco. Se estamos interessados na possibilidade de que a bola caia no outro lado da cerca, visto que precisa passar por um ou outro buraco, simplesmente somamos essas duas probabilidades (assim como fizemos para os dois lados do dado). No caso quântico, as coisas são diferentes em função do 

princípio da sobreposição, que permite que o elétron passe por ambas as fendas. O que classicamente eram possibilidades mutuamente distintas estão entrelaçadas na 

mecânica quântica.




Por consequência, as leis para combinar probabilidades são diferentes na teoria quântica. Se for preciso somar uma série de possibilidades intermediárias não observadas, são as 

amplitudes de 

probabilidade

 que devem ser adicionadas, e não as próprias probabilidades. No 

experimento da fenda dupla, devemos adicionar a amplitude para (passando por A) à amplitude para (passando por B). Lembre que as probabilidades são calculadas a partir de amplitudes por um tipo de processo de elevação ao quadrado. O efeito de adicionar antes de elevar ao quadrado é produzir o que um matemático chamaria de “termos cruzados”. Pode-se sentir o sabor dessa ideia considerando a simples equação aritmética




(2 + 3)

2

 = 2

2

 + 3

2

 + 12




Aquele 12 “extra” é o termo cruzado.




Talvez isso pareça um pouco misterioso. A noção básica é a seguinte: no mundo cotidiano, para ter a 

probabilidade de um resultado final, simplesmente adicionamos as probabilidades de possibilidades intermediárias independentes. No mundo quântico, a combinação de possibilidades intermediárias que não são diretamente observadas acontece do modo mais sutil e sofisticado. É por isso que o cálculo quântico envolve termos cruzados. Uma vez que as 

amplitudes de 

probabilidade são, de fato, números complexos, esses termos cruzados incluem efeitos de fase, de sorte que pode haver interferência construtiva ou destrutiva, como ocorre no 

experimento da fenda dupla.




Resumindo a questão em poucas palavras, as probabilidades clássicas correspondem à ignorância e se combinam por adição simples. As probabilidades quânticas se combinam de uma maneira aparentemente mais evasiva e incapaz de ser representada por imagens. Surge, então, a seguinte questão: apesar disso, seria possível compreender as probabilidades quânticas como tendo, também, sua origem na ignorância do físico em relação a todos os detalhes do que está acontecendo, de modo que as probabilidades básicas subjacentes – correspondentes a conhecimento inacessível, mas completamente detalhado sobre qual era a situação – ainda fizessem sentido na forma clássica?




Por trás dessa indagação encontra-se um desejo ansioso da parte de alguns em restaurar o determinismo na física, mesmo que seja para comprovar um tipo velado de determinismo. Considere, por exemplo, o decaimento de um núcleo radioativo (um que seja instável e propenso a se fragmentar). Tudo o que a teoria quântica pode prever é a 

probabilidade de que tal decaimento ocorra. Por exemplo, ela pode dizer que determinado núcleo tem uma 

probabilidade de ½ de decaimento na próxima hora, mas não pode prever se esse núcleo específico irá, de fato, decair durante aquela hora. Ainda assim, talvez esse núcleo tenha um pequeno relógio interno que especifique precisamente quando ele decairá, mas que não podemos ler. Se esse fosse o caso, e se outros núcleos do mesmo tipo tivessem seus próprios relógios internos cujas diferentes configurações fizessem eles decaírem em horários distintos, então o que atribuímos como probabilidades surgiria unicamente da nossa ignorância, da nossa incapacidade de obter acesso às configurações daqueles relógios internos escondidos. Embora os decaimentos possam parecer aleatórios, eles seriam inteiramente determinados por esses detalhes desconhecidos. Na realidade suprema, a 

probabilidade quântica não seria diferente da 

probabilidade clássica. Interpretações da 

mecânica quântica desse tipo são chamadas de teoria das 

variáveis ocultas

. Será que são, de fato, uma possibilidade?




O célebre matemático 

John von Neumann acreditava ter demonstrado que as propriedades incomuns das probabilidades quânticas implicavam que nunca poderiam ser interpretadas como a consequência de ignorância de 

variáveis ocultas. Na verdade, havia um erro em seu argumento que levou anos para ser detectado. Veremos mais tarde que é possível haver uma interpretação determinista da teoria quântica em que as probabilidades surgem da ignorância dos detalhes. No entanto, veremos também que a teoria que obtém êxito dessa forma tem outras propriedades que a fizeram parecer sem atrativos para a maioria dos físicos.




 Descoerência




Um aspecto dos problemas que estamos considerando neste capítulo pode ser expresso perguntando-se como é possível que os constituintes quânticos do mundo físico, como quarks, glúons e elétrons, cujo comportamento é nebuloso e intermitente, possam dar origem ao mundo macroscópico da experiência cotidiana, que parece tão claro e confiável. Um passo importante para obter alguma compreensão dessa transição foi feito através de um desenvolvimento que ocorreu nos últimos 25 anos. Os físicos acabaram percebendo que, em muitos casos, é importante levar em consideração – mais seriamente do que o fizeram antes – o ambiente no qual os processos quânticos estão de fato acontecendo.




O raciocínio convencional havia considerado que o ambiente era vazio, à exceção das entidades quânticas cujas interações eram o assunto de reflexão explícita. Na realidade, essa idealização nem sempre funciona e, quando não funciona, importantes consequências podem decorrer desse fato. O que foi negligenciado era a presença quase onipresente da radiação. Experimentos acontecem em um mar repleto de fótons, alguns vindos do Sol e outros vindos da universal radiação cósmica de fundo que é um eco prolongado do tempo em que o universo tinha meio milhão de anos e havia recém se tornado frio o bastante para que a matéria e a radiação se separassem de sua prévia mistura universal.




Ocorre que a consequência dessa radiação de fundo praticamente onipresente é afetar as fases das 

amplitudes de 

probabilidade relevantes. Levar em consideração essa chamada “randomização de fases” pode, em certos casos, ter o efeito de eliminar quase completamente os termos cruzados nos cálculos de 

probabilidade quântica. (

Grosso modo

, a média é de aproximadamente tantos sinais de mais quanto de menos, dando um resultado próximo a zero.) Tudo isso pode ocorrer com uma rapidez bastante surpreendente. Tal fenômeno é chamado de “

descoerência”.




A 

descoerência foi saudada por alguns como a pista para entender como os fenômenos quânticos microscópicos e os fenômenos clássicos macroscópicos estão relacionados entre si. Infelizmente, trata-se de apenas uma meia-verdade. Ela pode servir para fazer algumas probabilidades quânticas parecerem como probabilidades clássicas, mas não as torna iguais. Ainda permanece a perplexidade central do que é chamado de “problema da 

medição”.




O problema da 

medição




Na física clássica, a 

medição não é problemática. Ela é simplesmente a observação do que acontece. De antemão, podemos conseguir não mais do que atribuir uma 

probabilidade de ½ de que a moeda dê cara; porém, se é isso o que vemos, é unicamente porque é o que, de fato, aconteceu.




A 

medição na teoria quântica convencional é diferente porque o princípio da superposição admite possibilidades alternativas – e às vezes mutuamente excludentes – até o último momento, quando subitamente uma delas emerge como a realidade percebida nessa ocasião. Vimos que uma maneira de pensar nisso pode ser expressa em termos do 

colapso do pacote de onda. A 

probabilidade do elétron estava estendida entre “aqui”, “lá” e “em todo lugar”. No entanto, quando o físico faz a ele a pergunta experimental “onde você está?” e, nessa determinada ocasião, a resposta “aqui” aparece, então toda a 

probabilidade entra em colapso por essa realidade única. A grande questão que permanece sem resposta em nossa discussão até então é: como isso ocorre?




As medições são uma cadeia de consequências relacionadas pela qual o estado das coisas no mundo quântico microscópico produz um sinal correspondente observável no mundo cotidiano dos instrumentos de 

medição de laboratório. Podemos esclarecer essa questão considerando um experimento um tanto idealizado, mas nada ilusório, que mede o 

spin

 do elétron. A propriedade do 

spin

 corresponde ao comportamento dos elétrons como se fossem ímãs minúsculos. Em razão de um efeito quântico impossível de representar por imagens que o leitor simplesmente terá que aceitar cegamente, o ímã do elétron só pode apontar para dois sentidos opostos, que podemos convencionalmente chamar de “para cima” e “para baixo”.




O experimento é conduzido com um feixe de elétrons inicialmente despolarizados, ou seja, os elétrons estão em um estado que é uma sobreposição uniforme de “para cima” e “para baixo”. Então se faz com que esses elétrons passem por um campo magnético não homogêneo. Por causa do efeito magnético de seu 

spin

, eles serão defletidos para cima ou para baixo, de acordo com o sentido do 

spin

. A seguir, eles passam por um dos dois detectores adequadamente posicionados, D

u

 ou D

d

 (contadores Geiger, talvez), e o pesquisador ouvirá um clique em um dos detectores, registrando a passagem de um elétron no sentido para cima ou para baixo. Esse procedimento é chamado de 

experimento de 

Stern-Gerlach, em homenagem aos dois físicos alemães que conduziram pela primeira vez uma investigação desse tipo. (Na verdade, ele foi feito com um feixe atômico, mas foram os elétrons nos átomos que controlaram o que estava acontecendo.) E como devemos analisar o que está acontecendo?




Se o 

spin

 estiver para cima, o elétron é defletido para cima, depois passa por D

u

, que faz um clique e o pesquisador ouve D

u

 clicar. Se o 

spin

 estiver para baixo, o elétron é defletido para baixo, depois passa por D

d

, que faz um clique e o pesquisador ouve isso acontecer. Pode-se ver o que está ocorrendo nessa análise. Ele se apresenta com uma cadeia de consequências correlacionadas: se…, então…, então... Todavia, em uma ocasião real de 

medição, só uma dessas cadeias ocorre. O que faz esse determinado acontecimento ocorrer nesta ocasião em especial? O que determina que, desta vez, a resposta será “para cima”, e não “para baixo”?




A 

descoerência não responde essa pergunta para nós. O que ela faz é apertar os elos nas cadeias separadas, tornando-a mais do tipo clássico, mas não explica por que uma determinada cadeia é a possibilidade percebida em uma ocasião em especial. A essência do problema da 

medição é a busca para entender a origem dessa especificidade. Pesquisaremos a variedade de respostas que foram sugeridas, mas veremos que nenhuma delas é inteiramente satisfatória ou desprovida de perplexidade. As propostas podem ser classificadas de acordo com uma série de tópicos.



















6. Um 

experimento de 

Stern-Gerlach.










1. Irrelevância




Alguns intérpretes tentam usar estratagemas para solucionar o problema, alegando que é irrelevante. Um argumento a favor dessa posição é a declaração positivista de que a ciência deve apenas correlacionar fenômenos e não aspirar a compreendê-los. Se soubermos como fazer cálculos quânticos, e se as respostas se correlacionarem de modo altamente satisfatório com a experiência empírica, como de fato ocorre, então isso é tudo o que devemos almejar. Querer mais do que isso é simplesmente uma ganância inadequada do ponto de vista intelectual. Uma forma mais refinada do 

positivismo é representada pelo que se chama de a abordagem das “histórias consistentes”, que estabelece prescrições para obter sequências de previsões quânticas que sejam prontamente interpretáveis como resultados oriundos do uso de aparelhos clássicos de 

medição.




Um tipo diferente de argumento, que também se enquadra na rubrica de irrelevância, é a alegação de que a física quântica não deveria absolutamente buscar discutir eventos individuais, mas ter como preocupação própria os “agrupamentos”, ou seja, as propriedades estatísticas das coleções de eventos. Se esse fosse o caso, um relato puramente probabilístico é tudo o que teríamos o direito de esperar.




Um terceiro tipo de argumento nessa categoria geral afirma que a 

função de onda não tem a ver com estados de sistemas físicos, mas com estados do conhecimento humano de tais sistemas. Se nosso raciocínio é unicamente epistemológico, então “colapso” não é um fenômeno problemático: antes eu era ignorante; agora eu sei. Porém, parece muito estranho que a representação do que se alega estar tudo na mente deva, de fato, satisfazer uma equação com aparência física como a 

equação de Schrödinger.




Todos esses argumentos apresentam uma característica em comum: eles têm uma visão minimalista da tarefa da física. Em especial, supõem que ela não se preocupa em obter entendimento da natureza detalhada de determinados processos físicos. Esta pode ser uma visão agradável para os que têm certo tipo de disposição filosófica, mas é abominável para a mente do cientista, cuja ambição é obter o maior grau atingível de compreensão do que está acontecendo no mundo físico. Contentar-se com menos seria considerado alta traição.




2. Grandes sistemas




Os pais fundadores da 

mecânica quântica naturalmente estavam conscientes dos problemas que a 

medição apresentava para a teoria. 

Niels 

Bohr, em particular, ficou muito preocupado com essa questão. A resposta que ele propôs veio a ser conhecida como a 

interpretação de Copenhague

. A ideia principal era de que uma função exclusiva estava sendo exercida pelos aparelhos clássicos de 

medição. 

Bohr sustentava que era a intervenção desses grandes instrumentos de 

medição que produziam o efeito determinante.




Mesmo antes de a questão de 

medição vir à tona, fora necessário ter algum meio de ver como se poderia recuperar da teoria quântica os sucessos consideráveis da mecânica clássica para descrever os processos que ocorrem em uma escala diária. Não faria sentido descrever o microscópico à custa de perder um entendimento do macroscópico. Esse requisito, chamado de 

princípio de correspondência

, era algo como poder ver que “grandes” sistemas (a escala de grandeza era determinada pela 

constante de 

Planck) deveriam comportar-se de modo que pudessem ser aproximados de maneira excelente pelas equações de 

Newton. Mais tarde, as pessoas perceberam que a relação entre a 

mecânica quântica e a mecânica clássica era muito mais sutil do que essa simples imagem indicava. Posteriormente, veremos que há alguns fenômenos macroscópicos que exibem certas propriedades intrinsecamente quânticas, incluindo até mesmo a possibilidade de investigação tecnológica, como na 

computação quântica. Porém, tais fenômenos surgem em circunstâncias um tanto excepcionais, e o fluxo geral do princípio da correspondência estava na direção certa.




Bohr enfatizava que uma 

medição envolvia a entidade quântica e o instrumento clássico de 

medição, insistindo que deveríamos pensar no compromisso mútuo dos dois como um conjunto único de fatores (que ele chamava de “fenômeno”). Exatamente onde na cadeia de consequências correlacionadas que levam de uma extremidade à outra se localizava a particularidade de um resultado específico era uma questão que poderia ser evitada, contanto que as duas extremidades da cadeia fossem mantidas inseparavelmente conectadas.




À primeira vista, há algo atraente nessa proposta. Se você entrar em um laboratório de física, verá que ele está repleto dos tipos de instrumentos dos quais 

Bohr falava. Ainda assim, também há algo suspeito na proposta. Sua representação tem um tom dualista, como se a população do mundo físico fosse composta de duas classes diferentes de seres: entidades quânticas intermitentes e instrumentos clássicos de 

medição para determinação. Na verdade, porém, existe um único e monístico mundo físico. Aquelas partes de instrumentos clássicos são, elas próprias, compostas de constituintes quânticos (em última instância, quarks, glúons e elétrons). A interpretação original de Copenhague não conseguiu resolver o problema de como instrumentos de determinação poderiam emergir de um substrato quântico indeterminado.




Apesar disso, pode ser que 

Bohr e seus colegas estivessem acenando na direção certa, mesmo que ainda não o fizessem de modo enérgico o bastante. Hoje, penso que a maioria dos físicos quânticos praticantes endossaria o que se poderia chamar de uma interpretação neo-Copenhague. Nessa perspectiva, a grandeza e a complexidade de instrumentos macroscópicos é o que, de certa forma, os habilitam a desempenhar o papel de determinação. Como isso acontece não é entendido de maneira adequada, mas pelo menos é possível fazer uma correlação com outra propriedade (também não compreendida por completo) dos sistemas grandes. Trata-se de sua 

irreversibilidade

.




Com uma exceção que realmente não é significativa para a atual discussão, as leis fundamentais da física são reversíveis. Para ver o que isso significa, suponha, ao contrário de 

Heisenberg, que fosse possível fazer um filme de dois elétrons em interação. Esse filme faria o mesmo sentido se fosse exibido para frente ou para trás. Em outras palavras, no micromundo, não existe seta intrínseca de tempo, distinguindo o futuro do passado. No macromundo, obviamente, as coisas são muito diferentes. Os sistemas esgotam-se e o mundo cotidiano é irreversível. Um filme de uma bola quicando em que a bola sobe cada vez mais alto está sendo exibido de trás para frente. Esses efeitos estão relacionados à segunda lei da termodinâmica, que afirma que, em um sistema isolado, a entropia (a medida da desordem) nunca diminui. A razão pela qual isso acontece é que há muito mais maneiras de ser desordenado do que ordenado, de sorte que a desordem ganha facilmente. Simplesmente pense na sua mesa de trabalho caso você não intervenha de tempos em tempos para organizá-la.




Medição é o registro irreversível de um sinal macroscópico do estado das coisas no micromundo. Portanto, ela incorpora uma direção intrínseca de tempo: antes, não havia resultado; depois, existe um. Assim, há alguma plausibilidade em supor que um entendimento adequado de sistemas grandes e complexos que explicassem totalmente sua 

irreversibilidade também poderia proporcionar uma pista valiosa quanto à natureza da função que exercem na 

medição quântica. Contudo, no estado atual de conhecimento, isso continua sendo uma humilde aspiração, em vez de uma realização de fato.




3. Nova física




Alguns consideram que a solução do problema de 

medição exigirá um raciocínio mais radical do que apenas promover ainda mais princípios já conhecidos da ciência. Ghirardi, Rimmer e Weber fizeram uma sugestão particularmente interessante seguindo essas linhas inovadoras (que veio a ser conhecida como a teoria 

GRW). Eles propõem que existe uma propriedade universal de colapso de 

função de onda aleatório, mas que a taxa em que isso ocorre depende da quantidade de matéria presente. Para entidades quânticas independentes, essa taxa é minúscula demais para ter qualquer efeito observável; porém, na presença de quantidades macroscópicas de matéria (por exemplo, em um instrumento clássico de 

medição), ela se torna tão rápida que é praticamente instantânea.




Essa é uma sugestão que, em princípio, estaria aberta à investigação por meio de experimentos sensíveis para detectar outras manifestações dessa propensão ao colapso. No entanto, na ausência de tal confirmação empírica, a maioria dos físicos considera a teoria 

GRW 

ad hoc

 demais para ser persuasiva.




4. Consciência




Na análise do 

experimento de 

Stern-Gerlach, a última ligação na cadeia correlacionada era um observador humano ouvindo o clique do contador. Toda 

medição quântica de cujo resultado temos conhecimento teve, como seu último passo, a percepção consciente de alguém acerca do resultado. A 

consciência é a experiência mal compreendida, mas inegável (exceto por determinados filósofos) da interface entre o material e o mental. Os efeitos de drogas ou de dano cerebral confirmam que o material pode atuar sobre o mental. Por que não deveríamos esperar um poder recíproco do mental agindo sobre o material? Algo desse tipo parece acontecer quando executamos a intenção proposital de levantar um braço. Talvez, então, seja a intervenção de um observador consciente que determine o resultado de uma 

medição. À primeira vista, a proposta é algo atraente, e vários físicos renomados endossaram esse ponto de vista. Apesar disso, ela também apresenta algumas dificuldades bastante graves.




Na maioria das vezes e dos lugares, o universo foi desprovido de 

consciência. Devemos supor que, através dessas vastas extensões de espaço e tempo cósmicos, nenhum processo quântico resultou em uma consequência determinada? Suponha que alguém projetasse um experimento computadorizado em que o resultado é impresso em uma folha de papel, que depois é automaticamente armazenada sem que nenhum observador a veja antes de seis meses. Seria o caso de, apenas nesse tempo subsequente, haver uma impressão definitiva no papel?




Essas conclusões não são de todo impossíveis, mas muitos cientistas não as consideram nem um pouco plausíveis. As dificuldades se intensificam ainda mais se considerarmos a triste história do gato de Schrödinger. O infeliz animal está preso em uma caixa que também contém uma fonte radioativa com uma 

probabilidade de 50% de decair na próxima hora. Se ocorrer o decaimento, a radiação emitida acionará a liberação de gás venenoso que matará instantaneamente o gato. A aplicação dos princípios convencionais da teoria quântica à caixa e a seus conteúdos leva à conclusão de que, ao término de uma hora, antes que um observador consciente levante a tampa da caixa, o gato está em uma sobreposição equilibrada de “vivo” e “morto”. Somente depois que a caixa for aberta haverá um colapso de possibilidades, resultando na descoberta de um cadáver definitivamente imóvel ou de um felino definitivamente saltitante. Mas com certeza o animal sabe se está vivo ou não, sem exigir intervenção humana para ajudá-lo a chegar a essa conclusão? Talvez devêssemos concluir, portanto, que a 

consciência do gato é tão eficiente na determinação de resultados quânticos quanto a 

consciência humana. Então, onde vamos parar? As minhocas também podem colapsar a 

função de onda? Elas podem não ser exatamente conscientes, mas tenderíamos a supor que, de alguma forma, as minhocas têm a propriedade definitiva de estar vivas ou mortas. Esses tipos de dificuldades preveniram a maioria dos físicos de acreditar que hipotetizar uma função exclusiva para a 

consciência é a maneira de solucionar o problema da 

medição.




5. Muitos mundos




Uma proposta ainda mais audaciosa rejeita totalmente a ideia do colapso. Seus proponentes afirmam que o formalismo quântico deve ser tomado com maior seriedade do que impor sobre ele, vinda de fora, a hipótese inteiramente 

ad hoc

 da mudança descontínua da 

função de onda. Em vez disso, deve-se reconhecer que tudo o que pode acontecer 

de fato

 acontece.




Por que, então, os pesquisadores têm a impressão contrária, constatando que, em determinada ocasião, o elétron está “aqui” e em nenhum outro lugar? A resposta dada é que essa é a visão estreitamente paroquial de um observador neste universo, mas a realidade quântica é muito maior do que uma imagem tão limitada sugere. Além de existir um mundo em que o gato de Schrödinger vive, há também um mundo paralelo, mas desconectado, em que o gato de Schrödinger morre. Ou seja, em todos os atos de 

medição, a realidade física divide-se em uma multiplicidade de universos separados, em cada um dos quais pesquisadores diferentes (clonados) observam diferentes resultados possíveis da 

medição. A realidade é um multiverso, em vez de um simples universo.




Como as medições quânticas estão acontecendo o tempo todo, essa é uma proposta de prodigalidade ontológica impressionante. O pobre William de Occam (cuja “navalha” lógica pretende eliminar pressuposições desnecessariamente férteis) deve estar se revirando no túmulo com uma noção como essa da multiplicação de entidades. Uma maneira diferente de conceber essa proliferação imensa além da imaginação é localizá-la não como ocorrendo externamente ao cosmos, mas internamente aos estados de mente/cérebro dos observadores. Tal mudança representa uma troca de uma interpretação de muitos mundos para uma interpretação de muitas mentes, mas isso mal serve para mitigar a prodigalidade da proposta.




A princípio, os únicos físicos atraídos por esse modo de raciocinar eram os cosmologistas quânticos, que buscavam aplicar a teoria quântica ao próprio universo. Enquanto ainda estamos perplexos com a forma como o microscópico e o macroscópico se inter-relacionam, essa extensão na direção do cósmico é uma jogada audaciosa cuja viabilidade não é necessariamente evidente. Porém, se é para ser feita, a abordagem dos muitos mundos pode parecer a única opção de uso, pois, quando o cosmos está envolvido, não sobra espaço para o apelo científico aos efeitos de grandes sistemas externos ou da 

consciência. Ultimamente, parece ter havido um grau de inclinação cada vez maior entre outros físicos no sentido de adotar a abordagem dos muitos mundos, embora para muitos de nós ela ainda continue sendo um martelo-pilão a vapor metafísico utilizado para quebrar uma noz quântica reconhecidamente dura.




6. Determinismo




Em 1954, David 

Bohm publicou um relato da teoria quântica que era totalmente determinista, mas que resultou exatamente nas mesmas previsões feitas pela 

mecânica quântica convencional. Nessa teoria, as probabilidades surgem simplesmente da ignorância de certos detalhes. Essa incrível descoberta levou 

John Bell a reexaminar o argumento de Von Neumann de que isso era impossível e exibir a pressuposição imperfeita sobre a qual essa conclusão equivocada estivera baseada.




Bohm atingiu esse feito impressionante separando onda e partícula, que o pensamento de Copenhague havia unido em 

complementaridade indissolúvel. Na teoria de 

Bohm, existem partículas que são tão clássicas e desprovidas de problemas como até o próprio Isaac 

Newton gostaria que tivessem sido. Quando suas posições ou momenta são medidos, é apenas uma questão de observar o que ocorre sem ambiguidades. Além das partículas, no entanto, há uma onda completamente separada, cuja forma a qualquer instante contém informações sobre todo o ambiente. Essa onda não é diretamente discernível, mas tem consequências empíricas, pois influencia o movimento das partículas de uma maneira que é adicional aos efeitos das forças convencionais que também podem agir sobre elas. É essa influência da onda oculta (por vezes referida como a “

onda guia” ou a fonte do “potencial quântico”) que afeta sensivelmente as partículas e consegue produzir tanto a aparência de efeitos de interferência quanto as probabilidades características associadas a eles. Esses efeitos da 

onda guia são estritamente deterministas. Embora as consequências sejam bastante previsíveis, elas dependem muito sensivelmente dos detalhes finos das posições reais das partículas, e é essa sensibilidade a variações minúsculas que produz a aparência de aleatoriedade. Assim, são as posições das partículas que agem como as 

variáveis ocultas na teoria 

bohmiana.




Para entender melhor a teoria de 

Bohm, é esclarecedor questionar como ela lida com o 

experimento da fenda dupla. Devido à natureza das partículas, passíveis de serem representadas por uma imagem, nessa teoria o elétron deve definitivamente passar por uma das fendas. O que, então, havia de errado com o nosso argumento anterior de que isso não poderia ocorrer? O que possibilita contornar aquela conclusão anterior é o efeito da onda oculta. Sem sua existência independente e sua influência, seria de fato verdadeiro que, se o elétron passasse pela fenda A, a fenda B seria irrelevante e poderia estar aberta ou fechada. Mas a onda de 

Bohm contém informações instantâneas sobre o ambiente total, e então sua forma é diferente se B estiver fechada comparada a quando B está aberta. Essa diferença produz consequências importantes para o modo como a 

onda guia as partículas. Se B estiver fechada, a maioria delas é direcionada para o ponto oposto A; se B estiver aberta, a maioria delas é direcionada para o ponto médio da tela de detecção.




Podemos supor que uma versão determinada e representável por imagens da teoria quântica seria mais atraente para os físicos. Na realidade, poucos deles simpatizaram com as ideias bohmianas. A teoria é seguramente instrutiva e engenhosa, mas muitos acreditam ser engenhosa demais. Há nela uma atmosfera de maquinação que faz com que deixe de ser atraente. Por exemplo, a onda oculta precisa satisfazer uma equação de onda. De onde vem essa equação? A resposta sincera está fora do ar agora ou, mais precisamente, fora da mente de Schrödinger. Para obter os resultados certos, a equação de onda de 

Bohm deve ser a 

equação de Schrödinger, mas isso não segue nenhuma lógica interna da teoria e é simplesmente uma estratégia 

ad hoc

 elaborada para produzir respostas empiricamente aceitáveis.




Também há certas dificuldades técnicas que tornam a teoria menos do que totalmente satisfatória. Uma das mais desafiadoras dessas dificuldades tem a ver com as propriedades probabilísticas. Devo admitir que, por uma questão de simplicidade, até então não as enunciei de forma correta. O que é exatamente verdadeiro é que as probabilidades 

iniciais

 relacionadas às disposições de partículas coincidem com aquelas que a teoria quântica convencional prescreveria, então essa coincidência entre as duas teorias será mantida para todo o movimento subsequente. Porém, é preciso começar da maneira certa. Em outras palavras, o sucesso empírico da teoria de 

Bohm exige que o universo tenha calhado de iniciar com as probabilidades (quânticas) corretas inseridas ou, caso isso não tenha acontecido, algum processo de convergência rapidamente o conduziu nessa direção. Essa possibilidade não é inconcebível (um físico a chamaria de “atenuação” das probabilidades quânticas), mas ainda não foi demonstrada, nem sua escala do tempo foi estimada de modo confiável.




O problema da 

medição continua a nos causar ansiedade à medida que contemplamos a desconcertante variedade de propostas apenas – no melhor dos casos – parcialmente persuasivas que foram feitas para a sua solução. As opções utilizadas incluem descaso (irrelevância); física conhecida (

descoerência); física esperada (sistemas grandes); nova física desconhecida (

GRW); nova física oculta (

Bohm); conjectura 

metafísica (

consciência; muitos mundos). Trata-se de uma história intricada que é constrangedora se contada por um físico, dado o papel central que a 

medição tem no raciocínio físico. Para ser franco, ainda não temos uma compreensão intelectual da teoria quântica tão plena quanto gostaríamos. Conseguimos fazer os cálculos e, nesse sentido, explicar os fenômenos, mas não 

entendemos

 realmente o que está ocorrendo. Para 

Bohr, a 

mecânica quântica é indeterminada; para 

Bohm, é determinada. Para 

Bohr, o 

princípio da incerteza de 

Heisenberg é um princípio ontológico de indeterminação; para 

Bohm, é um princípio epistemológico de ignorância. Voltaremos a algumas dessas questões metafísicas e interpretativas no capítulo final. Enquanto isso, uma questão ainda mais especulativa nos aguarda.




Existem estados preferidos?




No século XIX, matemáticos como Sir William Rowan 

Hamilton desenvolveram compreensões bastante gerais da natureza dos sistemas dinâmicos newtonianos. Uma característica dos resultados dessas pesquisas foi estabelecer que há muitas maneiras equivalentes por meio das quais a discussão pode ser formulada. Em geral, é conveniente para os propósitos do raciocínio físico dar um papel de preferência para representar os processos explicitamente como ocorrendo no espaço, mas isso não é, absolutamente, uma necessidade essencial. Quando 

Dirac desenvolveu os princípios gerais da teoria quântica, essa igualdade democrática entre diferentes pontos de vista foi mantida na nova dinâmica resultante. Todos os 

observáveis, e seus autoestados correspondentes, tiveram o mesmo prestígio no que toca à teoria fundamental. Os físicos expressam essa convicção dizendo que não há uma “base preferida” (um conjunto especial de estados, correspondendo a um conjunto especial de 

observáveis, que sejam de significância única).




O combate ao problema da 

medição suscitou na mente de alguns a questão sobre a manutenção ou não desse princípio de não preferência. Entre as diversas propostas lançadas à mesa, há o atributo de que a maioria delas parece atribuir uma função especial a determinados estados, seja como estados finais de colapso, seja como estados que garantem a ilusão perspectiva de colapso: em uma discussão (neo-)Copenhague centrada no instrumento de 

medição, a posição espacial parece desempenhar uma função especial quando se fala de ponteiros em balanças ou marcas em placas fotográficas; igualmente, na interpretação dos muitos mundos, são esses mesmos estados que formam a base da divisão entre os mundos paralelos; na interpretação da 

consciência, presume-se que sejam os estados cerebrais correspondentes a essas percepções que sejam a base preferida da interface entre matéria e mente; a proposta 

GRW postula o colapso em estados de posição espacial; a teoria de 

Bohm atribui um papel especial às posições das partículas, detalhes minúsculos delas sendo as 

variáveis ocultas efetivas da teoria. Também é preciso observar que a 

descoerência é um fenômeno que ocorre no espaço. Se essas forem, de fato, indicações da necessidade de revisar o pensamento democrático anterior, a 

mecânica quântica demonstraria exercer ainda mais influência revisória a ser aplicada na física.

Capítulo 4




Desenvolvimentos adicionais




O período agitado da descoberta quântica essencial em meados do século XX foi seguido de um longo período de desenvolvimento em que as implicações da nova teoria foram investigadas e exploradas. Devemos agora atentar para algumas das constatações resultantes de tais desenvolvimentos adicionais.




Tunelamento




Relações de incerteza do tipo 

Heisenberg não se aplicam apenas a posições e momenta. Elas também podem ser aplicadas ao tempo e à energia. Embora a energia seja, amplamente falando, uma quantidade conservada em teoria quântica – assim como o é na teoria clássica –, isso só é verdadeiro até o ponto da incerteza relevante. Em outras palavras, existe a possibilidade na 

mecânica quântica de “tomar emprestada” alguma energia extra, contanto que seja devolvida com a presteza adequada. Essa modalidade um tanto pitoresca de argumento (que pode ficar mais precisa e mais convincente com cálculos detalhados) permite a ocorrência de algumas coisas de modo mecanicamente quântico, o que seria energeticamente proibido na física clássica. O exemplo mais antigo de um processo desse tipo a ser reconhecido relacionava a possibilidade de 

tunelamento através de uma barreira de potencial.




A situação prototípica está esboçada na Figura 7, na qual a “colina” quadrada representa uma região em que, para entrar, é preciso o pagamento de uma tarifa de energia (chamada de energia potencial) igual à altura da colina. Uma partícula em movimento carregará com ela a energia de seu movimento, o que os físicos chamam de energia cinética. 



















7. Tunelamento.










Na física clássica, a situação é bem definida. Uma partícula cuja energia cinética é maior do que a tarifa de energia potencial conseguirá passar, atravessando a barreira com uma velocidade adequadamente reduzida (assim como um carro desacelera ao subir uma colina), mas acelerando de novo no outro lado conforme sua energia cinética total é restaurada. Se a energia cinética for menor do que a barreira de potencial, a partícula não consegue passar pela “colina” e deve simplesmente ser refletida.




Da perspectiva da 

mecânica quântica, a situação é diferente em função da possibilidade peculiar de tomar energia emprestada contra o tempo. Isso pode permitir que uma partícula cuja energia cinética seja classicamente insuficiente para superar a colina ainda assim atravesse, algumas vezes, a barreira, contanto que atinja o outro lado de modo rápido o bastante para devolver a energia dentro do limite de tempo necessário. É como se a partícula tivesse tunelado através da colina. Substituir essa pitoresca narração de histórias por cálculos precisos leva à conclusão de que uma partícula cuja energia cinética não esteja abaixo demais da altura da barreira terá certa 

probabilidade de atravessá-la e certa 

probabilidade de ser refletida.




Existem núcleos radioativos que se comportam como se contivessem determinados constituintes, chamados de partículas alfa, que são aprisionados dentro do núcleo por uma barreira de potencial gerada pelas forças nucleares. Se apenas essas partículas pudessem atravessar essa barreira, teriam energia suficiente para escapar por completo do outro lado. Núcleos desse tipo exibem, de fato, o fenômeno de um decaimento alfa, e foi um sucesso inicial da aplicação de ideias quânticas em nível nuclear o uso de cálculos de 

tunelamento para dar um relato quantitativo das propriedades dessas emissões alfa.




Estatística




Na física clássica, 

partículas idênticas (duas do mesmo tipo, como dois elétrons) são, ainda assim, distinguíveis entre si. Se inicialmente as rotulamos de 1 e 2, essas marcas de discriminação terão uma significância permanente à medida que monitorarmos as trajetórias separadas das partículas. Se os elétrons por fim emergirem após uma série complicada de interações, ainda podemos, em princípio, dizer qual é 1 e qual é 2. Por outro lado, no mundo quântico nebulosamente impassível de representação por imagens, esse não é mais o caso. Como não há trajetórias 

observáveis de forma contínua, tudo o que podemos dizer após a interação é que 

um

 elétron emergiu aqui e 

um

 elétron emergiu lá. Qualquer classificação escolhida inicialmente não pode ser acompanhada. Na teoria quântica, 

partículas idênticas também são partículas indistinguíveis.




Como as classificações não têm significância intrínseca, a ordem determinada em que aparecem em uma 

função de onda (

ψ

) deve ser irrelevante. Para 

partículas idênticas, o estado (1, 2) deve ser fisicamente o mesmo que o estado (2,1). Isso não significa que a 

função de onda fica estritamente inalterada pela troca, pois ocorre que os mesmos resultados físicos seriam obtidos de 

ψ 

ou de −

ψ 

[11]. Esse pequeno argumento leva a uma grande conclusão. O resultado tem a ver com o que se chama de “estatística” – o comportamento de coletâneas de 

partículas idênticas. Segundo a 

mecânica quântica, há duas possibilidades (correspondentes aos dois sinais possíveis do comportamento de 

ψ

 na operação de troca): 




a estatística de Bose

 sustenta que 

ψ 

permanece inalterada frente à operação de troca. Isso equivale a dizer que a 

função de onda é simétrica com a troca de duas partículas. Partículas que apresentam essa propriedade são chamadas de 

bósons;




a estatística de Fermi

 sustenta que 

ψ

 muda de sinal frente à operação de troca, ou seja, a 

função de onda é antissimétrica na troca de duas partículas. Partículas que apresentam essa propriedade são chamadas de 

férmions.




As duas opções geram comportamentos distintos da estatística de partículas classicamente distinguíveis. Acontece que a estatística quântica leva a consequências importantes para uma compreensão essencial das propriedades da matéria e também para a construção tecnológica de novos dispositivos. (Diz-se que 30% do PIB dos Estados Unidos derivam de indústrias com base quântica: semicondutores, lasers etc.)




Elétrons são 

férmions. Isso implica que ambos nunca podem ser encontrados exatamente no mesmo estado. Esse fato resulta da argumentação de que a troca ao mesmo tempo não produziria nenhuma mudança (uma vez que os dois estados são o mesmo) e também uma mudança de sinal (por causa da estatística de Fermi). A única maneira de sair desse dilema é concluir que a 

função de onda de duas partículas é, na verdade, zero. (Outro meio de enunciar o mesmo argumento é destacar que não se pode fazer uma combinação antissimétrica de duas entidades idênticas.) Esse resultado é chamado de 

princípio de exclusão

 e oferece a base para entender a tabela periódica 

química, com suas propriedades recorrentes de elementos relacionados. De fato, o princípio de exclusão está na base da possibilidade de uma 

química complexa o bastante para, em última análise, sustentar o desenvolvimento da própria vida.




A história 

química é mais ou menos assim: em um átomo, existem apenas certos estados de energia disponíveis para os elétrons e, naturalmente, o princípio de exclusão exige que não exista mais de um elétron ocupando qualquer um deles. O menor estado de energia estável do átomo corresponde ao preenchimento dos estados menos energéticos disponíveis. Esses estados podem ser o que os físicos chamam de “degenerados”, o que significa que há diversos estados diferentes e que todos têm a mesma energia. Um conjunto de estados degenerados constitui um nível de energia. Podemos visualizar mentalmente o menor estado de energia do átomo como sendo composto por meio da adição de elétrons um a um em níveis sucessivos de energia, até o número necessário de elétrons no átomo. Uma vez que todos os estados de um determinado nível de energia estejam cheios, um elétron adicional terá que ir para o próximo nível de energia mais alto contido pelo átomo. Se esse nível, por sua vez, for preenchido, então ele passa para o próximo nível, e assim por diante. Em um átomo com muitos elétrons, os menores níveis de energia (também chamados de “camadas”) estarão todos cheios, com quaisquer elétrons de sobra ocupando a próxima camada. Essa “sobra” de elétrons são os que ficaram mais longe do núcleo e, por esse motivo, determinarão as interações químicas entre o átomo e outros átomos. À medida que subimos na escala de complexidade atômica (percorrendo a tabela periódica), o número de elétrons de sobra (0, 1, 2...) varia ciclicamente, conforme cada camada fica preenchida, e é esse padrão de repetição dos elétrons mais externos que produz as repetições químicas da tabela periódica.




Em contraste com os elétrons, fótons são 

bósons. Ocorre que o comportamento dos 

bósons é exatamente o oposto do comportamento dos 

férmions. O princípio de exclusão não se aplica a eles! Os 

bósons gostam de estar no mesmo estado. Eles são semelhantes aos habitantes do sul da Europa, amontoando-se alegremente no mesmo vagão do trem, enquanto os 

férmions são como os europeus do norte, espalhados isoladamente por todo o trem. Esse companheirismo dos 

bósons é um fenômeno que, em sua forma mais extrema, leva ao grau de concentração em um único estado que é chamado de 

condensação de Bose. É essa propriedade que está por trás do aparelho tecnológico do laser. O poder da 

luz do laser deve-se ao fato de ela ser o que se chama de “coerente”, ou seja, a 

luz consiste de muitos fótons que estão todos exatamente no mesmo estado, uma propriedade para a qual a estatística de Bose dá um forte incentivo. Também há efeitos associados à supercondutividade (o desaparecimento de resistência elétrica em temperaturas muito baixas) que dependem da 

condensação de Bose, levando a consequências 

observáveis das propriedades quânticas em nível macroscópico. (A baixa temperatura é necessária para evitar que a colisão térmica destrua a coerência.)




Elétrons e fótons também são partículas com 

spin

, ou seja, eles carregam uma quantia intrínseca de momentum angular (uma medida de efeitos de rotação) quase como se fossem pequenos piões. Nas unidades que são naturais na teoria quântica (definidas pela 

constante de 

Planck), o elétron tem 

spin

 

1

/

2

 e o 

fóton tem 

spin

 1. Esse fato ilustra uma regra geral: partículas de 

spin

 inteiro (0, 1...) são sempre 

bósons; partículas de 

spin

 fracionário (

1

/

2

, 

3

/

2

,…) são sempre 

férmions. Do ponto de vista de teoria quântica comum, esse 

teorema da estatística do

 

spin

 é apenas uma regra prática sem explicação. Porém, Wolfgang 

Pauli (que também formulou o princípio da exclusão) descobriu que, quando a teoria quântica e a relatividade especial são combinadas, o teorema emerge como uma consequência necessária dessa combinação. A junção das duas teorias gera percepções mais ricas do que cada uma é capaz de oferecer de modo independente. O todo acaba sendo mais do que a soma das partes.




Estrutura de bandas




A forma de matéria sólida mais simples de se imaginar é um cristal, em que os átomos constituintes são ordenados no padrão de uma disposição regular. Um cristal macroscópico, significativo na escala da experiência cotidiana, conterá tantos átomos que pode ser tratado, de modo eficiente, como infinitamente grande do ponto de vista microscópico da teoria quântica. A aplicação de princípios de 

mecânica quântica a sistemas desse tipo revela novas propriedades, intermediárias entre as de átomos individuais e as de partículas em movimento livre. Vimos que, em um átomo, possíveis energias de elétron vêm em uma série discreta de níveis distintos. Por outro lado, um elétron em movimento livre pode ter qualquer energia positiva, correspondente à energia cinética de seu movimento real. As propriedades energéticas de elétrons em cristais são um tipo de meio-termo entre esses dois extremos. Os valores possíveis de energia encontram-se dentro de uma série de bandas. 



















8. Estrutura de bandas.










Em uma banda, há uma variação contínua de possibilidades; entre bandas, nenhum nível de energia está disponível aos elétrons. Em resumo, as propriedades energéticas de elétrons em um cristal correspondem a uma série de faixas de valores alternados entre permitidos e proibidos. 




A existência dessa 

estrutura de bandas oferece a base para entender as propriedades elétricas de sólidos cristalinos. As correntes elétricas resultam da indução do movimento de elétrons dentro do sólido. Se a banda de energia mais alta de um cristal estiver totalmente cheia, essa mudança no estado do elétron exigirá a excitação de elétrons pela lacuna até a banda acima. A transição demandaria uma entrada significativa de energia por elétron excitado. Como isso é muito difícil de ser efetivado, um cristal com bandas totalmente preenchidas se comportará como um isolante. Será muito difícil induzir movimento em seus elétrons. Se, porém, um cristal tiver sua banda mais alta apenas parcialmente preenchida, a excitação será fácil, pois somente será necessária uma pequena entrada de energia para mover um elétron para um estado disponível de energia um pouco mais alta. Esse cristal se comportará como um condutor elétrico.




Experimentos de escolha demorada




Mais informações sobre as estranhas implicações do 

princípio da sobreposição foram dadas pela discussão de 

John Archibald Wheeler sobre o que ele chamava de “experimentos de escolha demorada”. Uma disposição possível está ilustrada na Figura 9. Um feixe estreito de 

luz é dividido em A em dois subfeixes, que são refletidos pelos espelhos em B e C para juntá-los novamente em D, onde um padrão de interferência pode formar-se em razão da diferença de fase entre os dois trajetos (as 

ondas saíram de fase). Podemos considerar um feixe inicial tão fraco que, a qualquer momento, apenas um único 

fóton atravessa o dispositivo. Os efeitos de interferência em D devem ser entendidos como resultantes da autointerferência entre os dois estados sobrepostos: (trajeto à esquerda) e (trajeto à direita). (Compare com a discussão do 

experimento da fenda dupla no Capítulo 2.) A nova característica que Wheeler discutiu surge se o dispositivo for modificado pela inserção de um aparelho X entre C e D. X é uma chave que deixa um 

fóton passar ou o desvia para um detector Y. Se a chave for ajustada para transmissão, o experimento é o mesmo que o anterior, com um padrão de interferência em D. Se a chave for ajustada para deflexão e o detector Y registrar um 

fóton, então não pode haver padrão de interferência em D, porque esse 

fóton deve certamente ter tomado o trajeto à direita para que fosse desviado por Y. Wheeler destacou o estranho fato de que o ajuste de X poderia ser escolhido enquanto o 

fóton está a caminho 

depois de

 A. Até que a configuração da chave seja selecionada, o 

fóton está, de alguma forma, possibilitando duas opções: a de seguir os trajetos à esquerda e à direita e também a de seguir apenas um deles. Experimentos engenhosos foram, de fato, conduzidos seguindo essas linhas de raciocínio. 




Somas sobre histórias




Richard 

Feynman descobriu uma maneira idiossincrática de reformular a teoria quântica. Essa reformulação gera as mesmas previsões da abordagem convencional, mas oferece um modo pictórico bastante distinto de pensar sobre como surgem esses resultados.

























9. Um 

experimento de escolha demorada.










A física clássica nos apresenta trajetórias claras, trajetos únicos de movimento conectando o ponto de partida A ao ponto final B. De modo convencional, o cálculo é feito pela solução das célebres equações da mecânica newtoniana. No século XVIII, descobriu-se que o trajeto real poderia ser prescrito de uma forma diferente, mas equivalente, ao descrevê-lo como a trajetória ligando A a B que deu o valor mínimo para uma determinada grandeza dinâmica associada a trajetos diferentes. Essa grandeza é chamada de “ação”, e não precisamos abordar sua definição aqui. O princípio da 

mínima ação (como naturalmente passou a ser chamado) assemelha-se à propriedade dos raios de 

luz, ou seja, eles tomam o trajeto de tempo mínimo entre dois pontos. (Se não houver refração, esse trajeto é uma linha reta, mas em um meio refrator, o princípio do tempo mínimo leva à conhecida curvatura dos raios, tal como quando um lápis dentro de um copo d’água parece estar dobrado.)




Devido à nebulosa impossibilidade de representar os processos quânticos em imagens, as partículas quânticas não têm trajetórias definitivas. 

Feynman sugeriu que, em vez disso, devemos imaginar uma partícula quântica se movendo de A para B 

por todos os trajetos possíveis

, em linha reta ou curva, rápida ou lentamente. Desse ponto de vista, a 

função de onda do raciocínio convencional surgiu da adição de contribuições de todas essas possibilidades, dando origem à descrição de “somas sobre histórias”.




Os detalhes de como os termos nessa imensa soma devem ser formados são técnicos demais para serem discutidos aqui. A contribuição de determinado trajeto está relacionada à ação associada a esse trajeto dividida pela 

constante de 

Planck. (As dimensões físicas de ação e de 

h

 são as mesmas, então sua razão é um número puro, independentemente das unidades em que escolhemos medir as grandezas físicas.) A forma real assumida por essas contribuições de diferentes trajetos é aquela na qual os trajetos vizinhos tendem a se anular, em função de rápidas flutuações nos sinais (mais precisamente, fases) de suas contribuições. Se o sistema considerado é do tipo cuja ação é grande em relação a 

h

, somente o trajeto de 

mínima ação contribuirá bastante (pois é próximo desse trajeto que as flutuações são menores e, portanto, o efeito de anulações é minimizado). Essa observação oferece uma maneira simples de entender por que grandes sistemas comportam-se de modo clássico, seguindo trajetos de 

mínima ação.




A formulação precisa e calculável dessas ideias não é nada fácil. Pode-se prontamente imaginar que a amplitude de variação representada pela multiplicidade de trajetos possíveis não é um simples agregado sobre o qual somar. Apesar disso, a abordagem de somas sobre histórias teve duas consequências importantes. Uma delas é ter levado 

Feynman a descobrir uma técnica de cálculo muito mais manejável, agora universalmente chamada de “integrais de 

Feynman”, que é a mais útil abordagem aos cálculos quânticos disponibilizada a físicos nos últimos cinquenta anos. Ela gera uma imagem física em que as interações se devem à troca de energia e momentum carregados pelo que se chamam 

partículas virtuais

. O adjetivo “virtual” é usado porque essas “partículas” intermediárias, que não podem aparecer nos estados inicial e final do processo, não são forçadas a ter massas físicas, mas, em vez disso, somas sobre todos os valores de massa possíveis.




A outra vantagem da abordagem de somas sobre histórias é que existem alguns sistemas quânticos um tanto sutis e complicados para os quais ela oferece uma maneira mais clara de formular o problema do que aquela dada pela abordagem mais convencional.




Mais sobre 

descoerência




Os efeitos ambientais da radiação onipresente que produz 

descoerência têm um significado que vai além de sua relevância parcial ao problema da 

medição. Um importante desenvolvimento recente foi a percepção de que eles também dizem respeito a como se deve pensar na 

mecânica quântica dos chamados sistemas caóticos.




As imprevisibilidades intrínsecas que estão presentes na natureza não surgem apenas dos processos quânticos. Foi uma grande surpresa para a maioria dos físicos quando, em torno de quarenta anos atrás, despontou a compreensão de que, mesmo na física newtoniana, há muitos sistemas cuja sensibilidade extrema aos efeitos de distúrbios muito pequenos torna seu comportamento futuro além de nosso poder de previsão exata. Esses sistemas caóticos (como são chamados) logo passam a ser sensíveis a detalhes ao nível da incerteza de 

Heisenberg ou abaixo dela. Ainda assim, seu tratamento de uma perspectiva quântica – um assunto chamado de 

caologia quântica

 – é problemático.




A razão para a perplexidade é a seguinte: os sistemas caóticos têm um comportamento cujo caráter geométrico corresponde aos famosos fractais (dos quais o conjunto de Mandelbrot, tema de centenas de pôsteres psicodélicos, é o melhor exemplo conhecido). Os fractais são o que se chamam de “autossemelhantes”, ou seja, eles se parecem basicamente os mesmos em qualquer escala em que forem examinados (dentes de serra compostos de dentes de serra..., por todo o caminho). Portanto, os fractais não têm escala natural. Os sistemas quânticos, por outro lado, têm uma escala natural, definida pela 

constante de 

Planck. Logo, a 

teoria do caos e a teoria quântica não se encaixam suavemente uma na outra. 




A incompatibilidade resultante leva à chamada “supressão quântica do caos”: os sistemas caóticos têm seu comportamento modificado quando se trata de depender de detalhes no nível quântico. Isso, por sua vez, leva a outro problema para os físicos, surgindo em sua forma mais aguda da consideração da 16

a

 lua de Saturno, chamada de Hipérion. Esse pedaço de rocha em forma de batata, em torno do tamanho de Nova York, está dando voltas de modo caótico. Se aplicarmos noções de supressão quântica à Hipérion, espera-se que o resultado seja incrivelmente eficiente, apesar do considerável tamanho da lua. Na verdade, com base nesse cálculo, o movimento caótico só poderia durar, no máximo, por volta de 37 anos. Na realidade, astrônomos vêm observando Hipérion por bem menos tempo do que isso, mas ninguém espera que sua estranha rotação cesse em pouco tempo. À primeira vista, estamos enfrentando um sério problema. Porém, a solução surge quando se leva a 

descoerência em consideração. A tendência da 

descoerência em mover as coisas em uma direção aparentemente mais clássica tem o efeito, por sua vez, de suprimir a supressão quântica do caos. Podemos esperar com confiança que Hipérion continue movendo-se em desordem por mais um longo tempo.




Outro efeito de um tipo bastante semelhante devido à 

descoerência é o 

efeito Zenão quântico

. Um núcleo radioativo em função do decaimento será forçado de volta ao seu estado inicial pelas “miniobservações” que resultam de sua interação com os fótons ambientais. Esse contínuo retorno à estaca zero tem o efeito de inibir o decaimento, um fenômeno já observado em experimentos. Esse efeito leva o nome de um antigo filósofo grego, Zenão, cuja reflexão sobre a observação de uma flecha estando 

agora

 em um ponto fixo o persuadiu de que a flecha não poderia estar realmente se movendo. 




Esses fenômenos evidenciaram que a relação entre teoria quântica e seu limite clássico é sutil, envolvendo o 

entrelaçamento de efeitos que não podem ser caracterizados tão somente por uma divisão simplista em “grandes” e “pequenos”.




Teoria quântica relativista




Nossa discussão do teorema da estatística do 

spin

 já demonstrou que a combinação da teoria quântica com a relatividade especial produz uma teoria unificada de conteúdo enriquecido. A primeira equação exitosa que conseguiu formular a combinação das duas de modo consistente foi a equação relativista do elétron, descoberta por Paul 

Dirac em 1928 [12]. Seus detalhes matemáticos são técnicos demais para serem apresentados em um livro como este, mas devemos destacar duas importantes e imprevistas consequências que se seguiram a esse desenvolvimento. 




Dirac produziu sua equação apenas com as necessidades da teoria quântica e da invariância relativista em mente. Portanto, deve ter sido uma gratificante surpresa quando ele descobriu que as previsões da equação das propriedades eletromagnéticas do elétron eram tais que se verificou que as interações magnéticas do elétron eram duas vezes mais intensas do que se teria ingenuamente esperado com base no raciocínio de que o elétron é um pião em miniatura e com carga elétrica. Já se sabia empiricamente que era isso que acontecia, mas ninguém havia conseguido entender o motivo desse comportamento aparentemente anômalo. 




A segunda – e ainda mais significativa – consequência resultou da brilhante transformação que 

Dirac fez de ameaça de derrota em vitória triunfal. Como estava, a equação tinha um defeito grosseiro. Ela permitia estados positivos de energia do tipo necessário para corresponder ao comportamento de elétrons reais, mas também permitia estados negativos de energia, os quais não faziam sentido em termos físicos. Ainda assim, não poderiam ser simplesmente descartados, pois os princípios de 

mecânica quântica inevitavelmente permitiriam a desastrosa consequência de transições a eles dos estados positivos de energia fisicamente aceitáveis. (Isso seria um desastre físico porque as transições para tais estados poderiam produzir quantidades ilimitadas de energia positiva de equilíbrio, resultando em um tipo de máquina descontrolada de movimento perpétuo.) Por um bom tempo, este foi um enigma altamente embaraçoso. 

Dirac então percebeu que a estatística de elétrons de Fermi poderia permitir uma saída do dilema. Com grande audácia, ele supôs que todos os estados de energia negativa já estavam ocupados. O princípio da exclusão bloquearia a possibilidade de qualquer transição para eles a partir dos estados positivos de energia. O que as pessoas haviam considerado espaço vazio (o 

vácuo) estava, na verdade, repleto desse “mar” de elétrons de energia negativa!




Parece uma imagem um tanto estranha e posteriormente, de fato, foi possível formular a teoria de modo que preservasse os resultados desejáveis de maneira menos pitoresca, mas também menos esquisita. Enquanto isso, o trabalho com o conceito do mar de energia negativa levou 

Dirac a uma descoberta de grande importância. Se fosse fornecida energia suficiente, por exemplo, por um 

fóton com muita energia, seria possível ejetar um elétron com energia negativa do mar, transformando-o em um elétron com energia positiva do tipo comum. O que, então, se deveria fazer com o “buraco” que esse processo deixara para trás no mar negativo? Como a ausência de energia negativa é o mesmo que a presença de energia positiva (dois sinais de menos resulta em um sinal de mais), então o buraco se comportaria como uma partícula com energia positiva. Contudo, a ausência de carga negativa é o mesmo que a presença de carga positiva, por isso essa “partícula-buraco” estaria com carga positiva, em contraste ao elétron com carga negativa.




Na década de 1930, o pensamento de físicos de partículas elementares era bastante conservador em comparação à liberdade especulativa que viria mais tarde. Eles não gostavam nem um pouco da ideia de sugerir a existência de algum tipo de partícula novo e, até então, desconhecido. Portanto, supunha-se inicialmente que essa partícula positiva sobre a qual 

Dirac falara poderia ser o bem conhecido próton com carga positiva. Porém, logo percebeu-se que o buraco precisava ter a mesma massa do elétron, enquanto o próton tem massa muito maior. Assim, a única interpretação aceitável disponível levava à previsão um tanto relutante de uma partícula totalmente nova – em pouco tempo batizada de pósitron – de massa eletrônica, mas com carga positiva. Sua existência logo foi confirmada por meio de experimentos, quando os pósitrons foram detectados em raios cósmicos. (Na verdade, já havia exemplos muito antes, porém não foram reconhecidos como tal. Os pesquisadores têm dificuldade em ver o que não estão procurando.)




Acabou se percebendo que essa união de elétron e pósitron era um exemplo específico de comportamento abundante na natureza. Existe tanto a matéria (como os elétrons) quanto a 

antimatéria

 com carga oposta (como os pósitrons). O prefixo “anti” é apropriado porque um elétron e um pósitron podem anular-se, desaparecendo em uma explosão de energia. (No modo antiquado de falar, o elétron preenche o buraco no mar e a energia liberada é irradiada. Inversamente, como vimos, um 

fóton de alta energia pode impulsionar um elétron para fora do mar, deixando um buraco para trás e criando, assim, um par elétron-pósitron.)




A fértil história da equação de 

Dirac, que levou a uma explicação das propriedades magnéticas e à descoberta da 

antimatéria, temas que não exerceram nenhuma função na motivação original da equação, é um impressionante exemplo do valor ao longo prazo que pode ser exibido por uma ideia científica realmente fundamental. É essa fertilidade notável que convence os físicos de que realmente estão “prestes a descobrir algo” e que, ao contrário das sugestões de alguns filósofos e sociólogos da ciência, não estão apenas concordando tacitamente em analisar as coisas de determinada maneira. Em vez disso, estão fazendo descobertas sobre como o mundo físico realmente é.




Teoria quântica de campos




Outra descoberta essencial foi feita por 

Dirac quando aplicou os princípios de 

mecânica quântica ao campo eletromagnético, em vez de aplicá-los às partículas. Esse desenvolvimento gerou o primeiro exemplo conhecido de uma 

teoria quântica de campos. Em retrospectiva, dar esse passo não é tão difícil em termos técnicos. A principal diferença entre uma partícula e um campo é que aquela tem apenas um número finito de graus de liberdade (maneiras independentes em que seu estado pode alterar-se), ao passo que um campo tem um número infinito de graus de liberdade. Existem técnicas matemáticas bastante conhecidas para lidar com essa diferença.




A 

teoria quântica de campos é de considerável interesse e nos oferece uma das formas mais esclarecedoras de pensar sobre a 

dualidade onda-partícula. Um campo é uma entidade dispersa no espaço e no tempo. Ele é, portanto, uma entidade que tem uma natureza intrinsecamente ondulatória. A aplicação da teoria quântica ao campo resulta em suas grandezas físicas (como energia e momentum) estando presentes em pacotes discretos e contáveis (

quanta). Todavia, essa capacidade de contagem é simplesmente o que associamos ao comportamento corpuscular. Logo, no estudo de um campo quântico, estamos investigando e compreendendo uma entidade que exibe explicitamente propriedades ondulatórias e corpusculares da maneira mais clara possível. É mais ou menos como se surpreender com o fato de que um mamífero possa pôr um ovo que dá origem a um ornitorrinco de bico parecido com o de um pato. Um exemplo real é sempre instrutivo ao máximo. Ocorre que, na 

teoria quântica de campos, os estados que demonstram propriedades ondulatórias (em termos técnicos, que têm fases definidas) são aqueles que contêm um número 

indefinido

 de partículas. Essa última propriedade é uma possibilidade natural devido ao 

princípio da sobreposição da teoria quântica, que permite a combinação de estados com diferentes números de partículas neles. Seria uma opção impossível na teoria clássica, em que só é preciso olhar e ver para contar o número de partículas realmente presentes.




O 

vácuo, na 

teoria quântica de campos, tem propriedades incomuns que são especialmente importantes. O 

vácuo é, obviamente, o menor estado de energia, em que não haverá excitações que correspondam a partículas. Ainda assim, embora não exista nada lá nesse sentido, na 

teoria quântica de campos não significa que não esteja acontecendo nada. A razão para isso é a seguinte: uma técnica matemática padrão, chamada de análise de Fourier, nos permite considerar um campo como sendo equivalente a uma coleção infinita de osciladores harmônicos. Cada oscilador tem uma frequência específica associada a ele, e o oscilador se comporta dinamicamente, como se fosse um pêndulo daquela determinada frequência. O 

vácuo no campo é o estado em que todos esses “pêndulos” estão em seus menores estados de energia. Para um pêndulo clássico, isso ocorre quando o peso está em repouso e na parte inferior. Essa é verdadeiramente uma situação em que nada está acontecendo. Porém, a 

mecânica quântica não possibilita um grau tão perfeito de tranquilidade. 

Heisenberg não admitirá que o “peso” tenha uma posição definida (na parte inferior) e um momentum definido (em repouso). Em vez disso, o pêndulo quântico deve estar em leve movimento, mesmo em seu menor estado de energia (próximo à parte inferior e quase em repouso, mas não totalmente). A vibração quântica resultante é chamada de 

movimento de ponto zero

. A aplicação dessas ideias – muitas vezes ao longo da disposição infinita de osciladores, que é um campo quântico – supõe que seu 

vácuo seja uma colmeia de atividade. As flutuações ocorrem continuamente e, no decorrer disso, “partículas” transitórias aparecem e desaparecem. Um 

vácuo quântico é mais como um 

plenum

 do que como o espaço vazio.




Quando os físicos começaram a aplicar a 

teoria quântica de campos a situações que envolvem interações entre os campos, enfrentaram algumas dificuldades. O número infinito de graus de liberdade tendia a produzir respostas infinitas para o que deveriam ter sido grandezas físicas finitas. Um caminho importante pelo qual isso ocorria era por meio da interação com o 

vácuo em constante flutuação. Por fim, encontrou-se uma forma de produzir sentido a partir da falta de sentido. Certos tipos de teorias de campos (chamadas de 

teorias renormalizáveis

) produzem apenas tipos limitados de infinitos, tão somente associados às massas de partículas e às intensidades de suas interações. A simples eliminação desses termos infinitos e sua substituição pelos valores finitos medidos das grandezas físicas relevantes é um procedimento que define resultados significativos, mesmo que o procedimento não seja exatamente puro. Também serve para fornecer expressões finitas que estejam em completa concordância com o experimento. A maioria dos físicos está contente com esse sucesso pragmático. No entanto, 

Dirac nunca se sentiu assim. Ele desaprovava completamente engodos questionáveis com grandezas formalmente infinitas.




Hoje, todas as teorias de partículas elementares (como a teoria do quark de matéria) são teorias quânticas de campos. As partículas são consideradas excitações energéticas do campo subjacente. (Uma teoria de campos apropriada também oferece a medida certa de lidar com as dificuldades do “mar” de elétrons com energia negativa.)




Computação quântica




Recentemente, tem havido considerável interesse na possibilidade de explorar o 

princípio da sobreposição como um meio de obter maior poder computacional.




A computação convencional baseia-se na combinação de operações binárias, expressas formalmente em combinações lógicas de zeros e uns, realizáveis em termos de 

hardware

 por chaves que são ligadas ou desligadas. Em um dispositivo clássico, é evidente, essas últimas possibilidades são mutuamente exclusivas. Uma chave está definitivamente ligada ou definitivamente desligada. No mundo quântico, porém, a chave poderia estar em um estado que é uma sobreposição dessas duas possibilidades clássicas. Uma sequência de tais sobreposições corresponderia a um tipo totalmente novo de processamento em paralelo. A capacidade de manter tantas bolas computacionais no ar ao mesmo tempo pode, em princípio, representar um aumento de poder de computação que a adição de elementos extras multiplicaria de modo exponencial se comparado ao aumento linear em circunstâncias convencionais. Muitas tarefas computacionais, como as decodificações ou a fatoração de números muito grandes, inviáveis nas máquinas atuais, então se tornariam possíveis.




As possibilidades são empolgantes. (Seus proponentes exibem apreço em falar sobre elas em termos de muitos mundos, como se o processamento fosse acontecer em universos paralelos, mas parece que realmente é apenas o 

princípio da sobreposição em si que serve de base para a viabilidade da 

computação quântica.) Contudo, a implantação na prática será uma empreitada nitidamente complexa, ainda com muitos problemas a serem resolvidos. Vários deles concentram-se na preservação estável dos estados sobrepostos. O fenômeno da 

descoerência mostra como pode ser problemático isolar um computador quântico da interferência ambiental deletéria. A 

computação quântica está recebendo séria atenção tecnológica e empreendedora, mas, como procedimento efetivo, continua sendo um brilho nos olhos de seus proponentes.

Capítulo 5




Conectividade




Einstein, por meio de sua explicação do 

efeito fotoelétrico, foi um dos avôs da teoria quântica. Entretanto, passou a detestar sua neta. Como a grande maioria dos físicos, estava profundamente convencido da realidade do mundo físico e acreditava na verdadeira confiabilidade da explicação que a ciência dava de sua natureza. Mas ele veio a crer que essa realidade só poderia ser garantida pelo tipo de objetividade ingênua que o raciocínio newtoniano presumira. Por consequência, 

Einstein abominava a nebulosa desordem que a ortodoxia de Copenhague atribuía à natureza do mundo quântico.




Seu primeiro ataque contra a teoria quântica moderna assumiu a forma de uma série de experimentos mentais altamente engenhosos, sendo que cada um deles objetivava contornar, de alguma maneira, as limitações do 

princípio da incerteza de 

Heisenberg. O adversário de 

Einstein nessa competição era 

Niels 

Bohr, que cada vez obtinha êxito em demonstrar que uma aplicação completa das ideias quânticas a todos os aspectos do experimento proposto resultava na sobrevivência incólume do 

princípio da incerteza. Por fim, 

Einstein admitiu sua derrota nessa batalha.




Após lamber suas feridas por um tempo, 

Einstein estava de volta, escolhendo um novo terreno para o combate. Com dois colaboradores mais jovens, Boris 

Podolsky e Nathan 

Rosen, ele demonstrou que havia algumas implicações de longo alcance muito específicas, até então despercebidas, para o comportamento da 

mecânica quântica de duas partículas aparentemente bem separadas. As questões são explicadas com maior facilidade em termos de um desenvolvimento posterior do que – considerando os nomes de seus descobridores – podemos chamar de pensamento EPR. O argumento deve-se a David 

Bohm e, embora seja um pouco complexo, vale a pena analisá-lo.




Suponha que duas partículas tenham 

spins

 

s

1

 e 

s

2

 e saiba-se que o 

spin

 total é zero. Isso implica, naturalmente, que 

s

2 

é –

s

1

. O 

spin

 é um vetor (ou seja, tem magnitude, direção e sentido – pense nele como se fosse uma seta), e seguimos a convenção matemática usando negrito para grandezas vetoriais. Um vetor de 

spin

, portanto, tem três componentes medidos ao longo de três direções espaciais escolhidas: x, y e z. Se medíssemos o componente x de 

s

1 

e obtivéssemos s’

1x

 como resposta, então o componente x de 

s

2 

deveria ser –s’

1x

. Se, por outro lado, tivéssemos medido o componente y de 

s

1

, obtendo a resposta s’

1y

, saberíamos que o componente y de 

s

2 

teria sido –s’

1y

. No entanto, a 

mecânica quântica não permite a 

medição simultânea dos componentes x e y do 

spin

, pois há uma relação de incerteza entre eles. 

Einstein argumentou que, embora isso possa ser verdadeiro segundo o pensamento quântico ortodoxo, o que quer que tenha acontecido com a partícula 1 não poderia ter efeito imediato sobre a distante partícula 2. No pensamento EPR, a separação espacial de 1 e 2 implica 

a independência do que acontece em 1 e do que acontece em 2

. Se esse fosse o caso, e se pudéssemos escolher medir o componente x ou y do 

spin

 em 1 e obter determinado conhecimento dos componentes x ou y, respectivamente, do 

spin

 em 2, então 

Einstein alegava que a partícula 2 devia, na verdade, ter esses valores definidos para seus componentes de 

spin

, realizando, de fato, as medições ou não. Isso era algo que a teoria quântica convencional negava, porque a relação de incerteza entre os componentes x e y do 

spin

 aplicava-se tanto à partícula 2 quanto à partícula 1.




A conclusão que 

Einstein derivou desse argumento moderadamente complicado é de que deveria haver algo incompleto na teoria quântica convencional. Esta não explicava o que ele acreditava serem valores definidos dos componentes do 

spin

. Quase todos os seus colegas físicos interpretam as coisas de modo diferente. Em sua visão, nem 

s

1 

nem 

s

2 

têm componentes definidos do 

spin

 até que uma 

medição seja realmente feita. Logo, a determinação do componente x de 1 força o componente de 2 a assumir o valor oposto. Isso equivale a dizer que a 

medição em 1 também força um colapso da 

função de onda em 2 no valor oposto do componente x do 

spin

. Se fosse o componente y a ser medido em 1, então o colapso em 2 teria sido no componente y oposto do 

spin

. Esses dois estados 2 (componente x conhecido; componente y conhecido) são absolutamente distintos entre si. Assim, a visão da maioria leva à conclusão de que 

a 

medição em 1 produz mudança instantânea em 2, uma mudança que depende exatamente do que é medido em 1

. Ou seja, há alguma conectividade não local contraintuitiva entre 1 e 2; ação em 1 produz consequências imediatas para 2, e as consequências são diferentes para ações diferentes em 1. Isso costuma ser chamado de 

efeito EPR

. A terminologia é de certo modo irônica, uma vez que o próprio 

Einstein se recusava a acreditar em tal conexão de longo alcance, considerando-a uma influência “fantasmagórica” demais para ser aceitável por um físico. Assim permaneceu a questão por um tempo. 




O próximo passo foi dado por 

John Bell. Ele analisou quais propriedades o sistema 1-2 teria se fosse um sistema genuinamente separado (conforme 

Einstein havia suposto), com propriedades em 1 dependendo apenas do que ocorria localmente em 1 e propriedades em 2 dependendo apenas do que acontecia em 2 em nível local. Bell demonstrou que, se essa estrita localidade fosse verdadeira, haveria certas relações entre as grandezas mensuráveis (elas são agora chamadas de 

desigualdades de Bell

) que a 

mecânica quântica previu que seriam violadas em determinadas circunstâncias. Isso representava um significativo passo à frente, movendo o argumento do reino dos experimentos mentais para o domínio empiricamente acessível do que poderia ser de fato investigado no laboratório. Os experimentos não eram fáceis de realizar, mas por fim, no início da década de 1980, 

Alain 

Aspect e seus colaboradores conseguiram conduzir uma investigação orquestrada com habilidade para confirmar as previsões da teoria quântica e negar a possibilidade de uma teoria puramente local do tipo que 

Einstein havia defendido. Havia ficado claro que existe um grau irredutível de não localidade presente no mundo físico. As entidades quânticas que interagiram entre si permanecem mutuamente entrelaçadas, por mais longe que possam, por fim, separar-se espacialmente. Parece que a natureza luta contra um reducionismo implacável. Nem o mundo subatômico pode ser tratado de maneira puramente atomística.




A implicação do 

efeito EPR da relacionalidade profundamente arraigada na estrutura fundamental do mundo físico é uma descoberta sobre a qual o pensamento físico e a reflexão 

metafísica ainda precisam chegar a um acordo para elucidar totalmente todas as suas consequências. Como parte desse processo contínuo de assimilação, é necessário ser o mais transparente possível sobre qual é a natureza do 

entrelaçamento implicada pelo EPR. Deve-se reconhecer que está envolvido um verdadeiro caso de ação a distância, e não meramente algum ganho de conhecimento adicional. Usando uma linguagem acadêmica, o 

efeito EPR é ontológico, e não apenas epistemológico. O acréscimo de conhecimento a distância não é problemático ou surpreendente. Suponha que uma urna contenha duas bolas: uma branca, a outra preta. Eu e você colocamos nossas mãos e retiramos uma das bolas com o punho cerrado. A seguir, você caminha um quilômetro pela estrada, abre a mão e verifica que tem a bola branca. Imediatamente sabe que eu devo ter a preta. O único detalhe que muda nesse episódio é o seu estado de conhecimento. Sempre tive a bola preta, você sempre ficou com a branca, mas agora ficou ciente disso. No 

efeito EPR, ao contrário, o que acontece em 1 

muda

 o que ocorre em 2. É como se, caso você descobrisse ter uma bola vermelha na mão, eu deveria ter uma bola azul na minha; porém, se você encontrasse uma bola verde, eu teria que estar com uma bola amarela e, antes de você olhar, nenhum de nós tivesse bolas de cores determinadas. 




Um leitor atento pode questionar toda essa conversa sobre mudança instantânea. A relatividade especial não proíbe que algo em 1 tenha qualquer efeito em 2 até que haja tempo para a transmissão de uma influência que se move, no máximo, à velocidade da 

luz? Não exatamente. O que a relatividade realmente proíbe é a transmissão instantânea de informação, aquela de um tipo que permitiria a sincronização imediata de um relógio em 2 com um relógio em 1. Ocorre que o tipo de 

entrelaçamento do EPR não permite o transporte de mensagens desse tipo. O motivo para isso é que sua conectividade não local assume a forma de correlações entre o que está acontecendo em 1 e o que está acontecendo em 2, e nenhuma mensagem pode ser lida fora dessas correlações sem conhecimento do que está ocorrendo nas duas extremidades. É como se um cantor em 1 estivesse cantando uma sequência aleatória de notas e um cantor em 2 também estivesse cantando uma sequência aleatória de notas, e somente se alguém conseguisse ouvi-los ao mesmo tempo é que perceberia que os dois cantores estão em algum tipo de harmonia entre eles. A percepção de que isso ocorre assim nos previne de adotar o tipo de argumento “quântico da moda”, o qual erroneamente afirma que o EPR “prova” a possibilidade da telepatia.

Capítulo 6




Lições e significados




A imagem de processo físico apresentada a nós pela teoria quântica é radicalmente diferente da que a experiência diária nos levaria a esperar. Sua peculiaridade é tal que chega a suscitar, com alguma intensidade, a questão de ser esta, de fato, como a natureza subatômica “realmente é” ou se a 

mecânica quântica não passa de uma maneira conveniente, porém estranha, de falar que nos permite fazer os cálculos. Podemos obter respostas que convergem assustadoramente com os resultados obtidos pelo uso em laboratório de aparelhos clássicos de 

medição, mas talvez não devamos realmente acreditar na teoria. A questão suscitada é basicamente de ordem filosófica e vai além do que pode ser determinado unicamente pelo uso dos próprios recursos desassistidos da ciência. Na verdade, esse questionamento quântico é apenas um exemplo específico – mesmo que desafiador de modo incomum – do fundamental debate filosófico entre os realistas e os positivistas.




Positivismo e 

realismo




Os positivistas consideram o papel da ciência como sendo de reconciliação de dados observacionais. Se é possível fazer previsões que explicam, de modo preciso e harmonioso, o comportamento dos aparelhos de 

medição, a tarefa está feita. Questões ontológicas (o que realmente existe lá?) são um luxo irrelevante e é melhor que sejam descartadas. O mundo dos positivistas é povoado de leituras de contadores e marcas em placas fotográficas.




Esse ponto de vista tem uma longa história. O Cardeal Bellarmine insistiu que Galileu considerasse o sistema Copérnico como um meio conveniente de “salvar as aparências”, uma boa maneira de fazer cálculos para determinar onde os planetas apareceriam no céu. Galileu não deveria pensar que a Terra de fato girava em torno do sol – ao contrário, deveria considerar que Copérnico usou tal pressuposição apenas como um dispositivo útil de cálculo. Essa oferta para preservar o prestígio não teve apelo algum junto a Galileu, nem sugestões semelhantes foram favoravelmente recebidas pelos cientistas em geral. Se a ciência só deve se ocupar em correlacionar dados, e não em nos dizer como o mundo físico realmente é, fica difícil admitir que a empreitada vale todo o tempo, o trabalho e o talento gastos para executá-la. Seus feitos pareceriam escassos demais para justificar tal grau de envolvimento. Além disso, a explicação mais natural da capacidade que uma teoria tem de salvar as aparências com certeza seria que ela apresenta alguma correspondência com o modo como as coisas são.




Ainda assim, 

Niels 

Bohr geralmente aparentava estar falando sobre a teoria quântica de uma ótica positivista. Uma vez, ele escreveu a um amigo que 




O mundo quântico não existe. Há apenas uma descrição física quântica abstrata. É errado pensar que a tarefa da física é descobrir como a natureza 

é

. A física lida com o que podemos dizer sobre a natureza.




A preocupação de 

Bohr com o papel de aparelhos clássicos de 

medição poderia ser vista como um incentivo a esse ponto de vista de conotação positivista. Vimos que, em seus últimos anos, ele ficou muito preocupado com questões filosóficas, escrevendo extensamente sobre elas. O material resultante tem interpretação árdua. O dom de 

Bohr em assuntos filosóficos estava muito aquém de seu extraordinário talento como físico. Além disso, ele acreditava – e exemplificava – que há dois tipos de verdade: um tipo trivial, que pode ser articulado de modo claro, e um tipo profundo, sobre o qual só se pode falar nebulosamente. Com certeza, o conjunto de seus escritos tem sido interpretado de várias formas pelos estudiosos. Alguns sentiram que havia, de fato, um tipo de 

realismo qualificado ao qual 

Bohr era adepto.




Para os realistas, o papel da ciência é descobrir como o mundo físico realmente é. Essa é uma tarefa que nunca será totalmente cumprida. Novos regimes físicos (encontrados em energias ainda maiores, por exemplo) sempre estarão aguardando investigação e podem vir a demonstrar características inesperadas em seu comportamento. Uma avaliação honesta das realizações da física pode, no máximo, alegar verossimilhança (uma representação precisa de uma variedade ampla, mas circunscrita, de fenômenos), e não verdade absoluta (uma representação total da realidade física). Os físicos são os cartógrafos do mundo físico, buscando teorias que sejam adequadas em uma escala escolhida, porém incapazes de descrever todos os aspectos do que está acontecendo. Uma visão filosófica desse tipo vê as conquistas da ciência física como a firme compreensão de uma realidade real. O mundo dos realistas está povoado de elétrons e fótons, quarks e glúons.




Um tipo de meio-termo entre 

positivismo e 

realismo é oferecido pelo 

pragmatismo, a posição filosófica que reconhece o fato tecnológico de que a física nos permite realizar as coisas, mas que não vai tão longe quanto a posição realista de pensar que sabemos como o mundo realmente é. Um pragmático poderia dizer que devemos levar a ciência a sério, mas não a ponto de acreditar nela. Ainda assim, de longe a explicação mais óbvia do sucesso tecnológico da ciência é seguramente por estar baseada no entendimento verossimilhante da maneira como a matéria realmente se comporta.




Uma série de defesas do 

realismo científico pode ser organizada. Uma, já observada, é que ele oferece uma compreensão natural dos sucessos preditivos da física e de sua fecundidade a longo prazo, assim como do trabalho confiável dos muitos dispositivos tecnológicos construídos à 

luz de sua imagem do mundo físico. O 

realismo também explica por que o empenho científico é visto como algo digno, atraindo a devoção vitalícia de muitas pessoas de bastante talento, pois é uma atividade que gera conhecimento real de como as coisas são. O 

realismo corresponde à convicção de cientistas de que experimentam a criação de descobertas e de que não são só maneiras melhores de fazer os cálculos, ou simplesmente um acordo tácito entre eles próprios de ver as coisas dessa forma. Essa convicção de descoberta origina-se intensamente da frequente experiência da recalcitrância exibida pela natureza diante da expectativa prévia do cientista. O físico pode abordar fenômenos com determinadas ideias em mente apenas para descobrir que elas são negadas pelo modo real com que se verifica o comportamento do mundo físico. A natureza força a reconsideração sobre nós, e isso geralmente motiva a eventual descoberta do caráter totalmente inesperado do que está acontecendo. O surgimento da teoria quântica é, sem dúvida, um exemplo notável do revisionismo imposto pela realidade física sobre o pensamento do cientista.




Se a teoria quântica está, de fato, dizendo-nos como o mundo subatômico realmente é, então sua realidade é algo muito diferente da objetividade ingênua com a qual podemos abordar o mundo dos objetos cotidianos. Essa foi a questão que 

Einstein considerou tão difícil de aceitar. Ele acreditava apaixonadamente na realidade do mundo físico, mas rejeitava a teoria quântica convencional porque supunha, de modo equivocado, que apenas o objetivo poderia ser o real.




A realidade quântica é nebulosa e intermitente por natureza. O filósofo e físico francês Bernard 

d’Espagnat falou de sua natureza como sendo “velada”. O pensador mais verdadeiramente filosófico dos fundadores da teoria quântica foi Werner 

Heisenberg. Ele acreditava que seria valioso tomar emprestado de Aristóteles o conceito de 

potentia 

e uma vez escreveu que




Em experimentos sobre eventos atômicos, precisamos lidar com coisas que são fatos, com fenômenos que são tão reais quanto qualquer fenômeno na vida diária. Mas os átomos ou partículas elementares não são tão reais; eles formam um mundo de potencialidades ou possibilidades, em vez de coisas ou fatos.




Um elétron não possui, o tempo todo, uma posição definitiva ou um momentum definitivo; em vez disso, ele possui a potencialidade de exibir um ou outro se uma 

medição transformar a potencialidade em realidade. Eu discordaria do raciocínio de 

Heisenberg de que esse fato torna um elétron “não tão real” quanto uma mesa ou uma cadeira. O elétron simplesmente goza de um tipo distinto de realidade, apropriado à sua natureza. Se formos conhecer as coisas como elas são, deveremos estar preparados para conhecê-las como realmente são, em seus próprios termos, por assim dizer.




Por que quase todos os físicos querem insistir na realidade, adequadamente compreendida, dos elétrons? Acredito que seja porque a pressuposição de que existem elétrons, com todas as sutis propriedades quânticas que os acompanham, torna inteligíveis grandes faixas de experiência física que, do contrário, pareceriam nebulosas. Ela explica as propriedades de condução dos metais, as propriedades químicas dos átomos, nossa capacidade de construir microscópios de elétrons e muito mais. É a 

inteligibilidade

 (e não a objetividade) que é a pista para a realidade – uma convicção que, incidentalmente, está em conformidade com uma tradição 

metafísica derivada do pensamento de Tomás de Aquino.




A realidade velada que caracteriza a essência da natureza dos elétrons é representada em nosso pensamento pelas funções de onda associadas a eles. Quando um físico pensa no que um elétron está “fazendo”, é a 

função de onda apropriada que está sendo considerada. É evidente que a 

função de onda não é uma entidade tão acessível quanto a presença objetiva de uma bola de bilhar, mas ela também não funciona no pensamento quântico de um modo que nos deixe confortáveis com a noção positivista de que é unicamente um dispositivo para fazer cálculos. A função um tanto fantasmagórica de onda parece um veículo apropriado para ser a portadora da potencialidade velada da realidade quântica.




Racionalidade




Se o estudo de física quântica tem algo a ensinar, é que o mundo está repleto de surpresas. Ninguém teria suposto de antemão que poderia haver entidades que, algumas vezes, se comportassem como se fossem 

ondas e, em outras, como se fossem partículas. Essa compreensão foi forçada sobre a comunidade física pela necessidade intransigente de experiência empírica verdadeira. Como 

Bohr disse uma vez, o mundo não é apenas mais estranho do que pensamos; ele é mais estranho do que poderíamos imaginar. Observamos anteriormente que inclusive a lógica precisa ser modificada quando aplicada ao mundo quântico.




Um slogan para a física quântica poderia muito bem ser: “Abaixo a tirania indevida do senso comum”. Esse comovente lema transmite uma mensagem de maior relevância do que para o domínio quântico apenas. Ele nos lembra de que nossos poderes de previsão racional são bastante míopes. A questão instintiva que um cientista deve formular sobre uma explicação proposta de algum aspecto da realidade, seja na ciência, seja além dela, não é “isso é razoável?”, como se sentíssemos que sabemos de antemão que forma a razão deverá assumir. Em vez disso, a pergunta adequada é: “O que faz você pensar que isso deveria ser assim?”. Esta é uma questão muito mais aberta, sem afastar a possibilidade de surpresa radical, mas insistindo que deve haver um suporte de evidências para o que está sendo afirmado.




Se a teoria quântica nos incentiva a manter flexível nossa concepção do que é razoável, ela também nos incentiva a reconhecer que não existe 

epistemologia universal, nenhuma maneira soberana pela qual podemos esperar obter todo o conhecimento. Embora possamos conhecer o mundo cotidiano em sua realidade newtoniana, só poderemos conhecer o mundo quântico se estivermos preparados para aceitá-lo em sua incerteza heisenbergiana. A insistência em uma representação ingenuamente objetiva dos elétrons leva apenas ao fracasso. Há um tipo de círculo epistemológico: como sabemos que uma entidade deve se conformar à natureza dessa entidade, a natureza da entidade é revelada por meio do que sabemos sobre ela. Não há como escapar dessa delicada circularidade. O exemplo da teoria quântica incentiva a crença de que o círculo pode ser benigno, e não vicioso.




Critérios metafísicos




Teorias físicas exitosas devem, com o tempo, conseguir exibir sua capacidade de ajuste aos fatos experimentais. O objetivo final de salvar as aparências é um empreendimento necessário, embora possa haver alguns períodos intermediários de dificuldade no caminho que conduz a esse fim (tal como quando 

Dirac a princípio se deparou com a aparentemente desastrosa previsão em termos empíricos dos estados negativos de energia do elétron). De especial persuasão será a propriedade de fertilidade sustentada, à medida que uma teoria se mostrar capaz de prever ou oferecer uma compreensão de fenômenos novos e inesperados (a explicação de 

Dirac das propriedades magnéticas dos elétrons e sua previsão do pósitron).




Ainda assim, esses sucessos empíricos não são, por si próprios, critérios suficientes para que a comunidade científica endosse uma teoria. A escolha entre uma interpretação indeterminista da teoria quântica e uma interpretação determinista não pode ser feita com base nesses fundamentos. 

Bohm salva as aparências, assim como 

Bohr. A questão entre eles precisa ser resolvida por outros motivos. Ocorre que a decisão depende de julgamento metafísico, e não apenas de medições físicas.




Os critérios metafísicos que a comunidade científica leva muito a sério na avaliação do peso a ser colocado em uma teoria incluem:




1. Escopo




A teoria deve tornar inteligível a maior variação possível de fenômenos. No caso de 

Bohr e 

Bohm, esse critério não leva a uma solução da questão entre eles em função da equivalência empírica dos dois conjuntos de resultados (embora se deva observar que o pensamento bohmiano precisa concluir sua representação com melhores argumentos para substanciar sua crença de que as probabilidades iniciais são dadas corretamente por um cálculo de 

função de onda). 




2. Economia




Quanto mais concisa e parcimoniosa for uma teoria, mais atraente ela parecerá. A teoria de 

Bohm tem pontuação menor aqui por causa de sua pressuposição da onda oculta, além das partículas 

observáveis. Essa multiplicação de entidades certamente é vista por muitos físicos como uma característica atraente da teoria.




3. Elegância




Essa é uma noção à qual se pode adicionar a propriedade de 

naturalidade

, que resulta da falta de artifícios indevidos. É nesse ponto que a maioria dos físicos encontra a principal dificuldade com as ideias bohmianas. Em especial, a apropriação 

ad hoc

, porém necessária, da 

equação de Schrödinger como a equação para a onda 

bohmiana tem um ar oportunista nada atraente.




Esses critérios não estão fora da própria física, mas também são do tipo que tornam sua avaliação uma questão de julgamento pessoal. Que eles sejam satisfeitos não é uma questão que pode ser reduzida à obediência de um protocolo formalizado. Não é um julgamento cuja avaliação pode ser delegada a um computador. O veredito da maior parte da comunidade da física quântica em favor de 

Bohr e contra 

Bohm é um exemplo paradigmático do que o filósofo da ciência, Michael 

Polanyi, teria chamado de papel do “conhecimento pessoal” na ciência. 

Polanyi, que fora um químico renomado antes de se voltar para a filosofia, enfatizava que, embora o objeto de estudo da ciência seja o mundo físico impessoal, a atividade de praticar ciência é inevitavelmente uma atividade de pessoas. Isso se dá porque envolve muitos atos de julgamento que exigem o exercício de habilidades tácitas que só podem ser adquiridas por pessoas que passaram por uma longa aprendizagem na comunidade de cientistas em busca da verdade. Esses julgamentos não se referem apenas à aplicação do tipo de critérios metafísicos que estivemos discutindo. Em um nível mais cotidiano, eles incluem habilidades como a capacidade do pesquisador de avaliar e eliminar efeitos artificiais “de segundo plano” que poderiam, ao contrário, contaminar os resultados de um experimento. Não existe um manual que diga ao pesquisador como fazer isso. Trata-se de algo aprendido com a experiência. Em uma frase que 

Polanyi repetia com frequência, nós todos “sabemos mais do que podemos dizer”, o que se manifesta nas habilidades tácitas de andar de bicicleta, no conhecimento de vinhos ou no projeto e execução de experimentos físicos exitosos.




Holismo




Vimos no Capítulo 5 que o 

efeito EPR mostra que há uma não localidade intrínseca presente no mundo quântico. Também vimos que o fenômeno da 

descoerência simplificou os efeitos incrivelmente potentes que o ambiente geral pode exercitar sobre as entidades quânticas. Embora a física quântica seja a física das coisas muito pequenas, ela não endossa uma representação da realidade puramente atomística, de “quinquilharias”.




A física não determina a 

metafísica (a visão de mundo mais ampla), mas certamente a limita, mais ou menos como a fundação de uma casa limita, mas não determina completamente o edifício que será construído sobre ela. O pensamento filosófico nem sempre levou em consideração de modo adequado as implicações desses aspectos holísticos da teoria quântica. Não pode haver dúvida de que incentivam a aceitação da necessidade de obter uma representação do mundo natural que tenha sucesso em reconhecer que seus blocos de construção são, de fato, partículas elementares, mas também que sua combinação dá origem a uma realidade mais integrada do que uma simples imagem constituinte sozinha poderia sugerir.




O 

papel do observador




Um clichê que é repetido com frequência é que a teoria quântica é “criada pelo observador”. Um raciocínio mais cuidadoso qualificará e reduzirá consideravelmente essa alegação. O que pode ser dito dependerá essencialmente de qual interpretação do processo de 

medição será escolhida. Essa é a questão central porque, entre as medições, a 

equação de Schrödinger prescreve que um sistema quântico evolui de forma perfeitamente contínua e determinada. Também é importante lembrar que a definição geral de 

medição é o registro macroscópico irreversível do sinal de um estado microscópico das coisas. Esse acontecimento pode envolver um observador, mas, via de regra, isso não é necessário.




Apenas a interpretação da 

consciência atribui um papel único aos atos de um observador consciente. Todas as outras interpretações ocupam-se exclusivamente dos aspectos do processo físico, sem apelo à presença de uma pessoa. Mesmo na interpretação da 

consciência, o 

papel do observador está confinado a fazer a escolha consciente do que será medido e, a seguir, trazer à tona, de modo inconsciente, qual resultado realmente acabou sendo revelado. A realidade só pode ser transformada nos limites da potencialidade quântica já presente.




Na visão neo-Copenhague, o investigador escolhe o aparelho a ser usado e o que deve ser medido, e então o resultado é decidido dentro desse aparelho por processos físicos macroscópicos. Se, ao contrário, é a nova física da 

GRW que está sendo aplicada, é o processo aleatório que produz o resultado real. Se a teoria 

bohmiana estiver correta, o 

papel do observador consistirá simplesmente na função clássica de ver o que já é, inequivocamente, a situação. Na interpretação dos muitos mundos, é o observador que sofre a ação da realidade física, sendo clonado para aparecer em todos os universos paralelos, em cujo enorme portfólio todos os resultados possíveis são realizados em algum lugar.




Nenhum fator comum une essas diferentes representações possíveis do 

papel do observador. No máximo, pareceria apropriado falar apenas da “realidade influenciada pelo observador” e evitar falar da “realidade criada pelo observador”. O que já não era, em algum sentido, potencialidade presente nunca poderia ser trazido à existência.




Em associação a esse assunto, devemos também questionar a afirmativa, geralmente feita em conjunto com paralelos alegados ao conceito da 

maya

 no pensamento oriental, de que o mundo quântico é um “mundo dissolvente” de insubstancialidade. Esse é um tipo de meia-verdade. Existe a característica quântica “velada” que já discutimos, junto à função amplamente reconhecida que a potencialidade exerce na compreensão quântica. Ainda assim, também há aspectos persistentes do mundo quântico que precisam igualmente ser levados em consideração. Grandezas físicas como energia e momentum são conservadas na teoria quântica, tal como ocorre na física clássica. Lembre também que um dos triunfos iniciais da 

mecânica quântica foi explicar a estabilidade dos átomos. O princípio da exclusão quântica reforça a estrutura fixa da tabela periódica. De forma alguma o mundo quântico inteiro dissolve-se em evanescência.




Moda quântica




Parece apropriado encerrar este capítulo com um aviso de saúde intelectual. É evidente que a teoria quântica é estranha e surpreendente, mas não é tão esquisita a ponto de permitir que “vale tudo”. É evidente que ninguém argumentaria com tamanha crueza, mas existe um tipo de discurso que pode perigosamente se aproximar da adoção dessa atitude caricata. Pode-se chamar isso de “moda quântica”. Quero sugerir que se mantenha a sobriedade ao se fazer um apelo à percepção quântica.




Vimos que o 

efeito EPR não oferece uma explicação da telepatia, pois seu grau de emaranhamento mútuo não facilitaria a transferência de informações. É possível que os processos quânticos no cérebro tenham alguma conexão com a existência da mente consciente humana, mas a incerteza subatômica aleatória é, de fato, muito distinta do exercício do livre-arbítrio de um agente. A 

dualidade onda-partícula é um fenômeno bastante surpreendente e instrutivo, cujo caráter aparentemente paradoxal foi solucionado para nós por meio das observações da 

teoria quântica de campos. Porém, ela não nos concede uma licença para ceder ao desejo de adotar qualquer par de noções de aparência contraditória que nos vier à imaginação. Como uma droga potente, a teoria quântica é maravilhosa quando aplicada de modo correto, mas desastrosa quando abusada e mal-aplicada.

Leituras complementares




Há um grande número de livros referentes à teoria quântica. A lista a seguir é uma breve seleção pessoal que pode ser útil ao leitor que esteja em busca de maiores informações.




Livros que usam mais matemática do que este, mas que ainda têm uma linguagem ascessível:




Hey, T.; Walters P.

 

The quantum universe

. Cambridge: Cambridge University Press, 1987.




Polkinghorne, J.C

. 

The quantum world.

 Penguin, 1990.




Rae, M

. 

Quantum physics: illusion or reality?

 

Cambridge: Cambridge University Press, 1986.




Um livro que usa matemática em nível profissional e que se ocupa muito mais de questões interpretativas do que é comum em livros-texto:




Isham, C.J

. 

Lectures on quantum theory: mathematical and structural foundations

. 

Imperial College Press, 1995.




A exposição clássica por um dos fundadores do tema:




Dirac, P.A.M

. 

The principles of quantum mechanics

. 4.ed. Oxford: Oxford University Press, 1958.




Uma discussão filosoficamente sofisticada de questões interpretativas:




D’espagnat, B

. 

Reality and the physicist: knowledge, duration and the quantum world

. 

Cambridge: Cambridge University Press, 1989.




Uma introdução mais geral a questões da filosofia da ciência:




Newton-Smith, W.H

. 

The rationality of science

.

 

Routledge and Kegan Paul, 1981.




Porém, 

Newton-Smith despreza o pensamento de Michael 

Polanyi, o que pode ser encontrado em:




Polanyi, M.

 

Personal knowledge

.

 

Routledge and Kegan Paul, 1958.




Livros de relevância especial à versão 

bohmiana da teoria quântica:




Bohm, D.; Hiley, B

. 

The undivided universe. 

Routledge, 1993.




Cushing, J. T

. 

Quantum mechanics: historical contingency and the Copenhagen hegemony

. Chicago: University of Chicago Press, 1994.




Escritos reflexivos por duas figuras fundadoras:




Bohr, N

. 

Atomic physics and human knowledge. 

Wiley, 1958.




Heisenberg, W

. 

Physics and philosophy: the revolution in modern science

.

 

Allen & Unwin, 1958.




Biografia de físicos quânticos eminentes:




Cassidy, D.C

. 

Uncertainty: the life and science of Werner 

Heisenberg

. W.H. Freeman, 1992.




Gleick, J

. 

Genius: the life and science of Richard 

Feynman

. Pantheon, 1992.




Kragh, H.S

. 

Dirac: a scientific biography

. Cambridge: Cambridge University Press, 1990.




Moore, W

. 

Schrödinger: life and thought

. Cambridge: Cambridge University Press, 1989.




Pais, A

. 

“Subtle is the Lord...”: the science and life of Albert 

Einstein

. Oxford: Oxford University Press, 1982.




_______. 

Niels 

Bohr’s times in physics, philosophy and polity

. 

Oxford: Oxford University Press, 1991.

Glossário




De modo geral, este glossário limita-se a definir termos que são recorrentes neste texto ou que são de especial significância para uma compreensão básica da teoria quântica. Outros termos que ocorrem apenas uma vez ou que apresentam importância menos essencial são definidos no próprio texto e podem ser acessados pelo índice remissivo.




bósons:

 partículas cujas 

funções de onda

 com várias partículas são simétricas.




caologia quântica:

 assunto ainda não compreendido totalmente sobre a mecânica quântica dos sistemas caóticos.




colapso do pacote de onda:

 mudança descontínua na 

função de onda

 ocasionada por um ato de medição.




complementaridade:

 o fato, bastante enfatizado por Niels Bohr, de que há maneiras distintas e mutuamente exclusivas pelas quais um sistema quântico pode ser considerado.




constante de Planck:

 nova constante física fundamental que determina a escala da teoria quântica.




descoerência:

 efeito ambiental sobre sistemas quânticos que é capaz de induzir rapidamente o comportamento quase clássico.




desigualdades de Bell:

 condições que deveriam ser satisfeitas em uma teoria que era estritamente local em sua natureza, sem correlações não locais.




dualidade onda-partícula:

 propriedade quântica segundo a qual as entidades se comportam, algumas vezes, de forma corpuscular e, em outras, de forma ondulatória.




efeito EPR:

 consequência contraintuitiva de que duas entidades quânticas que tenham interagido entre si retêm um poder de influência mútua por maior que seja a distância que as separa.




epistemologia:

 discussão filosófica acerca da significância do que podemos conhecer.




equação de Schrödinger:

 equação fundamental da teoria quântica que determina como a 

função de onda

 varia com o tempo.




estatística:

 comportamento de sistemas compostos de partículas idênticas.




fenômenos de interferência:

 efeitos que surgem da combinação de ondas, que podem resultar em reforço (ondas em fase) ou anulação (ondas fora de fase).




férmions:

 partículas cujas 

funções de onda

 com várias partículas são antissimétricas.




física clássica:

 teoria física determinista e passível de ser representada por imagens, do tipo que Isaac Newton descobriu.




física estatística:

 tratamento do comportamento físico de sistemas complexos com base em seus estados mais prováveis.




fórmula de Balmer: 

fórmula simples para as frequências de linhas proeminentes no espectro do hidrogênio.




função de onda: 

representação matemática mais útil de um estado na teoria quântica. É uma solução da

 equação de Schrödinger.




graus de liberdade:

 as diferentes maneiras independentes pelas quais um sistema dinâmico pode mudar no decorrer de seu movimento.




interpretação de Copenhague: 

família de interpretações da teoria quântica derivadas de Niels Bohr que destacam a indeterminação e o papel de aparelhos clássicos na medição. 




interpretação dos muitos mundos:

 interpretação da teoria quântica segundo a qual todos os resultados possíveis de medição são realmente realizados em mundos paralelos distintos.




momentum angular: 

grandeza dinâmica que é a medida do movimento rotatório.




não comutação:

 propriedade de que a ordem da multiplicação importa, de forma que AB não é o mesmo que BA.




observáveis:

 grandezas que podem ser medidas de modo experimental.




ontologia: 

discussão filosófica sobre a natureza do ser.




positivismo:

 posição filosófica de que a ciência deve ocupar-se unicamente com a correlação direta de fenômenos observados.




pragmatismo:

 posição filosófica de que a ciência, na verdade, tem a ver com a capacidade técnica de realizar as coisas.




princípio da incerteza:

 fato de que, na teoria quântica, os 

observáveis

 podem ser agrupados em pares (como posição e momentum, tempo e energia), de modo que os dois membros do par não podem ser medidos simultaneamente com precisão exata. A escala do limite de exatidão simultânea é determinada pela

 constante de Planck.




princípio de exclusão:

 condição de que dois 

férmions

 (como dois elétrons) não podem estar no mesmo estado.




problema da medição:

 controversa questão na interpretação da teoria quântica referente a como se deve entender a obtenção de um resultado definitivo em cada ocasião de medição.




quarks e glúons:

 candidatos atuais para os constituintes básicos da matéria nuclear.




radiação:

 energia carregada pelo campo eletromagnético.




realismo:

 posição filosófica de que a ciência explica como o mundo físico realmente é.




sobreposição:

 princípio fundamental da teoria quântica que permite a adição de estados que, na 

física clássica

, seriam imiscíveis.




spin:

 o 

momentum angular

 intrínseco às partículas elementares.




teoria do caos:

 física de sistemas cuja extrema sensibilidade a detalhes de circunstância torna seu comportamento futuro intrinsecamente imprevisível.




teoria bohmiana: 

interpretação determinista da teoria quântica proposta por David Bohm.




teoria quântica de campos:

 aplicação da teoria quântica a campos, como o campo eletromagnético ou o campo associado aos elétrons.




variáveis ocultas:

 quantidades não 

observáveis que ajudam a fixar o que acontece de fato em uma interpretação determinista da teoria quântica.

Apêndice matemático




Exponho de modo conciso alguns detalhes matemáticos que esclarecerão, para os que desejarem aproveitá-los, vários pontos que surgem no texto principal, que não se aprofunda na questão matemática. (Há uma referência aos itens no texto, indicada pelos números de suas seções.) As exigências que este apêndice faz dos leitores variam da capacidade de manejar equações algébricas ao conhecimento elementar da notação do cálculo.




1. A fórmula de 

Balmer




É mais útil apresentá-la em sua forma levemente modificada, conforme escrita por Rydberg. Se 

v

n

 é a frequência da enésima linha no espectro visível do hidrogênio (

n

 assumindo os valores inteiros, 3, 4,...), então




 




vn=cR 

 1 _ 1  , 




onde 

c

 é a velocidade da 

luz e 

R

 é uma constante chamada de Rydberg. Tal expressão da fórmula, como a diferença dos dois termos, acabou revelando-se uma jogada perspicaz (veja o item 3). Outras séries de linha espectral em que o primeiro termo é 1/1

2

, 1/3

2

 etc., foram identificadas posteriormente.




2. O 

efeito fotoelétrico




Segundo 

Planck, a radiação eletromagnética oscilando 

v

 vezes por segundo é emitida em 

quanta de energia 

hv

, onde 

h

 é a 

constante de 

Planck e tem o minúsculo valor de 6,63.10

−34

 joule-segundo. (Se substituirmos 

v

 pela frequência angular 

ω 

= 2

πν

, a fórmula torna-se 

ħω

, onde 

ħ 

= 

h

/2

π

, também chamado de 

constante de 

Planck e pronunciado “agá cortado”.)




Einstein supôs que esses 

quanta tinham existência permanente. Se a radiação caísse sobre um metal, um dos elétrons no metal poderia absorver um quantum, adquirindo com isso sua energia. Se a energia necessária para o elétron escapar do metal fosse 

W

, então essa fuga ocorreria se 

hν 

> 

W

, mas seria impossível se 

hν 

< 

W

. Portanto, havia uma frequência (

ν

o

 = 

W

/

h

) abaixo da qual nenhum elétron poderia ser emitido, por mais intenso que o feixe de radiação incidente pudesse ser. Acima dessa frequência, alguns elétrons seriam emitidos, mesmo que o feixe fosse bastante fraco.




Uma teoria ondulatória pura da radiação resultaria em um comportamento totalmente diferente, uma vez que seria esperado que a energia transmitida aos elétrons dependesse da intensidade do feixe, mas não de sua frequência.




As propriedades observadas a partir da emissão fotoelétrica estão em conformidade com as previsões da imagem corpuscular, e não com a imagem ondulatória.




3. O átomo de 

Bohr




Bohr supôs que o átomo de hidrogênio consiste de um elétron de carga -

e

 e massa 

m

 se deslocando em um círculo ao redor de um próton de carga 

e

. A sua massa é grande o bastante (1.836 vezes a massa do elétron) para que o efeito de seu movimento seja desprezado. Se o raio do círculo é 

r

 e a velocidade do elétron é 

v

, então o equilíbrio entre atração eletrostática e aceleração centrífuga resulta em




A energia do elétron é a soma de sua energia cinética e a energia potencial eletrostática




que, usando (3.1), pode ser representada como




Bohr impôs uma nova condição quântica, exigindo que o momentum angular do elétron tivesse de ser um múltiplo inteiro da 

constante de 

Planck 

ħ,




mvr 

= 

nħ 

(

n 

= 1,2,...).  




Assim, as energias possíveis correspondentes são




Se a energia liberada quando um elétron se move do estado 

n

 para o estado 2 for emitida como um único 

fóton, a frequência desse 

fóton será




Essa é exatamente a fórmula de 

Balmer (1.1). Além de 

Bohr explicar essa fórmula, ele possibilitou que a constante 

R

 de Rydberg fosse calculada em termos de outras constantes físicas conhecidas, 




um número que estava de acordo com o valor conhecido de modo experimental. A descoberta de 

Bohr representou um triunfo notável para a nova maneira quântica de pensar.




(No cálculo adequado do átomo de hidrogênio com a 

mecânica quântica, usando-se a 

equação de Schrödinger (veja o item 6), os níveis de energia discreta surgem de uma forma um tanto distinta, tendo alguma analogia com as frequências harmônicas de uma corda aberta, e o número 

n

 é mais indiretamente relacionado ao momentum angular.)




4. Operadores não comutativos




As matrizes que 

Heisenberg empregou em geral não comutam entre si, mas por fim verificou-se que a teoria quântica exigia uma generalização adicional em que os 

operadores diferenciais não comutativos fossem incorporados no formalismo. Esse é o desenvolvimento que levou os físicos a usar a matemática do 

espaço de Hilbert.




No caso geral, as fórmulas da 

mecânica quântica podem ser obtidas a partir daquelas da física clássica, fazendo-se as seguintes substituições para posição 

x

 e momentum 

p

:




Devido à aparência do operador diferencial ∂/∂

x 

em (4.1), as variáveis 

x

 e 

p

 não comutam entre si, em contraste com a propriedade de 

comutação que trivialmente se aplica aos números que a física clássica atribui a posições e momenta. Quando ∂/∂

x 

está à esquerda, ele diferencia o 

x

 à direita, bem como qualquer outra entidade à direita, de forma que podemos escrever




Definindo o colchete comutador [

p,x

] = 

p.x 

− 

x.p

, podemos reescrever isso como




 [

p

,

x

] = − 

iħ

. 




Essa relação é conhecida como 

condição de quantização

. Um leitor atento perceberá que outra solução de (4.3) seria dada por




Dirac deu ênfase especial ao fato de que há muitos modos equivalentes de formular a 

mecânica quântica.




5. Ondas de 

De Broglie




A fórmula de 

Planck




E 

= 

hν




torna a energia proporcional ao número de vibrações por unidade de intervalo de 

tempo

. A teoria da relatividade engloba espaço e tempo, momentum e energia como combinações quádruplas naturais. Portanto, o jovem 

De Broglie propôs que, na teoria quântica, o momentum deveria ser proporcional ao número de vibrações por unidade de intervalo de 

espaço

. Isso nos leva à fórmula




onde 

λ

 é o comprimento de onda. As equações (5.1) e (5.2) juntas oferecem uma maneira de relacionar propriedades corpusculares (

E

 e 

p

) e propriedades ondulatórias (

v

 e 

λ

). A dependência espacial de uma forma de onda de comprimento de onda 

λ 

é dada por




e

i

2

πx

/

λ




A combinação de (4.1) e (5.3) recupera (5.2).




6. A 

equação de Schrödinger




A energia de uma partícula é a soma de sua energia cinética (½ 

mv

2

 = ½ 

p

2

/

m

, onde 

p 

é 

mv

) e sua energia potencial (que, em geral, podemos representar como função de 

x

, 

V

(

x

)). A relação de 

mecânica quântica entre energia e tempo que é análoga a (4.1) é




A diferença de sinais entre (6.1) e (4.1) deve-se ao fato de que a dependência do tempo de uma forma de onda movendo-se para a direita e correspondendo à dependência espacial (5.3) é




e

−

i

2

πνt

, 




de modo que o sinal de mais em (6.1) é necessário para resultar em 

E 

= 

hν

.




Usando (4.1) e (6.1) para transformar 

E 

= ½ 

mv

2

 + 

V 

em uma equação diferencial para a 

função de onda da 

mecânica quântica 

ψ

, temos




em uma dimensão espacial e




no espaço tridimensional do vetor 

x 

= (

x, y, z

), onde




Essas expressões são a 

equação de Schrödinger, escrita pela primeira vez por ele com base em uma linha de argumentação um tanto diferente. O operador entre colchetes nas equações (6.3) é chamado de 

hamiltoniano

.




Observe que as equações (6.3) são 

equações lineares

 em 

ψ

, o que equivale a dizer que, se 

ψ

1

 e 

ψ

2

 são duas soluções, então




λ

1

ψ

1

 + 

λ

2

ψ

2

, 




também é para qualquer par de números 

λ

1

 e 

λ

2

.




Max Born enfatizou que a 

função de onda tem a interpretação de representar uma onda de 

probabilidade. A 

probabilidade de encontrar uma partícula no ponto 

x

 é proporcional ao quadrado do módulo da 

função de onda correspondente (complexa).




7. Espaços lineares




A propriedade de linearidade no final do item 6 é uma característica essencial da teoria quântica e a base para o 

princípio da sobreposição. 

Dirac generalizou essas ideias com base em funções de onda, formulando a teoria em termos de 

espaços vetoriais abstratos.




Um conjunto de vetores |

α

i 

〉

 forma um 

espaço vetorial

 se qualquer combinação deles




λ

1

 | 

α

1

 

〉

 + 

λ

2

 | 

α

2

 

〉

 + . . ., 




também pertencer ao espaço, onde 

λi

 são números arbitrários (complexos). 

Dirac chamou esses vetores de “kets”. Eles são as generalizações das funções de onda 

ψ

 de Schrödinger. Também existe um espaço dual de “bras”, antilinearmente relacionados aos kets




onde os 

λ

*

i

 

são os conjugados complexos dos 

λ

i

. (O bras 

〈

 

α 

| obviamente corresponde às funções de onda complexas conjugadas, 

ψ

*.) Um produto escalar pode ser formado entre um bra e um ket (resultando em um “bra(c)ket” – 

Dirac apreciava esse jogo de palavras, já que “bracket” em inglês significa 

colchete

). Isso corresponde, em termos de 

função de onda, à integral ʃ

ψ

*

1

ψ

2

d

x

. Ela é denotada por 

〈

 

α

1

 | 

α

2

 

〉

 

e tem a propriedade que




〈

α

1

 | 

α

2

〉

 = 

〈

α

2

 | 

α

1

 

〉

*.




Segue-se de (7.3) que 

〈

 

α 

| 

α 

〉

 

é um número real e, na verdade, na teoria quântica impõe-se a condição de que ele seja positivo (deve corresponder a |

ψ

|

2

).




A relação entre um estado físico e um ket é o que se chama de 

representação de raios

, o que significa que | 

α 

〉

 e 

λ 

| 

α 

〉

 representam o mesmo estado físico para qualquer número complexo 

λ

 não zero.




8. Autovetores e 

autovalores




Os 

operadores em 

espaços vetoriais são definidos por seu efeito de transformar kets em outros kets:




O

| 

α

〉

 = | 

α

′

〉

. 




Na teoria quântica, os 

operadores indicam o modo como as grandezas 

observáveis são representadas no formalismo (compare com os 

operadores (4.1) que atuam sobre uma 

função de onda). Expressões significativas são os números que surgem como “sanduíches” bra-operador-ket (chamados de “elementos matriciais”; eles são relacionados a 

amplitudes de 

probabilidade):




〈

β 

|

O

| 

α 

〉

. 




O 

conjugado hermitiano

 de um operador, 

O

†, é definido pela relação entre elementos matriciais:




〈

β 

|

O

| 

α

〉

 = 

〈

α 

|

O

†

 | 

β 

〉

*. 




Uma significância especial se anexa aos 

operadores que têm seu próprio conjugado hermitiano:




O

†

 = 

O

. 




Eles são chamados de 

hermitianos

, e somente esses 

operadores representam entidades fisicamente 

observáveis.




Como os resultados de observações reais são sempre números reais, para que esse esquema tenha sentido físico, é preciso haver uma maneira de associar números e 

operadores, o que se consegue usando as ideias de 

autovetores

 e 

autovalores

. Se um operador 

O 

transforma um ket |

α 

〉

 em um múltiplo numérico de si mesmo,




O

| 

α

〉

 = 

λ 

| 

α 

〉

, 




então se diz que |

α

〉

 

é um autovetor de 

O

 com autovalor 

λ

. Pode-se demonstrar que os 

autovalores de 

operadores hermitianos são sempre números reais.




A interpretação física correspondente a esses fatos matemáticos é que os 

autovalores reais de um observável são os resultados possíveis que podem ser obtidos pela 

medição desse observável, enquanto os 

autovetores associados correspondem aos estados físicos em que aquele determinado resultado será obtido com certeza (

probabilidade um). Somente dois 

observáveis cujos 

operadores correspondentes comutam serão simultaneamente mensuráveis.




9. As relações de incerteza




A discussão do microscópio de raios gama demonstrou que a 

medição quântica força sobre o observador um acordo entre boa resolução espacial (comprimento de onda curto) e pouco distúrbio (baixa frequência). A inserção desse equilíbrio em termos quantitativos leva às relações de incerteza de 

Heisenberg, em que se verifica que a incerteza na posição, Δ

x

, e a incerteza no momentum, Δ

p

, não podem ter um produto Δ

x 

. Δ

p 

cuja magnitude seja menor do que a ordem da 

constante de 

Planck 

ħ

.




10. Schrödinger e 

Heisenberg




Se 

H

 é o hamiltoniano (operador de energia), a leitura da 

equação de Schrödinger é




Se 

H

 não depende explicitamente do tempo, como em geral é o caso, (10.1) pode ser solucionado formalmente escrevendo




| 

α

, 

t

〉

 = 

e

−

iHt/ħ

 

| 

α

, 0

〉

. 




Todas as consequências físicas da teoria derivam das propriedades dos elementos matriciais da forma 

〈

α 

|

O

| 

β 

〉

. A representação explícita da dependência do tempo (10.2) resulta em




〈

α

, 0|

e

iHt/h

. 

O 

. 

e

−

iHt/h

 

| 

β

, 0

〉

. 




Associando os termos de uma maneira diferente, temos




〈

α

, 0|. 

e

iHt/ħ

Oe

−

iHt/ħ

 

. | 

β

, 0

〉




onde agora a dependência do tempo foi empurrada, por assim dizer, para um operador dependente do tempo




O

(

t

) = 

e

iHt/ħ

Oe

−

iHt/ħ

 




(10.5) pode, então, ser tratado como a solução da equação diferencial




Esse modo de pensar sobre a teoria quântica, em que a dependência do tempo é associada às 

observáveis do operador, e não aos estados, é exatamente como 

Heisenberg originalmente abordou o assunto. Assim, a discussão demonstrou a equivalência das abordagens dos dois maiores fundadores da teoria quântica, apesar da aparência inicial de que eles tenham tratado a questão de maneira bastante distinta.




11. Estatística




Se 1 e 2 são 

partículas idênticas e indistinguíveis, então |1, 2

〉

 e |2, 1

〉

 

devem corresponder ao mesmo estado físico. Devido à natureza de representação de raios do formalismo (veja o item 7), isso implica que




| 2, 1

〉

 = 

λ 

| 1, 2

〉




onde 

λ 

é o número. Porém, a troca de 1 e 2 duas vezes não equivale a mudança alguma e, por isso, deve restaurar exatamente o situação original. Portanto, deve ser verdade que




λ

2

 = 1, 




dando as duas possibilidades, 

λ 

= +1 (estatística de Bose) ou 

λ 

= −1 (estatística de Fermi).




12. A equação de 

Dirac




Na placa memorial em homenagem a Paul 

Dirac, na Abadia de Westminster, está inscrita a seguinte equação:




iγ

∂

ψ 

= 

mψ

. 




Trata-se de sua célebre equação de 

ondas relativistas para o elétron, escrita em uma notação de espaço-tempo quadridimensional e (usando as unidades físicas naturais à teoria quântica que determinam 

ħ

= 1). Os 

γ

s são matrizes de quatro por quatro e 

ψ

 é o que se chama de espinor de quatro componentes (2 estados (

spin) vezes 2 (elétron/pósitron)). Este é o ponto máximo em que podemos levar o assunto em um livro introdutório desse tipo; porém, seja no papel, seja na página ou em pedra na Abadia, o espectador deve ter a oportunidade de prestar homenagem ao que é uma das mais belas e profundas equações da física.

John Polkinghorne é físico e teólogo. Lecionou física matemática em Cambridge de 1968 a 1979. Em 1997, foi ordenado cavaleiro da Ordem do Império Britânico. É autor de 

The Quantum World

 (1986), 

The Faith of a Physicist

 (1994) e 

Science and Theology

 (1998), entre outros.




Texto de acordo com a nova ortografia.




Título original: 

Quantum Theory




Primeira edição na 

Coleção 

L

&

PM

 

Pocket

: dezembro de 2011




Tradução

: 

Iuri Abreu




Capa

: Ivan Pinheiro Machado.

 

Foto

: AGE/Other Images




Preparação

: Elisângela Rosa dos Santos




Revisão

: Viviane Borba Barbosa




C

ip

-Brasil. Catalogação na Fonte




Sindicato Nacional dos Editores de Livros, RJ










P832t




Polkinghorne, J. C., 1930-




    Teoria quântica / John Polkinghorne; tradução de Iuri Abreu. – Porto Alegre, RS: L&PM, 2016. 




   (Coleção L&PM POCKET Encyclopaedia; v. 985)




    Tradução de: 

Quantum Theory




    Apêndice




    Inclui bibliografia e índice




    ISBN 978.85.254.3695-5




    1. Teoria quântica. 2. Física quântica. I. Título. II. Série. 




11-6681.                        	CDD: 530.12




                                     	CDU: 530.145










© John Polkinghorne, 2002




Teoria quântica

 foi originalmente publicado em inglês em 2009.




Esta tradução é publicada conforme acordo com a Oxford University Press.




Todos os direitos desta edição reservados a L&PM Editores




Rua Comendador Coruja

,

 3

14, loja 9

 

– Floresta – 90220-180 




Porto Alegre – RS – Brasil / Fone: 51.3225-5777 – Fax: 51.3221.5380




Pedidos & Depto. comercial

: vendas@lpm.com.br




Fale conosco: 

info@lpm.com.br




www.lpm.com.br

Table of Contents







Agradecimentos










Prefácio










Capítulo 1: Rachaduras clássicas










Capítulo 2: O nascer da luz










Capítulo 3: Perplexidades obscuras










Capítulo 4: Desenvolvimentos adicionais










Capítulo 5: Conectividade










Capítulo 6: Lições e significados










Leituras complementares










Glossário










Apêndice matemático
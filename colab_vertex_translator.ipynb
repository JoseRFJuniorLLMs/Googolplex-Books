{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö BooksKDP - Tradutor Massivo com Vertex AI\n",
    "\n",
    "**Pipeline:**\n",
    "1. Clona reposit√≥rio do GitHub\n",
    "2. Filtra livros em EN/ES/RU\n",
    "3. Traduz massivamente para PT-BR usando Gemini\n",
    "4. Salva tradu√ß√µes e faz push para GitHub\n",
    "\n",
    "**Requisitos:** TPU habilitado no Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. SETUP INICIAL\n",
    "# ============================================\n",
    "\n",
    "!pip install -q google-cloud-aiplatform langdetect tqdm gitpython\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Optional, List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "print(\"‚úÖ Bibliotecas instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. AUTENTICA√á√ÉO VERTEX AI\n",
    "# ============================================\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Configure seu projeto\n",
    "PROJECT_ID = \"seu-projeto-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Modelo Gemini\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "# Configura√ß√£o para tradu√ß√£o\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=8192,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vertex AI configurado: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. CLONE DO REPOSIT√ìRIO\n",
    "# ============================================\n",
    "\n",
    "REPO_URL = \"https://github.com/JoseRFJuniorLLMs/Googolplex-Books.git\"\n",
    "REPO_DIR = \"/content/Googolplex-Books\"\n",
    "\n",
    "# Remove se j√° existe\n",
    "!rm -rf {REPO_DIR}\n",
    "\n",
    "# Clona\n",
    "!git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "\n",
    "# Configura git\n",
    "!git config user.email \"colab@vertex.ai\"\n",
    "!git config user.name \"Colab Vertex Translator\"\n",
    "\n",
    "print(f\"‚úÖ Reposit√≥rio clonado em {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. CONFIGURA√á√ïES\n",
    "# ============================================\n",
    "\n",
    "TXT_DIR = Path(REPO_DIR) / \"txt\"\n",
    "TRANSLATED_DIR = Path(REPO_DIR) / \"translated\"\n",
    "CACHE_DB = Path(REPO_DIR) / \"data\" / \"translation_cache.db\"\n",
    "\n",
    "# Criar diret√≥rios\n",
    "TRANSLATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DB.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Idiomas para traduzir\n",
    "TARGET_LANGUAGES = {'en', 'es', 'ru'}  # Ingl√™s, Espanhol, Russo\n",
    "\n",
    "# Processamento paralelo\n",
    "MAX_WORKERS = 5  # Requests paralelos ao Gemini\n",
    "CHUNK_SIZE = 3000  # Caracteres por chunk\n",
    "RATE_LIMIT_DELAY = 0.5  # Segundos entre requests\n",
    "\n",
    "print(f\"üìÅ TXT: {TXT_DIR}\")\n",
    "print(f\"üìÅ Translated: {TRANSLATED_DIR}\")\n",
    "print(f\"üåç Idiomas: {TARGET_LANGUAGES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. CACHE DE TRADU√á√ïES\n",
    "# ============================================\n",
    "\n",
    "class TranslationCache:\n",
    "    \"\"\"Cache SQLite para tradu√ß√µes.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self._init_db()\n",
    "    \n",
    "    def _init_db(self):\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS translations (\n",
    "                    hash TEXT PRIMARY KEY,\n",
    "                    original TEXT,\n",
    "                    translated TEXT,\n",
    "                    source_lang TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "    \n",
    "    def _hash(self, text: str) -> str:\n",
    "        return hashlib.sha256(text.encode()).hexdigest()[:32]\n",
    "    \n",
    "    def get(self, text: str) -> Optional[str]:\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cur = conn.execute('SELECT translated FROM translations WHERE hash = ?', (self._hash(text),))\n",
    "            row = cur.fetchone()\n",
    "            return row[0] if row else None\n",
    "    \n",
    "    def set(self, original: str, translated: str, lang: str):\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            conn.execute('''\n",
    "                INSERT OR REPLACE INTO translations (hash, original, translated, source_lang)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', (self._hash(original), original, translated, lang))\n",
    "\n",
    "cache = TranslationCache(CACHE_DB)\n",
    "print(\"‚úÖ Cache inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. FUN√á√ïES DE TRADU√á√ÉO\n",
    "# ============================================\n",
    "\n",
    "LANG_NAMES = {\n",
    "    'en': 'ingl√™s',\n",
    "    'es': 'espanhol', \n",
    "    'ru': 'russo',\n",
    "    'fr': 'franc√™s',\n",
    "    'de': 'alem√£o',\n",
    "    'it': 'italiano'\n",
    "}\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detecta idioma do texto.\"\"\"\n",
    "    try:\n",
    "        sample = text[:5000]\n",
    "        return detect(sample)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def create_chunks(text: str, max_chars: int = CHUNK_SIZE) -> List[str]:\n",
    "    \"\"\"Divide texto em chunks.\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if len(current) + len(para) < max_chars:\n",
    "            current += ('\\n\\n' if current else '') + para\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            current = para\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def translate_chunk(chunk: str, source_lang: str, retries: int = 3) -> str:\n",
    "    \"\"\"Traduz um chunk usando Gemini.\"\"\"\n",
    "    # Verifica cache\n",
    "    cached = cache.get(chunk)\n",
    "    if cached:\n",
    "        return cached\n",
    "    \n",
    "    lang_name = LANG_NAMES.get(source_lang, source_lang)\n",
    "    \n",
    "    prompt = f\"\"\"Traduza o texto abaixo de {lang_name} para portugu√™s brasileiro.\n",
    "\n",
    "REGRAS IMPORTANTES:\n",
    "- Tradu√ß√£o fiel e liter√°ria ao original\n",
    "- Mantenha a estrutura de par√°grafos\n",
    "- Preserve nomes pr√≥prios\n",
    "- N√ÉO adicione coment√°rios ou explica√ß√µes\n",
    "- Retorne APENAS a tradu√ß√£o, nada mais\n",
    "\n",
    "TEXTO ORIGINAL:\n",
    "\\\"\\\"\\\"\\n{chunk}\\n\\\"\\\"\\\"\n",
    "\n",
    "TRADU√á√ÉO EM PORTUGU√äS BRASILEIRO:\"\"\"\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            \n",
    "            if response.text:\n",
    "                translated = response.text.strip()\n",
    "                cache.set(chunk, translated, source_lang)\n",
    "                time.sleep(RATE_LIMIT_DELAY)  # Rate limiting\n",
    "                return translated\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Erro tradu√ß√£o: {e}\")\n",
    "    \n",
    "    return chunk  # Fallback: retorna original\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de tradu√ß√£o definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. ENCONTRAR LIVROS PARA TRADUZIR\n",
    "# ============================================\n",
    "\n",
    "def find_books_to_translate() -> List[Dict]:\n",
    "    \"\"\"Encontra livros em EN/ES/RU que precisam tradu√ß√£o.\"\"\"\n",
    "    books = []\n",
    "    \n",
    "    if not TXT_DIR.exists():\n",
    "        print(f\"‚ùå Pasta n√£o existe: {TXT_DIR}\")\n",
    "        return books\n",
    "    \n",
    "    for txt_file in TXT_DIR.rglob(\"*.txt\"):\n",
    "        # Path de sa√≠da\n",
    "        relative = txt_file.relative_to(TXT_DIR)\n",
    "        translated_path = TRANSLATED_DIR / relative.parent / f\"{txt_file.stem}_pt.txt\"\n",
    "        \n",
    "        # J√° traduzido?\n",
    "        if translated_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # L√™ amostra para detectar idioma\n",
    "        try:\n",
    "            with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                sample = f.read(5000)\n",
    "            \n",
    "            lang = detect_language(sample)\n",
    "            \n",
    "            if lang in TARGET_LANGUAGES:\n",
    "                file_size = txt_file.stat().st_size\n",
    "                books.append({\n",
    "                    'path': txt_file,\n",
    "                    'output': translated_path,\n",
    "                    'lang': lang,\n",
    "                    'size': file_size,\n",
    "                    'name': txt_file.stem\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro lendo {txt_file.name}: {e}\")\n",
    "    \n",
    "    # Ordena por tamanho (menores primeiro para progresso r√°pido)\n",
    "    books.sort(key=lambda x: x['size'])\n",
    "    \n",
    "    return books\n",
    "\n",
    "# Encontra livros\n",
    "books_to_translate = find_books_to_translate()\n",
    "\n",
    "print(f\"\\nüìö LIVROS PARA TRADUZIR: {len(books_to_translate)}\")\n",
    "print(f\"\\nPor idioma:\")\n",
    "for lang in TARGET_LANGUAGES:\n",
    "    count = len([b for b in books_to_translate if b['lang'] == lang])\n",
    "    print(f\"  {LANG_NAMES.get(lang, lang)}: {count}\")\n",
    "\n",
    "# Mostra primeiros 10\n",
    "print(f\"\\nPrimeiros 10:\")\n",
    "for book in books_to_translate[:10]:\n",
    "    size_kb = book['size'] / 1024\n",
    "    print(f\"  [{book['lang']}] {book['name'][:50]} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 8. TRADU√á√ÉO MASSIVA\n",
    "# ============================================\n",
    "\n",
    "def translate_book(book: Dict) -> bool:\n",
    "    \"\"\"Traduz um livro completo.\"\"\"\n",
    "    try:\n",
    "        # L√™ texto\n",
    "        with open(book['path'], 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Divide em chunks\n",
    "        chunks = create_chunks(text)\n",
    "        \n",
    "        # Traduz cada chunk\n",
    "        translated_chunks = []\n",
    "        for chunk in chunks:\n",
    "            translated = translate_chunk(chunk, book['lang'])\n",
    "            translated_chunks.append(translated)\n",
    "        \n",
    "        # Junta\n",
    "        final_text = '\\n\\n'.join(translated_chunks)\n",
    "        \n",
    "        # Salva\n",
    "        book['output'].parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(book['output'], 'w', encoding='utf-8') as f:\n",
    "            f.write(final_text)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erro em {book['name']}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Executa tradu√ß√£o\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ INICIANDO TRADU√á√ÉO MASSIVA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "success = 0\n",
    "fail = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=len(books_to_translate), desc=\"Traduzindo\", unit=\"livro\") as pbar:\n",
    "    for book in books_to_translate:\n",
    "        pbar.set_postfix_str(f\"{book['name'][:30]}...\")\n",
    "        \n",
    "        if translate_book(book):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ TRADU√á√ÉO CONCLU√çDA!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Tempo total: {elapsed/60:.1f} minutos\")\n",
    "print(f\"Sucesso: {success}\")\n",
    "print(f\"Falhas: {fail}\")\n",
    "print(f\"Velocidade: {success/(elapsed/60):.1f} livros/minuto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 9. COMMIT E PUSH PARA GITHUB\n",
    "# ============================================\n",
    "\n",
    "# Token do GitHub (configure nas secrets do Colab)\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
    "except:\n",
    "    GITHUB_TOKEN = input(\"Digite seu GitHub Token: \")\n",
    "\n",
    "# Configura remote com token\n",
    "!git remote set-url origin https://{GITHUB_TOKEN}@github.com/JoseRFJuniorLLMs/Googolplex-Books.git\n",
    "\n",
    "# Add e commit\n",
    "!git add translated/ data/translation_cache.db\n",
    "!git status --short | head -20\n",
    "\n",
    "commit_msg = f\"feat: Adiciona {success} livros traduzidos via Vertex AI\"\n",
    "!git commit -m \"{commit_msg}\"\n",
    "\n",
    "# Push\n",
    "!git push origin main\n",
    "\n",
    "print(\"\\n‚úÖ Push para GitHub conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 10. ESTAT√çSTICAS FINAIS\n",
    "# ============================================\n",
    "\n",
    "# Conta arquivos traduzidos\n",
    "translated_files = list(TRANSLATED_DIR.rglob(\"*.txt\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ESTAT√çSTICAS FINAIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total de arquivos traduzidos: {len(translated_files)}\")\n",
    "\n",
    "# Tamanho total\n",
    "total_size = sum(f.stat().st_size for f in translated_files)\n",
    "print(f\"Tamanho total: {total_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Cache stats\n",
    "with sqlite3.connect(CACHE_DB) as conn:\n",
    "    cur = conn.execute(\"SELECT COUNT(*) FROM translations\")\n",
    "    cache_count = cur.fetchone()[0]\n",
    "print(f\"Chunks em cache: {cache_count}\")\n",
    "\n",
    "print(\"\\nüéâ Processo conclu√≠do!\")\n",
    "print(f\"üìÅ Tradu√ß√µes em: {TRANSLATED_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [],
   "gpuType": "V2-8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
